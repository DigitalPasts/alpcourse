[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ancient Language Processing (ALP 2026)",
    "section": "",
    "text": "How do we turn an ancient text into data? How do we apply data science techniques to historical, cultural, and linguistic questions? What are the ramifications of such transformations when confronted with classical approaches to ancient texts? The Ancient Language Processing course will focus specifically on how to answer the above questions when working with ancient languages and scripts from the emergence of writing in Mesopotamia and Egypt, to the rest of the world up till 800 CE. This course will introduce students of ancient history, ancient Near Eastern languages, and computer science to the computational processing of ancient texts. They will engage with inscribed artefactsâ€“from dataset pre-processing to computational analysis via text parsing, vector space models (VSMs), statistical approaches, and graph theory.\n\n\nTime: Wednesday 11:00-14:00 IST / 10:00-13:00 CET\nLocation: Hybrid, on zoom and FU Berlin, room -1.2057"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Schedule",
    "section": "",
    "text": "Unit 1: What is Ancient Language Processing?\n\n\n\n\n\nDate\nTopic\nAssignment\nMaterials\n\n\n\n\n15/04/2026\nBrushing-up on your Python\nChoose one ancient language dataset: familiarize yourself with the data model, consider how this data model fits with the datasets presented in class. Remodel your data accordingly. Do not forget collecting metadata as defined in the class examples.\nTutorials: - How to define a function | - List comprehensions | - Load, parse and extract data from ORACC json files | - RegEx101.com\n\n\n22/04/2026\nNo Class (Independence Day)\n\n\n\n\n29/04/2026\nDoing Computational Philology: the (ideal) pipeline\nExplore your dataset: upload the data into Voyant (or similar programs) or into an LLM and describe it based on distant reading techniques (i.e. quantification). Following this analysis decide where close reading would be necessary.\nReading: Rockwell & Sinclair (2016) | Sommerschield et al. (2023) | Tools: - Voyant Tools; - AntConc; - LLM (ChatGPT, Claude, Olmoe etc.)\n\n\n\n\n\nUnit 2: Digitization and Annotation\n\n\n\n\n\nDate\nTopic\nAssignment\nMaterials\n\n\n\n\n06/05/2026\nPipeline I: Optical / Handwritten Character Recognition\n\nGordin, Alper et al. (2024) | Lincke (2021) | Chauhan's blogpost (2022)\n\n\n13/05/2026\nPipeline II: Annotation (Lemmatization, PoS-Tagging, Treebanking)\n\nOng & Gordin (2024) | Sahala & Lincke (2024) | Farsi et al. (2025)\n\n\n\n\n\nUnit 3: Operationalization and the Vector Space\n\n\n\n\n\nDate\nTopic\nAssignment\nMaterials\n\n\n\n\n20/05/2026\nPipeline III: Operationalization of words I - the vector space\n\nTutorials: - Playing with Neural Networks; Reading: Riemenschneider (2025) | Gordin et al. (2025)\n\n\n27/05/2026\nPipeline IV: Operationalization of words II - Measuring distances\nProcess your data and measure distances: Data model using cuneiform data from ORACC with metadata can be found in the ALP 2024 course repository\nTutorials: - Coding the past: understand TF-IDF in python | - Programming Historian: analyzing documents with TF-IDF | - Karsdorp, Kestemont, & Riddell 2021 | Reading: Gavin (2020) | Bennett & Sahala (2023) | Schweitzer (2023) | Jurafsky & Martin (2024)\n\n\n03/06/2026\nPipeline V: Operationalization of words III - Word vectors\n\n\n\n\n\n\n\nUnit 4: Network Analysis and Visualization\n\n\n\n\n\nDate\nTopic\nAssignment\nMaterials\n\n\n\n\n10/06/2026\nPipeline VI: Visualization I - Graph theory\n\n\n\n\n17/06/2026\nPipeline VII: Visualization II - Network metrics\n\n\n\n\n24/06/2026\nPresentation of class projects and feedback\n\n\n\n\n01/07/2026\nPresentation of class projects and feedback"
  },
  {
    "objectID": "about.html#unit-2-digitization-and-annotation",
    "href": "about.html#unit-2-digitization-and-annotation",
    "title": "Schedule",
    "section": "Unit 2: Digitization and Annotation",
    "text": "Unit 2: Digitization and Annotation\n\n\n\nDate\nTopic\nAssignment\nMaterials\n\n\n\n\n30/04/25\nPipeline I: Optical / Handwritten Character Recognition\n\nOCR, HTR, layout analysis, line segmentation, transcribing\n\nGordin, Alper et al.Â (2024); Lincke (2021); Chauhanâ€™s blogpost (2022)\n\n\n07/05/25\nPipeline II: Annotation\nLemmatization, PoS-Tagging, Treebanking\n\nOng & Gordin (2024); Sahala & Lincke (2024); Farsi et al.Â (2025)"
  },
  {
    "objectID": "about.html#unit-3-operationalization-and-the-vector-space",
    "href": "about.html#unit-3-operationalization-and-the-vector-space",
    "title": "Schedule",
    "section": "Unit 3: Operationalization and the Vector Space",
    "text": "Unit 3: Operationalization and the Vector Space\n\n\n\nDate\nTopic\nAssignment\nMaterials\n\n\n\n\n14/05/25\nNo Class\n\n\n\n\n21/05/25\nPipeline III: Operationalization of words I - the vector space\n\nTutorials:\n\nPlaying with Neural Networks\n\nReading:\nRiemenschneider (2025); Gordin et al.Â (2025)\n\n\n28/05/25\nPipeline IV: Operationalization of words II - measuring distances\nProcess your data and measure distances: Data model using cuneiform data from ORACC with metadata can be found in the ALP 2024 course repository\nTutorials:\n\nCoding the past: understand TF-IDF in python\nProgramming Historian: analyzing documents with TF-IDF\nKarsdorp, Kestemont, & Riddell 2021\nLiterature connector: textual similarity notebook\n\nReading:\nGavin (2020); Bennett & Sahala (2023); Schweitzer (2023); Jurafsky & Martin (2024)"
  },
  {
    "objectID": "about.html#unit-4-network-analysis-and-visualization",
    "href": "about.html#unit-4-network-analysis-and-visualization",
    "title": "Schedule",
    "section": "Unit 4: Network Analysis and Visualization",
    "text": "Unit 4: Network Analysis and Visualization\n\n\n\nDate\nTopic\nAssignment\nMaterials\n\n\n\n\n04/06/25\nPipeline V: Visualization I - Graph theory\n\n\n\n\n11/06/25\nPipeline VI: Visualization II - Network metrics\n\n\n\n\n18/06/25\nPresentation of class projects and feedback"
  },
  {
    "objectID": "coursework.html",
    "href": "coursework.html",
    "title": "Assignments and Active Participation",
    "section": "",
    "text": "We meet on a weekly basis for two and a half hours (synchronic mode) which sums up to c.Â 30 hours over the course of the semester (equivalent to 1 credit point). See schedule for further information on the meetings.\nWe will build a shared glossary of terms and concepts that are important in Ancient Language Processing. Students are going to work in groups of two on the terms assigned to them, providing a short definition and links to more extensive explanations. Writing will be done on the GitHub collaborative platform and will be accessible through the course homepage. (c.Â 30 hours = 1 credit point)\nStudents will develop their own projects based on data provided by the instructors or by data that they contribute. The project plan requires (a) a humanistic research question, (b) operationalize it (i.e., develop a workflow to tackle the question computationally) and (c) present a poster describing their data set, methods, analysis, interpretation and results. The structure of the poster will follow a template of a typical computational humanities paper. (c.Â 120 hours = 4 credit points)\nACL (2023) Term Paper Template. Computational Humanities article style. https://2023.aclweb.org/calls/style_and_formatting/"
  },
  {
    "objectID": "coursework.html#assessment",
    "href": "coursework.html#assessment",
    "title": "Assignments and Active Participation",
    "section": "Assessment",
    "text": "Assessment\n\n\n\n\n\n\n\n\n\nActivity\nNumber\nWeight (%)/activity\nWeight (%)\n\n\nParticipation\n13\nmandatory\n0\n\n\nAssignments\n4\n2\n30\n\n\nResearch poster (1,000 words) + code notebooks (github) + 10 min. presentation\n1\n80\n70\n\n\nTotal (%)\n\n\n100"
  },
  {
    "objectID": "notebooks/02_Python_Brush_up.html",
    "href": "notebooks/02_Python_Brush_up.html",
    "title": "Brushing up on your Python Skills",
    "section": "",
    "text": "The basics of this class are taught in Python. And the neglected basics of ALP is preprocessing our texts.\nPreprocessing for ALP is much broader than what computer and data scientists usually mean. Philological conventions in printed and digital publications hold much more information that needs to be correctly parsed before any computational manipulation (analysis).\nIn this notebook, we are going to provide four examples of messy texts: two in Egyptian and two in Akkadian. We are going to work through the process of how we should parse the texts, what information we are losing when parsing them, and brushing up on basic Python syntax and functions while weâ€™re at it."
  },
  {
    "objectID": "notebooks/01_colab_intro.html",
    "href": "notebooks/01_colab_intro.html",
    "title": "Intro to Google Colab (and some other tips and tricks)",
    "section": "",
    "text": "Google Colab is a cloud-based notebook-style platform for programming in Python (among other languages), which allows you to intersperse executable code with chunks of text like this. It lets you (mostly) avoid the command line, and results in a file that is easy to read and easy to share with others.\nWeâ€™ll be using Colab for in-class exercises and for many of the homework assignments."
  },
  {
    "objectID": "notebooks/05_Vector_Space_Model.html",
    "href": "notebooks/05_Vector_Space_Model.html",
    "title": "Intro",
    "section": "",
    "text": "In this notebook, we explore a collection of ancient Akkadian and ancient Egyptian texts using the vector space model approach described by Karsdorp et al.Â in the chapter â€œExploring Texts using the Vector Space Modelâ€. By representing the texts as numeric vectors capturing word frequencies, we can quantify the lexical similarities and differences between corpora in each of these two ancient languages. The vector space model allows us to reason about texts spatially and apply geometric concepts like distance metrics to assess how â€œcloseâ€ texts are to each other based on shared vocabulary.\nWe preprocess the texts by tokenizing them into words, constructing a document-term matrix recording word frequencies per text, and analyzing the matrix using tools from the Python scientific computing stack, including NumPy, SciPy and Scikit-learn. Through techniques like tSNE (t-Distributed Stochastic Neighbor Embedding) and aggregation by text metadata like script type, language or genre, we explore patterns in the Akkadian and Egyptian corpora and showcase how the vector space model can yield quantitative insights into ancient textual data. The notebook serves as an example application of the concepts and methods covered in depth by Karsdorp et al.Â in their chapter.\nThis notebook has been prepared by Avital Romach and is based on her research. It should be cited accordingly (see citation information at the bottom)."
  },
  {
    "objectID": "notebooks/05_Vector_Space_Model.html#imports",
    "href": "notebooks/05_Vector_Space_Model.html#imports",
    "title": "Intro",
    "section": "Imports",
    "text": "Imports\n\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nimport requests\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom scipy.spatial.distance import pdist, squareform\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport plotly.express as px"
  },
  {
    "objectID": "notebooks/05_Vector_Space_Model.html#functions",
    "href": "notebooks/05_Vector_Space_Model.html#functions",
    "title": "Intro",
    "section": "Functions",
    "text": "Functions\n\nTo upload corpus and metadata from GitHub\n\nFunctions and import for the Akkadian corpus\nThe Akkadian corpus consists of a part of the Royal Inscriptions of the Neo-Assyrian Period (RINAP), licensed CC-BY-SA, and was taken from Open Richely Annotated Cuneiform Corpus (ORACC).\n\ndef create_corpus_from_github_api(url):\n  # URL on the Github where the csv files are stored\n  github_url = url\n  response = requests.get(github_url)\n\n  corpus = []\n  # Check if the request was successful\n  if response.status_code == 200:\n    files = response.json()\n    for file in files:\n      if file[\"download_url\"][-3:] == \"csv\":\n        corpus.append(pd.read_csv(file[\"download_url\"], encoding=\"utf-8\", index_col=\"Unnamed: 0\").fillna(\"\"))\n        # For Egyptian adapt like this:\n        #corpus.append(pd.read_csv(file[\"download_url\"], encoding=\"utf-8\").fillna(\"\"))\n  else:\n    print('Failed to retrieve files:', response.status_code)\n\n  return corpus\n\ndef get_metadata_from_raw_github(url):\n  metadata = pd.read_csv(url, encoding=\"utf-8\", index_col=\"Unnamed: 0\").fillna(\"\")\n  return metadata\n\n\n# Prepare Akkadian corpus (list of dataframes)\n\ncorpus = create_corpus_from_github_api('https://api.github.com/repos/DigitalPasts/ALP-course/contents/course_notebooks/data/rinap01')\ncorpus.extend(create_corpus_from_github_api('https://api.github.com/repos/DigitalPasts/ALP-course/contents/course_notebooks/data/rinap05'))\n\n\n# Prepare Akkadian metadata\nmetadata = get_metadata_from_raw_github(\"https://raw.githubusercontent.com/DigitalPasts/ALP-course/master/course_notebooks/data/rinap1_5_metadata.csv\")\n\n\n\nFunctions and import for the Egyptian corpus\nThe Egyptian corpus is an extract of the database of the Thesaurus Linguae Aegyptiae (TLA), containing literary (and if you like: medical) texts. This export from the database is not published under a free license. Therefore, we access it from a private GitHub repository using an access token.\n\ndef create_corpus_from_private_github_api(url, token):\n# URL on the Github where the csv files are stored\n    headers = {\n        \"Authorization\": f\"token {token}\"\n    }\n    github_url = url\n    response = requests.get(github_url, headers=headers)\n\n    dtype_dict = {\"lemma_id\": \"str\"}\n\n    corpus = []\n    # Check if the request was successful\n    if response.status_code == 200:\n        files = response.json()\n        for file in files:\n            if file[\"download_url\"][-3:] == \"csv\" or \".csv?token=\" in file[\"download_url\"]:\n                corpus.append(pd.read_csv(file[\"download_url\"], encoding=\"utf-8\", sep = ',', dtype=dtype_dict).fillna(\"\"))\n    else:\n        print('Failed to retrieve files:', response.status_code)\n\n    return corpus\n\nfrom io import StringIO\n\ndef get_metadata_from_raw_private_github(url, token):\n    headers = {\n        \"Authorization\": f\"token {token}\"\n    }\n    github_url = url\n    response = requests.get(github_url, headers=headers)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        csv_data = StringIO(response.text)\n        metadata = pd.read_csv(csv_data, encoding=\"utf-8\", sep = ',', index_col=\"text_id\").fillna(\"\")\n        return metadata\n    else:\n        raise Exception(f\"Failed to retrieve metadata: {response.status_code}\")\n\n\n# only if corpus is not yet loaded\n# Prepare Egyptian corpus (list sof dataframes)\n\nif False:\n\n  #tla_access_token = \"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\"\n  tla_access_token = \"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\"\n\n  ## TLA Literature\n  corpus = create_corpus_from_private_github_api('https://api.github.com/repos/thesaurus-linguae-aegyptiae/test-rawdata/contents/alp-course-2024/TLA_literature/erzaehlungen', tla_access_token)\n\n  corpus.extend(create_corpus_from_private_github_api('https://api.github.com/repos/thesaurus-linguae-aegyptiae/test-rawdata/contents/alp-course-2024/TLA_literature/reden', tla_access_token))\n\n  corpus.extend(create_corpus_from_private_github_api('https://api.github.com/repos/thesaurus-linguae-aegyptiae/test-rawdata/contents/alp-course-2024/TLA_literature/lehren', tla_access_token))\n\n  ## TLA Medical\n  #corpus = create_corpus_from_private_github_api('https://api.github.com/repos/thesaurus-linguae-aegyptiae/test-rawdata/contents/alp-course-2024/TLA_medical/TLA_pEbers', tla_access_token)\n\n  #corpus.extend(create_corpus_from_private_github_api('https://api.github.com/repos/thesaurus-linguae-aegyptiae/test-rawdata/contents/alp-course-2024/TLA_medical/TLA_pEdwinSmith', tla_access_token))\n\nFailed to retrieve files: 401\nFailed to retrieve files: 401\nFailed to retrieve files: 401\n\n\n\n# Egyptian metadata\n# metadata = get_metadata_from_raw_private_github(\"https://raw.githubusercontent.com/thesaurus-linguae-aegyptiae/test-rawdata/master/alp-course-2024/TLA_literature/TLA_metadata.csv\", tla_access_token)\n\n\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n&lt;ipython-input-7-3058004b5d4e&gt; in &lt;cell line: 0&gt;()\n      1 # Egyptian metadata\n----&gt; 2 metadata = get_metadata_from_raw_private_github(\"https://raw.githubusercontent.com/thesaurus-linguae-aegyptiae/test-rawdata/master/alp-course-2024/TLA_literature/TLA_metadata.csv\", tla_access_token)\n\n&lt;ipython-input-5-af2596670e89&gt; in get_metadata_from_raw_private_github(url, token)\n     36         return metadata\n     37     else:\n---&gt; 38         raise Exception(f\"Failed to retrieve metadata: {response.status_code}\")\n\nException: Failed to retrieve metadata: 404\n\n\n\n\ncorpus[0].head()\n\n\n    \n\n\n\n\n\n\nref\ninst\nfrag\nnorm\ncf\nsense\npos\nunicode\nunicode_word\nreading\nbreak\nbreak_perc\nmask\nlang\ntext\nline\nword\n\n\n\n\n9515\nQ003414.2.1\nperÊ¾u[bud//offspring]N$perÊ¾i\nNUNUZ\nperÊ¾i\nperÊ¾u\noffspring\nN\n['ğ’‰­']\nğ’‰­\nNUNUZ\n['complete']\n0.00\n\n\nQ003414\n2\n1\n\n\n9516\nQ003414.2.2\nBaltil[AÅ¡Å¡ur]QN\nbal-til{ki}\nBaltil\nBaltil\nAÅ¡Å¡ur\nQN\n['ğ’„', 'ğ’Œ€', 'ğ’† ']\nğ’„ğ’Œ€ğ’† \nbal-til{KI}\n['complete', 'complete', 'complete']\n0.00\n\n\nQ003414\n2\n2\n\n\n9517\nQ003414.2.3\nÅ¡Å«quru[very valuable]AJ\nÅ¡u-â¸¢qu-ruâ¸£\nÅ¡Å«quru\nÅ¡Å«quru\nvery valuable\nAJ\n['ğ’‹—', 'ğ’„£', 'ğ’Š’']\nğ’‹—ğ’„£ğ’Š’\nÅ¡u-â¸¢qu-ruâ¸£\n['complete', 'damaged', 'damaged']\n0.33\n\n\nQ003414\n2\n3\n\n\n9518\nQ003414.2.4\nnarÄm[loved one]N\nna-ram\nnarÄm\nnarÄmu\nloved one\nN\n['ğ’ˆ¾', 'ğ’‰˜']\nğ’ˆ¾ğ’‰˜\nna-ram\n['complete', 'complete']\n0.00\n\n\nQ003414\n2\n4\n\n\n9519\nQ003414.2.5\nu\n{d}[(...)\n\n\n\nu\n['ğ’€­', 'x']\nğ’€­x\n{d}[x]\n['complete', 'missing']\n0.50\n\n\nQ003414\n2\n5\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# Prepare text_ids (list of unique ids), and metadata\n\ntext_ids = []\nfor text in corpus:\n  text_ids.append(text[\"text\"].iloc[0])\n\n\nfor id in text_ids:\n  if id not in metadata.index:\n    print(f\"Text {id} missing from metadata\")\n\nmetadata = metadata[metadata.index.isin(text_ids)]\n\nmetadata\n\n\n    \n\n\n\n\n\n\nlangs\nproject\ncdli_id\ncollection\ncredits\ndate_of_origin\ndesignation\ndisplay_name\ndynastic_seat\nexemplars\n...\nruler\nscript\nscript_remarks\nscript_type\nsubgenre\nsupergenre\ntrans\nid_text\nseal_id\nattribution\n\n\n\n\nQ003414\n0x08000000\nrinap/rinap1\nP463047\nArchÃ¤ologisches Institut der UniversitÃ¤t ZÃ¼ric...\nCreated by Hayim Tadmor, Shigeo Yamada, Jamie ...\n744-727\nTiglath-pileser III 01\nRINAP 1 Tigl. III 01\nAssyria\nZhArchSlg 1917 (+) ZhArchSlg 1918 (+) NA 12/76\n...\nTiglath-pileser III\nNeo-Assyrian\ninscribed\nCuneiform\nTiglath-pileser III\nLIT\n['en']\n\n\n\n\n\nQ003415\n0x08000000\nrinap/rinap1\nP463047\n\nCreated by Hayim Tadmor, Shigeo Yamada, Jamie ...\n744-727\nTiglath-pileser III 02\nRINAP 1 Tigl. III 02\nAssyria\nNA 12/76\n...\nTiglath-pileser III\nNeo-Assyrian\ninscribed\nCuneiform\nTiglath-pileser III\nLIT\n['en']\n\n\n\n\n\nQ003416\n0x08000000\nrinap/rinap1\nP463048\n\nCreated by Hayim Tadmor, Shigeo Yamada, Jamie ...\n744-727\nTiglath-pileser III 03\nRINAP 1 Tigl. III 03\nAssyria\nNA 09/76\n...\nTiglath-pileser III\nNeo-Assyrian\ninscribed\nCuneiform\nTiglath-pileser III\nLIT\n['en']\n\n\n\n\n\nQ003417\n0x08000000\nrinap/rinap1\nP463048\n\nCreated by Hayim Tadmor, Shigeo Yamada, Jamie ...\n744-727\nTiglath-pileser III 04\nRINAP 1 Tigl. III 04\nAssyria\nNA 09/76\n...\nTiglath-pileser III\nNeo-Assyrian\ninscribed\nCuneiform\nTiglath-pileser III\nLIT\n['en']\n\n\n\n\n\nQ003418\n0x08000000\nrinap/rinap1\nP463049\nBritish Museum, London, UK\nCreated by Hayim Tadmor, Shigeo Yamada, Jamie ...\n744-727\nTiglath-pileser III 05\nRINAP 1 Tigl. III 05\nAssyria\nBM 118934 (Layard, MS A pp. 113-114)\n...\nTiglath-pileser III\nNeo-Assyrian\ninscribed\nCuneiform\nTiglath-pileser III\nLIT\n['en']\n\n\n\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nQ009295\n0x08000000\nrinap/rinap5\n\nBritish Museum, London, UK\nCreated by Jamie Novotny and Joshua Jeffers, 2...\n668-ca. 631\nAshurbanipal 1027\nRINAP 5 Asb. 1027\nAssyria\nK 06681\n...\nAshurbanipal\nNeo-Assyrian\ninscribed\nCuneiform\nAshurbanipal\nLIT\n['en']\n\n\nCreated by Jamie Novotny and Joshua Jeffers, 2...\n\n\nQ009296\n0x08000000\nrinap/rinap5\n\nBritish Museum, London, UK\nCreated by Jamie Novotny and Joshua Jeffers, 2...\n668-ca. 631\nAshurbanipal 1028\nRINAP 5 Asb. 1028\nAssyria\nK 06806\n...\nAshurbanipal\nNeo-Assyrian\ninscribed\nCuneiform\nAshurbanipal\nLIT\n['en']\n\n\nCreated by Jamie Novotny and Joshua Jeffers, 2...\n\n\nQ009297\n0x08000000\nrinap/rinap5\n\nBritish Museum, London, UK\nCreated by Jamie Novotny and Joshua Jeffers, 2...\n668-ca. 631\nAshurbanipal 1029\nRINAP 5 Asb. 1029\nAssyria\nBu 1891-05-09, 0204\n...\nAshurbanipal\nNeo-Assyrian\ninscribed\nCuneiform\nAshurbanipal\nLIT\n['en']\n\n\nCreated by Jamie Novotny and Joshua Jeffers, 2...\n\n\nQ009298\n0x08000000\nrinap/rinap5\n\n\nCreated by Jamie Novotny and Joshua Jeffers, 2...\n668-ca. 631\nAshurbanipal 1030\nRINAP 5 Asb. 1030\nAssyria\nShikaft-i Gulgul rock relief\n...\nAshurbanipal\nNeo-Assyrian, Neo-Babylonian\ninscribed\nCuneiform\nAshurbanipal\nLIT\n['en']\n\n\nCreated by Jamie Novotny and Joshua Jeffers, 2...\n\n\nQ009503\n0x08000000\nrinap/rinap5\n\nBritish Museum, London, UK\nCreated by Jamie Novotny, 2022. Lemmatized by ...\nca. 626-612\nSÃ®n-Å¡arru-iÅ¡kun 2001\nRINAP 5 Ssi 2001\nAssyria\n1855-12-05, 0252\n...\nSÃ®n-Å¡arru-iÅ¡kun\nNeo-Assyrian\ninscribed\nCuneiform\nSÃ®n-Å¡arru-iÅ¡kun\nLIT\n['en']\n\n\nCreated by Jamie Novotny, 2022. Lemmatized by ...\n\n\n\n\n432 rows Ã— 31 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\n\nTo convert dataframe to string\nFunction to split the text dataframes according to a column. Used to separate text to lines: * param df: dataframe containing one word in each row. * param column: the column by which to split the dfs, perferably text or line. * return: a list of dataframes split according to the value given to the column parameter.\n\ndef split_df_by_column_value(df, column):\n\n    dfs = []\n    column_values = df[column].unique()\n    for value in column_values:\n        split_df = df[df[column]==value]\n        dfs.append(split_df)\n    return dfs\n\n\nsplit_df_by_column_value(corpus[0].head(), \"line\")\n\n[              ref                          inst         frag    norm      cf  \\\n 9515  Q003414.2.1  perÊ¾u[bud//offspring]N$perÊ¾i        NUNUZ   perÊ¾i   perÊ¾u   \n 9516  Q003414.2.2               Baltil[AÅ¡Å¡ur]QN  bal-til{ki}  Baltil  Baltil   \n 9517  Q003414.2.3       Å¡Å«quru[very valuable]AJ   Å¡u-â¸¢qu-ruâ¸£  Å¡Å«quru  Å¡Å«quru   \n 9518  Q003414.2.4             narÄm[loved one]N       na-ram   narÄm  narÄmu   \n 9519  Q003414.2.5                             u    {d}[(...)                   \n \n               sense pos          unicode unicode_word      reading  \\\n 9515      offspring   N            ['ğ’‰­']            ğ’‰­        NUNUZ   \n 9516          AÅ¡Å¡ur  QN  ['ğ’„', 'ğ’Œ€', 'ğ’† ']          ğ’„ğ’Œ€ğ’†   bal-til{KI}   \n 9517  very valuable  AJ  ['ğ’‹—', 'ğ’„£', 'ğ’Š’']          ğ’‹—ğ’„£ğ’Š’   Å¡u-â¸¢qu-ruâ¸£   \n 9518      loved one   N       ['ğ’ˆ¾', 'ğ’‰˜']           ğ’ˆ¾ğ’‰˜       na-ram   \n 9519                  u       ['ğ’€­', 'x']           ğ’€­x       {d}[x]   \n \n                                      break  break_perc mask lang     text  \\\n 9515                          ['complete']        0.00            Q003414   \n 9516  ['complete', 'complete', 'complete']        0.00            Q003414   \n 9517    ['complete', 'damaged', 'damaged']        0.33            Q003414   \n 9518              ['complete', 'complete']        0.00            Q003414   \n 9519               ['complete', 'missing']        0.50            Q003414   \n \n       line  word  \n 9515     2     1  \n 9516     2     2  \n 9517     2     3  \n 9518     2     4  \n 9519     2     5  ]\n\n\nFunction to convert the values from the text dataframe to a string of text with or without line breaks and word segmentation. * param df: the text dataframe * param column: the chosen column from the dataframe to construct the text from (preferably unicode_word, cf, or lemma) * param break_perc: a parameter which dictates whether to include broken words depending on the percentage of how broken they are. Compares this value to the break_perc column in the dataframe. Parameter is set to 1 (i.e.Â all words, whether broken or not, are included); can be any float between 0 and 1. * param mask: boolean whether to mask named entities or not; set to True. * return: a string which includes all the words in the texts according to the column chosen. Extra spaces that were between broken words or empty lines are removed.\n\ndef df2str(df, column, break_perc=1, mask=True, segmentation=True):\n\n    # check if column exists in dataframe. If not, return empty text.\n    if column not in df.columns:\n        return (\"\", 0, 0)\n    else:\n        # remove rows that include duplicate values for compound words\n        if column not in [\"norm\", \"cf\", \"sense\", \"pos\"]:\n            df = df.drop_duplicates(\"ref\").copy()\n        # if column entry is empty string, replace with UNK (can happen with normalization or lemmatization)\n        mask_empty = df[column]==\"\"\n        df[column] = df[column].where(~mask_empty, other=\"UNK\")\n        # mask proper nouns\n        if mask and \"pos\" in df.columns:\n            mask_bool = df[\"pos\"].isin([\"PN\", \"RN\", \"DN\", \"GN\", \"MN\", \"SN\", \"n\"])\n            df[column] = df[column].where(~mask_bool, other=df[\"pos\"])\n\n        # change number masking from `n` to `NUM`\n        # !comment out for Egyptian\n        #if mask:\n        #    mask_num = df[column]==\"n\"\n        #    df[column] = df[column].where(~mask_num, other=\"NUM\")\n\n        # remove rows without break_perc (happens with non-Akkadian words)\n        if \"\" in df[\"break_perc\"].unique():\n            df = df[df[\"break_perc\"]!=\"\"].copy()\n        # filter according to break_perc\n        mask_break = df[\"break_perc\"] &lt;= break_perc\n        df[column] = df[column].where(mask_break, other=\"X\")\n        # calculate text length with and without UNK and x tokens\n        text_length_full = df.shape[0]\n        mask_partial = df[column].isin([\"UNK\", \"X\", \"x\"])\n        text_length_partial = text_length_full - sum(mask_partial)\n        # create text lines\n        text = \"\"\n        df_lines = split_df_by_column_value(df, \"line\")\n        for line in df_lines:\n            word_list = list(filter(None, line[column].to_list()))\n            if word_list != []:\n                text += \" \".join(map(str, word_list)).replace(\"x\", \"X\").strip() + \"\\n\"\n\n        if segmentation == False:\n            # remove all white spaces (word segmentation and line breaks)\n            text = re.sub(r\"[\\s\\u00A0]+\", \"\", text)\n\n        return (text, text_length_full, text_length_partial)\n\n\ndf2str(corpus[0], \"cf\")\n\n('perÊ¾u Baltil Å¡Å«quru narÄmu UNK DN UNK UNK UNK UNK\\npitqu DN Å¡a ana bÄ“lÅ«tu mÄtu UNK UNK UNK UNK UNK UNK\\nrabÃ» ana Å¡arrÅ«tu Å¡akkanakku UNK UNK UNK UNK UNK UNK\\nmuá¹£á¹£ibu Å¡agigurrÃ» ana UNK UNK UNK UNK UNK UNK UNK UNK Å¡urÄ«nu\\nzikaru dannu nÅ«ru kiÅ¡Å¡atu niÅ¡u etellu UNK kalÃ» malku UNK UNK UNK\\ndÄÊ¾ipu gÄ“rÃ» eá¹­lu qardu sÄpinu UNK nakru Å¡a hurÅ¡Änu\\netguru kÄ«ma qÃ» salÄtu UNK UNK UNK UNK UNK UNK\\n',\n 75,\n 39)\n\n\n\n\nTo convert to specific word levels and create dictionaries\nFunction to convert the dataframes into strings of lemmatized texts. * param corpus: a list of dataframes * param break_perc: a parameter which dictates whether to include broken words depending on the percentage of how broken they are. Compares this value to the break_perc column in the dataframe. Parameter is set to 1 (i.e.Â all words, whether broken or not, are included); can be any float between 0 and 1. * param mask: boolean whether to mask named entities or not; set to True. * return: a dictionary where the keys are the text IDs and the values are the lemmatized texts\n\ndef get_lemmatized_texts(corpus, break_perc=1, mask=True):\n\n    texts_dict = {}\n    for df in corpus:\n        # get the text number from the dataframe \"text\" column\n        key = df[\"text\"].iloc[0]\n        text, text_length_full, text_length_partial = df2str(df, \"lemma_id\", break_perc, mask)\n        texts_dict[key] = (text, text_length_full, text_length_partial)\n    return texts_dict\n\n\nget_lemmatized_texts((split_df_by_column_value(corpus[0], \"text\")))\n\n{'Q003414': ('', 0, 0)}\n\n\nFunction to convert the dataframes into strings of normalized texts. * param corpus: a list of dataframes * param break_perc: a parameter which dictates whether to include broken words depending on the percentage of how broken they are. Compares this value to the break_perc column in the dataframe. Parameter is set to 1 (i.e.Â all words, whether broken or not, are included); can be any float between 0 and 1. * param mask: boolean whether to mask named entities or not; set to True. * return: a dictionary where the keys are the text IDs and the values are the normalized texts\n\ndef get_normalized_texts(corpus, break_perc=1, mask=True):\n\n    texts_dict = {}\n    for df in corpus:\n        # get the text number from the dataframe \"text\" column\n        key = df[\"text\"].iloc[0]\n        text, text_length_full, text_length_partial = df2str(df, \"norm\", break_perc, mask)\n        texts_dict[key] = (text, text_length_full, text_length_partial)\n    return texts_dict\n\n\nget_normalized_texts((split_df_by_column_value(corpus[0], \"text\")))\n\n{'Q003414': ('perÊ¾i Baltil Å¡Å«quru narÄm UNK DN UNK UNK UNK UNK\\npitiq DN Å¡a ana bÄ“lÅ«t mÄtÄti UNK UNK UNK UNK UNK UNK\\nirbÃ» ana Å¡arrÅ«ti Å¡akkanakku UNK UNK UNK UNK UNK UNK\\nmuá¹£á¹£ib Å¡agigurÃª ana UNK UNK UNK UNK UNK UNK UNK UNK Å¡urinnÄ«\\nzikaru dannu nÅ«r kiÅ¡Å¡at niÅ¡Ä«Å¡u etel UNK kal malkÄ« UNK UNK UNK\\ndÄÊ¾ipu gÄrÃªÅ¡u eá¹­lu qardu sÄpinu UNK nakiri Å¡a hursÄnÄ«\\netgurÅ«ti kÄ«ma qÃª usallituma UNK UNK UNK UNK UNK UNK\\n',\n  75,\n  39)}\n\n\nFunction to convert the dataframes into strings of segmented unicode texts. * param corpus: a list of dataframes * param break_perc: a parameter which dictates whether to include broken words depending on the percentage of how broken they are. Compares this value to the break_perc column in the dataframe. Parameter is set to 1 (i.e.Â all words, whether broken or not, are included); can be any float between 0 and 1. * param mask: boolean whether to mask named entities or not; set to True. * return: a dictionary where the keys are the text IDs and the values are the segmented unicode texts\n\ndef get_segmented_unicode_texts(corpus, break_perc=1, mask=True):\n\n    texts_dict = {}\n    for df in corpus:\n        # get the text number from the dataframe \"text\" column\n        key = df[\"text\"].iloc[0]\n        text, text_length_full, text_length_partial = df2str(df, \"unicode_word\", break_perc, mask)\n        texts_dict[key] = (text, text_length_full, text_length_partial)\n    return texts_dict\n\n\nget_segmented_unicode_texts((split_df_by_column_value(corpus[0], \"text\")))\n\n{'Q003414': ('ğ’‰­ ğ’„ğ’Œ€ğ’†  ğ’‹—ğ’„£ğ’Š’ ğ’ˆ¾ğ’‰˜ ğ’€­X DN ğ’€Š ğ’€ X X\\nğ’‰¿ğ’„˜ DN ğ’ƒ» ğ’€€ğ’ˆ¾ ğ’ğ’‚ ğ’†³ğ’†³ X ğ’€­ ğ’‹¾ ğ’€  ğ’‹ƒ X\\nğ’…•ğ’ğ’Œ‘ ğ’€€ğ’ˆ¾ ğ’ˆ—ğ’Œ‘ğ’‹¾ ğ’„Šğ’€´ X X X X X ğ’ˆ \\nğ’ˆ¬ğ’¦ ğ’Š®ğ’…†ğ’ƒ¸ğ’Œ ğ’€€ğ’ˆ¾ X X X X X X X X ğ’‹—ğ’Š‘ğ’…”ğ’‰Œ\\nğ’£ğ’…—ğ’Š’ ğ’†—ğ’‰¡ ğ’‰¡ğ’Œ¨ ğ’†§ğ’†³ ğ’Œ¦ğ’Œğ’‹— ğ’‚Šğ’Œ€ X ğ’†— ğ’‚·ğ’†  X X ğ’‹¾\\nğ’•ğ’„¿ğ’ ğ’‚µğ’Š‘ğ’‚Šğ’‹— ğ’„¨ ğ’ƒ¼ğ’º ğ’Š“ğ’‰¿ğ’‰¡ X ğ’ˆ¾ğ’† ğ’Š‘ ğ’ƒ» ğ’„¯ğ’Š“ğ’€€ğ’‰Œ\\nğ’€‰ğ’„–ğ’Š’ğ’‹¾ ğ’† ğ’ˆ  ğ’† ğ’‚Š ğ’Œ‘ğ’Š©ğ’‡·ğ’Œ…ğ’ˆ  ğ’Œ‘X ğ’Œ‘ X X X X\\n',\n  75,\n  50)}\n\n\n\n\nTo create the vector space model\n\nvectorizing texts\nConverts a list of texts into a term-document matrix based on TF-IDF scores.\nFull documentation of the variables of TfidfVectorizer from sklearn, see: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer * param corpus: a dataframe in which the texts are in a \"text\" column and the dataframeâ€™s index is the text ids. * param analyzer: whether the feature should be made of word or character n-grams. use \"word\" for word features, \"char_wb\" for character n-grams within word boundaries, or \"char\" for character n-grams without word boundaries. * param ngram_range: the lower and upper boundary of the range of n-values for different n-grams to be extracted. * param max_df: threshold to ignore terms that have a document frequency above a certain value. If the threshold is a float, it represent a proportion of the documents. If the threshold is an integer, it represents absolute counts of number of documents in which the terms appears. * param min_df: threshold to ignore terms that have a document frequency below a certain value. If the threshold is a float, it represent a proportion of the documents. If the threshold is an integer, it represents absolute counts of number of documents in which the terms appears. * param max_features: if not None, build a vocabulary that only considers the top max_features ordered by term frequency across the corpus. * param stop_words: if None, no stop words are used. Otherwise, can be a list with words to be removed from resulting tokens. * return: counts the raw counts of the vectorizer, counts_df a dataframe of the counts where the index is the text ids and the columns are the tokens, stop_words an updated list of stop words\n\nFigure 1. Example of a document-term matrix extracted from a corpus, see Fig. 3 in Karsdorp, F., Kestemont, M., & Riddell, A. (2021). Humanities Data Analysis: Case Studies with Python. Princeton University Press.\n\ndef vectorize(corpus, analyzer=\"word\", ngram_range=(1,1), max_df=1.0, min_df=1, max_features=None, stop_words=[\"UNK\", \"X\"]):\n\n    vectorizer = TfidfVectorizer(input=\"content\", lowercase=False, analyzer=analyzer,\n                                 # RegEx for Akkadian\n                                 #token_pattern=r\"(?u)\\b\\w+\\b\", ngram_range=ngram_range,\n                                 # RegEx for Egyptian\n                                 token_pattern=r\"(?u)\\b[\\w\\.]+\\b\", ngram_range=ngram_range,\n                                 max_df=max_df, min_df=min_df, max_features=max_features, stop_words=stop_words)\n\n    counts = vectorizer.fit_transform(corpus[\"text\"].tolist()).toarray()\n    stop_words = vectorizer.stop_words_\n\n    # saving the vocab used for vectorization, and switching the dictionary so that the feature index is the key\n    vocab = vectorizer.vocabulary_\n    switched_vocab = {value: key for key, value in vocab.items()}\n    # adding the vocab words to the counts dataframe for easier viewing.\n    column_names = []\n    x = 0\n    while x &lt; len(switched_vocab):\n        column_names.append(switched_vocab[x])\n        x += 1\n\n    counts_df = pd.DataFrame(counts, index=corpus.index, columns=column_names)\n\n    return (counts, counts_df, stop_words)\n\n\n\ncalculating distances between vectorized documents\nConverts a term-document matrix to a text similarity matrix. * param counts: the raw counts from the vectorize function. * param metric: the metric by which to calculate the distances between the texts in the corpus. For one place to look into the different types of matrics see â€œComputing distances between documentsâ€ in Karsdrop, Kestemont, & Riddell 2021 Valid metrics are: â€˜braycurtisâ€™, â€˜canberraâ€™, â€˜chebyshevâ€™, â€˜cityblockâ€™, â€˜correlationâ€™, â€˜cosineâ€™, â€˜diceâ€™, â€˜euclideanâ€™, â€˜hammingâ€™, â€˜jaccardâ€™, â€˜jensenshannonâ€™, â€˜kulczynski1â€™, â€˜mahalanobisâ€™, â€˜matchingâ€™, â€˜minkowskiâ€™, â€˜rogerstanimotoâ€™, â€˜russellraoâ€™, â€˜seuclideanâ€™, â€˜sokalmichenerâ€™, â€˜sokalsneathâ€™, â€˜sqeuclideanâ€™, â€˜yuleâ€™. * param text_ids: list of unique text_ids. * return: a dataframe matrix of distance between texts.\n\ndef distance_calculator(counts, metric, text_ids):\n\n    return pd.DataFrame(squareform(pdist(counts, metric=metric)), index=text_ids, columns=text_ids)\n\n\n\nreducing dimensions with pca or tsne\nReduces multidimensional data into two dimensions using PCA. * param df: dataframe holding the dimensions to reduce. All columns should include numerical values only. The dataframeâ€™s index should hold the unique text ids. * param metadata: the rest of the metadata in the corpus, to help visualize the resulting clusters in meaningful ways. The metadataâ€™s index should hold the unique text ids. * return: a dataframe with the coordinates of the two remaining dimensions on all other columns from the metadata.\n\ndef reduce_dimensions_pca(df, metadata):\n\n    pca = PCA(n_components=2)\n    reduced_data = pca.fit_transform(df)\n    reduced_df = pd.DataFrame(data=reduced_data, index=df.index, columns=[\"component 1\", \"component 2\"])\n    reduced_df_metadata = metadata.join(reduced_df)\n    return reduced_df_metadata\n\nReduces multidimensional data into two dimensions using TSNE.\nSee full documentation of sklearnâ€™s TSNE on: https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html * param df: dataframe holding the dimensions to reduce. All columns should include numerical values only. The dataframeâ€™s index should hold the unique text ids. * param perplexity: perplexity is a measure the weighs the importance of nearby versus distant points when creating a lower-dimension mapping. t-SNE first converts the distances between points into conditional probabilities that represent similarities, using Gaussian probability distributions. The perplexity parameter influences the variance used to compute these probabilities. A higher perplexity leads to a broader Gaussian that considers a larger number of neighbors when assessing similarity. Lower perplexity puts more focus on the local structure and considers fewer neighbors. A good perplexity depends greatly on dataset size and density. The documentation recommends a value between 5 and 50. We recommend to start with the square root of the length of the corpus. * param n_iter: maximum number of iterations for optimization. * param metric: the metric to be used when calculating distances between vectors. Valid metrics are: â€˜braycurtisâ€™, â€˜canberraâ€™, â€˜chebyshevâ€™, â€˜cityblockâ€™, â€˜correlationâ€™, â€˜cosineâ€™, â€˜diceâ€™, â€˜euclideanâ€™, â€˜hammingâ€™, â€˜jaccardâ€™, â€˜jensenshannonâ€™, â€˜kulczynski1â€™, â€˜mahalanobisâ€™, â€˜matchingâ€™, â€˜minkowskiâ€™, â€˜rogerstanimotoâ€™, â€˜russellraoâ€™, â€˜seuclideanâ€™, â€˜sokalmichenerâ€™, â€˜sokalsneathâ€™, â€˜sqeuclideanâ€™, â€˜yuleâ€™. * param metadata: the rest of the metadata in the corpus, to help visualize the resulting clusters in meaningful ways. The metadataâ€™s index should hold the unique text ids. * return: a dataframe with the coordinates of the two remaining dimensions on all other columns from the metadata.\n\ndef reduce_dimensions_tsne(df, perplexity, n_iter, metric, metadata):\n\n    tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=n_iter, metric=metric, init=\"pca\")\n    reduced_data = tsne.fit_transform(df)\n    reduced_df = pd.DataFrame(data=reduced_data, index=df.index, columns=[\"component 1\", \"component 2\"])\n    reduced_df_metadata = metadata.join(reduced_df)\n    return reduced_df_metadata"
  },
  {
    "objectID": "notebooks/05_Vector_Space_Model.html#process-texts-from-dataframes-and-combine-results-with-metadata-dataframe",
    "href": "notebooks/05_Vector_Space_Model.html#process-texts-from-dataframes-and-combine-results-with-metadata-dataframe",
    "title": "Intro",
    "section": "Process texts from dataframes and combine results with metadata dataframe",
    "text": "Process texts from dataframes and combine results with metadata dataframe\n\n# Function to combine processed texts with metadata\n\ndef get_corpus_metadata(texts_dict, metadata):\n  texts_df = pd.DataFrame(texts_dict, index=[\"text\", \"full_length\", \"partial_length\"]).transpose()\n  df = metadata.join(texts_df)\n  return df\n\n\n## vectorize lemma forms\ncorpus_dict = get_lemmatized_texts(corpus, break_perc=0)\n## vectorize normalized forms\ncorpus_dict = get_normalized_texts(corpus, break_perc=0)\n## vectorize Unicode cuneiform\ncorpus_dict = get_segmented_unicode_texts(corpus, break_perc=0)\n\ncorpus_metadata = get_corpus_metadata(corpus_dict, metadata)\n\n## For Akkadian\n## remove texts which have less than n words excluding UNK and X\nn = 10\nprint(f\"Number of texts before filtering: {corpus_metadata.shape[0]}\")\ncorpus_metadata = corpus_metadata[corpus_metadata[\"partial_length\"]&gt;=n]\nprint(f\"Number of texts after filtering: {corpus_metadata.shape[0]}\")\n\n\n# For Egyptian use this instead, reset the index\n# n = 150\n# print(f\"Number of texts before filtering: {corpus_metadata.shape[0]}\")\n# corpus_metadata = corpus_metadata[corpus_metadata[\"partial_length\"]&gt;=n].set_index(\"text_name\")\n# print(f\"Number of texts after filtering: {corpus_metadata.shape[0]}\")\n\nNumber of texts before filtering: 432\nNumber of texts after filtering: 308\n\n\n\ncorpus_metadata\n\n\n    \n\n\n\n\n\n\nlangs\nproject\ncdli_id\ncollection\ncredits\ndate_of_origin\ndesignation\ndisplay_name\ndynastic_seat\nexemplars\n...\nscript_type\nsubgenre\nsupergenre\ntrans\nid_text\nseal_id\nattribution\ntext\nfull_length\npartial_length\n\n\n\n\nQ003414\n0x08000000\nrinap/rinap1\nP463047\nArchÃ¤ologisches Institut der UniversitÃ¤t ZÃ¼ric...\nCreated by Hayim Tadmor, Shigeo Yamada, Jamie ...\n744-727\nTiglath-pileser III 01\nRINAP 1 Tigl. III 01\nAssyria\nZhArchSlg 1917 (+) ZhArchSlg 1918 (+) NA 12/76\n...\nCuneiform\nTiglath-pileser III\nLIT\n['en']\n\n\n\nğ’‰­ ğ’„ğ’Œ€ğ’†  X ğ’ˆ¾ğ’‰˜ X X ğ’€Š ğ’€ X X\\nğ’‰¿ğ’„˜ X ğ’ƒ» ğ’€€ğ’ˆ¾ ğ’ğ’‚ X X ğ’€­ ğ’‹¾ ğ’€ ...\n75\n35\n\n\nQ003417\n0x08000000\nrinap/rinap1\nP463048\n\nCreated by Hayim Tadmor, Shigeo Yamada, Jamie ...\n744-727\nTiglath-pileser III 04\nRINAP 1 Tigl. III 04\nAssyria\nNA 09/76\n...\nCuneiform\nTiglath-pileser III\nLIT\n['en']\n\n\n\nğ’„¿ğ’ˆ¾ ğ’Š• ğ’ˆ—ğ’‹¾ğ’…€ ğ’„¿ğ’ˆ¾ ğ’ˆ¤ğ’Š‘ğ’‚Š ğ’„ğ’…€ ğ’„¿ğ’ˆ¾ n X\\nğ’Š­ ğ’„¿ğ’ˆ¾ ğ’„‘ğ’„–ğ’ ğ’ˆ—ğ’Œ‘ğ’‹¾ ğ’Šğ’„« ğ’Œ‘ğ’…†ğ’...\n37\n36\n\n\nQ003418\n0x08000000\nrinap/rinap1\nP463049\nBritish Museum, London, UK\nCreated by Hayim Tadmor, Shigeo Yamada, Jamie ...\n744-727\nTiglath-pileser III 05\nRINAP 1 Tigl. III 05\nAssyria\nBM 118934 (Layard, MS A pp. 113-114)\n...\nCuneiform\nTiglath-pileser III\nLIT\n['en']\n\n\n\nX X ğ’€€ğ’ˆ¾ ğ’†³ğ’‹—ğ’‰¡ ğ’…‹ğ’‡·ğ’†ª ğ’Œ·ğ’Œ X X ğ’Œğ’‹—ğ’‹¾ ğ’†•ğ’‘ ğ’„¿ğ’ˆ¾ ğ’Œ‹ğ’…— ğ’‡¯ ğ’„°ğ’Š‘\\nX X X...\n184\n98\n\n\nQ003419\n0x08000000\nrinap/rinap1\nP463049\nBritish Museum, London, UK\nCreated by Hayim Tadmor, Shigeo Yamada, Jamie ...\n744-727\nTiglath-pileser III 06\nRINAP 1 Tigl. III 06\nAssyria\nBM 118934 (Layard, MS A p. 114)\n...\nCuneiform\nTiglath-pileser III\nLIT\n['en']\n\n\n\nğ’€€ğ’ˆ¾ ğ’Œğ’‹—ğ’‹¼ ğ’€ğ’‹“ğ’ˆ  ğ’†³ GN ğ’€€ğ’ˆ¾ X\\nğ’Œ· ğ’†•ğ’‘ ğ’‚ğ’ƒ² ğ’ˆ¬ğ’‰ºğ’… X X X X X\\nğ’ˆ¬...\n111\n54\n\n\nQ003420\n0x08000000\nrinap/rinap1\nP463050\nBritish Museum, London, UK\nCreated by Hayim Tadmor, Shigeo Yamada, Jamie ...\n744-727\nTiglath-pileser III 07\nRINAP 1 Tigl. III 07\nAssyria\nBM 118933 (Layard, MS A pp. 111-112)\n...\nCuneiform\nTiglath-pileser III\nLIT\n['en']\n\n\n\nX X SN SN SN SN SN\\nX ğ’†³ğ’‚Š ğ’Š­ GN ğ’‹—ğ’‹› ğ’†³ğ’‚Š ğ’ƒ»ğ’†¥ğ’Œ… ğ’„‘ğ’€ğ’Œ… ğ’…ˆğ’† ...\n141\n115\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nQ009293\n0x08000000\nrinap/rinap5\n\nBritish Museum, London, UK\nCreated by Jamie Novotny and Joshua Jeffers, 2...\n668-ca. 631\nAshurbanipal 1025\nRINAP 5 Asb. 1025\nAssyria\nK 04498\n...\nCuneiform\nAshurbanipal\nLIT\n['en']\n\n\nCreated by Jamie Novotny and Joshua Jeffers, 2...\nX X ğ’‹—ğ’ˆ¨ğ’‹™ ğ’†ğ’‹¼ X\\nX X ğ’‹³ğ’‰Œğ’‚Š ğ’ˆ¬ğ’…† ğ’ƒ» X X\\nX X X X X X\\nX...\n38\n10\n\n\nQ009294\n0x08000000\nrinap/rinap5\n\nBritish Museum, London, UK\nCreated by Jamie Novotny and Joshua Jeffers, 2...\n668-ca. 631\nAshurbanipal 1026\nRINAP 5 Asb. 1026\nAssyria\nK 08361\n...\nCuneiform\nAshurbanipal\nLIT\n['en']\n\n\nCreated by Jamie Novotny and Joshua Jeffers, 2...\nX X X X X X X\\nX X\\nX X ğ’‹¾ğ’†· X X X X\\nX X ğ’† ğ’†—ğ’‹™ ğ’Œ‘ğ’ƒ»...\n84\n22\n\n\nQ009295\n0x08000000\nrinap/rinap5\n\nBritish Museum, London, UK\nCreated by Jamie Novotny and Joshua Jeffers, 2...\n668-ca. 631\nAshurbanipal 1027\nRINAP 5 Asb. 1027\nAssyria\nK 06681\n...\nCuneiform\nAshurbanipal\nLIT\n['en']\n\n\nCreated by Jamie Novotny and Joshua Jeffers, 2...\nX X X X X X X\\nX X ğ’† ğ’‚Šğ’‰¡ ğ’‚Šğ’Œ“ X X\\nğ’‚·ğ’†ª ğ’‡·ğ’€ªğ’Œ‹ ğ’€€ğ’„­ğ’„‘ ğ’‰Œğ’ˆ¨ğ’…… ...\n80\n39\n\n\nQ009297\n0x08000000\nrinap/rinap5\n\nBritish Museum, London, UK\nCreated by Jamie Novotny and Joshua Jeffers, 2...\n668-ca. 631\nAshurbanipal 1029\nRINAP 5 Asb. 1029\nAssyria\nBu 1891-05-09, 0204\n...\nCuneiform\nAshurbanipal\nLIT\n['en']\n\n\nCreated by Jamie Novotny and Joshua Jeffers, 2...\nX X X X X X X X X X X X X\\nX X X X X X X X X X...\n204\n23\n\n\nQ009298\n0x08000000\nrinap/rinap5\n\n\nCreated by Jamie Novotny and Joshua Jeffers, 2...\n668-ca. 631\nAshurbanipal 1030\nRINAP 5 Asb. 1030\nAssyria\nShikaft-i Gulgul rock relief\n...\nCuneiform\nAshurbanipal\nLIT\n['en']\n\n\nCreated by Jamie Novotny and Joshua Jeffers, 2...\nDN ğ’…‡ X X\\nX ğ’„¿ğ’² ğ’ˆ— X X\\nX X X\\nX X ğ’‹™ X X X X X X...\n183\n85\n\n\n\n\n308 rows Ã— 34 columns"
  },
  {
    "objectID": "notebooks/03_text_analysis.html",
    "href": "notebooks/03_text_analysis.html",
    "title": "Ancient Language Processing / Gordin, Lincke, and Mara",
    "section": "",
    "text": "Computational text analysis\nIn this notebook we will become more acquanited with the principles of computational text analysis and exploration, otherwise also known as quantitative text analysis (QTA). Let us start by acquiring some textual data:\n\n# downloading a text edition from the Electronic Babylonian Library (eBL)\n\nimport requests\n\nurl = 'https://www.ebl.lmu.de/1f5aa538-af19-49fd-9fca-a367bf63e433'\ndestination = 'L 1 1 Old Babylonian I.atf'\n\ndef download_file(url, destination):\n    response = requests.get(url)\n    with open(destination, 'wb') as file:\n        file.write(response.content)\n\ndownload_file(url, destination)\n\n\nfrom selenium import webdriver\nimport time\n\nurl = 'blob:https://www.ebl.lmu.de/1f5aa538-af19-49fd-9fca-a367bf63e433'\ndestination = 'L 1 1 Old Babylonian I.atf'\n\ndef download_blob(url, destination):\n    options = webdriver.ChromeOptions()\n    options.add_argument('--ignore-certificate-errors')\n    options.add_argument('--ignore-ssl-errors')\n    options.add_argument('--headless')  # Optional: Run in headless mode (without opening a browser window)\n\n    # Replace 'path/to/chromedriver' with the path to your Chrome WebDriver executable\n    driver = webdriver.Chrome(executable_path='path/to/chromedriver', options=options)\n\n    driver.get(url)\n\n    # Wait for some time for the content to load (adjust this as needed)\n    time.sleep(5)\n\n    # Save page source to a file\n    with open(destination, 'w') as file:\n        file.write(driver.page_source)\n\n    driver.quit()\n\ndownload_blob(url, destination)"
  },
  {
    "objectID": "notebooks/09_Vectors_to_Networks.html",
    "href": "notebooks/09_Vectors_to_Networks.html",
    "title": "Intro",
    "section": "",
    "text": "In this notebook we continue with vector space models, visualize our ancient datasets through network analysis, and analyse the networks using graph theory. How can we tap into the potential of the mathematics behind graph theory for textual and historical connections? This notebook will present a selection of fundamental terms and concepts in graph theory that are easily applicable to exploratory network analysis. Exploratory network analysis involves applying exploratory data analysis methods to network data. It encompasses a variety of statistical and visual techniques aimed at examining the structure of networks, including the positions of nodes and edges. These methods help identify specific structures or patterns of interest, such as central nodes, and provide an overview of the networkâ€™s structure to inform further analysis.\nThis notebook has been prepared by Eliese-Sophia Lincke and Shai Gordin based on the notebook and research prepared by Avital Romach for lesson 05, and drawing content from a tutorial on graph theory and network analysis of Shai Gordin. It should be cited accordingly (see citation information at the bottom)."
  },
  {
    "objectID": "notebooks/09_Vectors_to_Networks.html#imports",
    "href": "notebooks/09_Vectors_to_Networks.html#imports",
    "title": "Intro",
    "section": "Imports",
    "text": "Imports\n\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nimport requests\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom scipy.spatial.distance import pdist, squareform\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport plotly.express as px"
  },
  {
    "objectID": "notebooks/09_Vectors_to_Networks.html#functions",
    "href": "notebooks/09_Vectors_to_Networks.html#functions",
    "title": "Intro",
    "section": "Functions",
    "text": "Functions\n\nTo upload corpus and metadata from GitHub\n\nFunctions and import for the Akkadian corpus\nThe Akkadian corpus consists of a part of the Royal Inscriptions of the Neo-Assyrian Period (RINAP), licensed CC-BY-SA, and was taken from Open Richely Annotated Cuneiform Corpus (ORACC).\n\ndef create_corpus_from_github_api(url):\n  # URL on the Github where the csv files are stored\n  github_url = url\n  response = requests.get(github_url)\n\n  corpus = []\n  # Check if the request was successful\n  if response.status_code == 200:\n    files = response.json()\n    for file in files:\n      if file[\"download_url\"][-3:] == \"csv\":\n        corpus.append(pd.read_csv(file[\"download_url\"], encoding=\"utf-8\", index_col=\"Unnamed: 0\").fillna(\"\"))\n        # For Egyptian adapt like this:\n        #corpus.append(pd.read_csv(file[\"download_url\"], encoding=\"utf-8\").fillna(\"\"))\n  else:\n    print('Failed to retrieve files:', response.status_code)\n\n  return corpus\n\ndef get_metadata_from_raw_github(url):\n  metadata = pd.read_csv(url, encoding=\"utf-8\", index_col=\"Unnamed: 0\").fillna(\"\")\n  return metadata\n\n\n# Prepare Akkadian corpus (list of dataframes)\n\n  #corpus = create_corpus_from_github_api('https://api.github.com/repos/DigitalPasts/ALP-course/contents/course_notebooks/data/rinap01')\n  #corpus.extend(create_corpus_from_github_api('https://api.github.com/repos/DigitalPasts/ALP-course/contents/course_notebooks/data/rinap05'))\n\n\n# Prepare Akkadian metadata\n#metadata = get_metadata_from_raw_github(\"https://raw.githubusercontent.com/DigitalPasts/ALP-course/master/course_notebooks/data/rinap1_5_metadata.csv\")\n\n\n\nFunctions and import for the Egyptian corpus\nThe Egyptian corpus is an extract of the database of the Thesaurus Linguae Aegyptiae (TLA), containing literary (and if you like: medical) texts. This export from the database is not published under a free license. Therefore, we access it from a private GitHub repository using an access token.\n\ndef create_corpus_from_private_github_api(url, token):\n# URL on the Github where the csv files are stored\n    headers = {\n        \"Authorization\": f\"token {token}\"\n    }\n    github_url = url\n    response = requests.get(github_url, headers=headers)\n\n    dtype_dict = {\"lemma_id\": \"str\"}\n\n    corpus = []\n    # Check if the request was successful\n    if response.status_code == 200:\n        files = response.json()\n        for file in files:\n            if file[\"download_url\"][-3:] == \"csv\" or \".csv?token=\" in file[\"download_url\"]:\n                corpus.append(pd.read_csv(file[\"download_url\"], encoding=\"utf-8\", sep = ',', dtype=dtype_dict).fillna(\"\"))\n    else:\n        print('Failed to retrieve files:', response.status_code)\n\n    return corpus\n\nfrom io import StringIO\n\ndef get_metadata_from_raw_private_github(url, token):\n    headers = {\n        \"Authorization\": f\"token {token}\"\n    }\n    github_url = url\n    response = requests.get(github_url, headers=headers)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        csv_data = StringIO(response.text)\n        metadata = pd.read_csv(csv_data, encoding=\"utf-8\", sep = ',', index_col=\"text_id\").fillna(\"\")\n        return metadata\n    else:\n        raise Exception(f\"Failed to retrieve metadata: {response.status_code}\")\n\n\n# only if corpus is not yet loaded\n# Prepare Egyptian corpus (list sof dataframes)\n\nif True:\n\n  #tla_access_token = \"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\"\n  tla_access_token = \"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\"\n\n  ## TLA Literature\n  corpus = create_corpus_from_private_github_api('https://api.github.com/repos/thesaurus-linguae-aegyptiae/test-rawdata/contents/alp-course-2024/TLA_literature/erzaehlungen', tla_access_token)\n\n  corpus.extend(create_corpus_from_private_github_api('https://api.github.com/repos/thesaurus-linguae-aegyptiae/test-rawdata/contents/alp-course-2024/TLA_literature/reden', tla_access_token))\n\n  corpus.extend(create_corpus_from_private_github_api('https://api.github.com/repos/thesaurus-linguae-aegyptiae/test-rawdata/contents/alp-course-2024/TLA_literature/lehren', tla_access_token))\n\n  ## TLA Medical\n  #corpus = create_corpus_from_private_github_api('https://api.github.com/repos/thesaurus-linguae-aegyptiae/test-rawdata/contents/alp-course-2024/TLA_medical/TLA_pEbers', tla_access_token)\n\n  #corpus.extend(create_corpus_from_private_github_api('https://api.github.com/repos/thesaurus-linguae-aegyptiae/test-rawdata/contents/alp-course-2024/TLA_medical/TLA_pEdwinSmith', tla_access_token))\n\n\n# Egyptian metadata\nmetadata = get_metadata_from_raw_private_github(\"https://raw.githubusercontent.com/thesaurus-linguae-aegyptiae/test-rawdata/master/alp-course-2024/TLA_literature/TLA_metadata.csv\", tla_access_token)\n\n\ncorpus[0].head()\n\n\n    \n\n\n\n\n\n\ntext\nline\nword\nref\nfrag\nnorm\nunicode_word\nlemma_id\ncf\npos\nmask\nsense\ninst\nreading\nbreak_perc\nunicode\nbreak\n\n\n\n\n0\n2FPMTQIP45HI3LODGNIZFDO77I\n1\n2\nIBUBdxQqwvMcu0CovD1Q5OKc7B8\n[â€¦]\n[â€¦]\n[â€¦]\n\n\n\n\n\n\n\n1.0\n['â€¦']\n['missing']\n\n\n1\n2FPMTQIP45HI3LODGNIZFDO77I\n1\n3\nIBUBd1Mxn1WtEUULim1Z51mT3oc\nqd\nqd\nğ“ªğ“‚§ğ“Œğ“²ğ“»ğ“‘•\n162450\nqdd\nVERB\n\nschlafen\n\n\n0.1\n['ğ“ª', 'ğ“‚§', 'ğ“Œ', 'ğ“²', 'ğ“»']\n['complete', 'complete', 'complete', 'complete...\n\n\n2\n2FPMTQIP45HI3LODGNIZFDO77I\n1\n4\nIBUBd9E0G7Z51UgbrcPup3GC3iY\n[â€¦]\n[â€¦]\n[â€¦]\n\n\n\n\n\n\n\n1.0\n['â€¦']\n['missing']\n\n\n3\n2FPMTQIP45HI3LODGNIZFDO77I\n1\n6\nIBUBd9Ae0nMTckagsrRI5pGbPxQ\n[â€¦]\n[â€¦]\n[â€¦]\n\n\n\n\n\n\n\n1.0\n['â€¦']\n['missing']\n\n\n4\n2FPMTQIP45HI3LODGNIZFDO77I\n1\n7\nIBUBd5FjpDQ83E60jslkgkHSx4Y\n[â€¢]\nâ€¢\nâ€¢\n\nâ€¢\nPUNCT\n\n\n\n\n1.0\n['â€¢']\n['missing']\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# Prepare text_ids (list of unique ids), and metadata\n\ntext_ids = []\nfor text in corpus:\n  text_ids.append(text[\"text\"].iloc[0])\n\n\nfor id in text_ids:\n  if id not in metadata.index:\n    print(f\"Text {id} missing from metadata\")\n\nmetadata = metadata[metadata.index.isin(text_ids)]\n\nmetadata\n\n\n    \n\n\n\n\n\n\ntext_name\ncorpus_manual\npath\ndateEarliest\ndateLatest\nlanguage_manual\ntlaTextLangName\ntlaTextScriptName\ncontributors\n\n\ntext_id\n\n\n\n\n\n\n\n\n\n\n\n\n\nIMCRUSHSQ5HBHBGS3XQ2WYI3HM\npVandier = pLille 139 || Recto: Meryre und Sis...\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nFrÃ¼hdemotisch\nFrÃ¼hdemotisch\nSpÃ¤thieratische Buchschrift\n['Lutz Popko', 'AltÃ¤gyptisches WÃ¶rterbuch', 'J...\n\n\n2AT7OVM3PZDEDMYZVBIZGYAP44\npBrooklyn 47.218.135 || Brooklyner Weisheitstext\nLehren\nsawlit || Literarische Texte || 3. Weisheitsle...\n\n\nFrÃ¼hdemotisch\nFrÃ¼hdemotisch\nSpÃ¤thieratische Buchschrift\n['Peter Dils', 'AltÃ¤gyptisches WÃ¶rterbuch', 'L...\n\n\n62AD3D3IY5EMTHDDJZ5UV42RWM\nhintere Innenwand || Das Buch von der Himmelsk...\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nMittelÃ¤gyptisch\nfrÃ¼hes und klassisches MittelÃ¤gyptisch\nregulÃ¤re Hieroglyphenschrift\n['Lutz Popko', 'AltÃ¤gyptisches WÃ¶rterbuch', 'J...\n\n\nEQTNMAUWUBDP5D7K2UJLHOZO5I\nhintere Innenwand || Rahmentext\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nMittelÃ¤gyptisch\nfrÃ¼hes und klassisches MittelÃ¤gyptisch\nregulÃ¤re Hieroglyphenschrift\n['Lutz Popko', 'AltÃ¤gyptisches WÃ¶rterbuch', 'J...\n\n\n5EGJISAVIVHCLJE2JUKHR23V6E\nlinke Innenwand || Rahmentext\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nMittelÃ¤gyptisch\nfrÃ¼hes und klassisches MittelÃ¤gyptisch\nregulÃ¤re Hieroglyphenschrift\n['Lutz Popko', 'AltÃ¤gyptisches WÃ¶rterbuch', 'B...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nC6KGH3XC7RGU3DSL7HKYY2K3WM\npTurin CGT 54014, falsch CGT 54024 || Die Lehr...\nLehren\nsawlit || Literarische Texte || 3. Weisheitsle...\n\n\nMittelÃ¤gyptisch\ntraditionelles MittelÃ¤gyptisch\nNeuhieratische Buchschrift\n['Peter Dils', 'AltÃ¤gyptisches WÃ¶rterbuch', 'B...\n\n\nWOAIM6KKGJDJ5OUDFHRFJ542NM\n01. tBerlin 8934 (tB) || Die Lehre des Ani (Ve...\nLehren\nsawlit || Literarische Texte || 3. Weisheitsle...\n\n\nNeuÃ¤gyptisch\ntraditionelles MittelÃ¤gyptisch\nSpÃ¤thieratische Buchschrift\n['Peter Dils', 'AltÃ¤gyptisches WÃ¶rterbuch', 'J...\n\n\n2Z4GIGWZJVEMZO37R4ZP7RIARE\npChassinat II = pLouvre E 25352 || Die Geister...\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nTraditionsÃ¤gyptisch\ntraditionelles MittelÃ¤gyptisch\nSpÃ¤thieratische Buchschrift\n['Peter Dils', 'AltÃ¤gyptisches WÃ¶rterbuch', 'F...\n\n\nYWJMWQT65NDTPL6MPG6TLVJ5MI\n03. pChassinat I = pLouvre E 25351 || Die Gesc...\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nTraditionsÃ¤gyptisch\ntraditionelles MittelÃ¤gyptisch\nSpÃ¤thieratische Buchschrift\n['Peter Dils', 'AltÃ¤gyptisches WÃ¶rterbuch', 'F...\n\n\n4VLZLA44UVGJZN22WIWP774LOQ\nStele Louvre C 284 (\"Bentresch-Stele\") || Stel...\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nTraditionsÃ¤gyptisch\ntraditionelles MittelÃ¤gyptisch\nregulÃ¤re Hieroglyphenschrift\n['Lutz Popko', 'AltÃ¤gyptisches WÃ¶rterbuch', 'J...\n\n\n\n\n209 rows Ã— 9 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\n\nTo convert dataframe to string\nFunction to split the text dataframes according to a column. Used to separate text to lines.\n\ndef split_df_by_column_value(df, column):\n\n    dfs = []\n    column_values = df[column].unique()\n    for value in column_values:\n        split_df = df[df[column]==value]\n        dfs.append(split_df)\n    return dfs\n\n\nsplit_df_by_column_value(corpus[0].head(), \"line\")\n\n[                         text  line  word                          ref frag  \\\n 0  2FPMTQIP45HI3LODGNIZFDO77I     1     2  IBUBdxQqwvMcu0CovD1Q5OKc7B8  [â€¦]   \n 1  2FPMTQIP45HI3LODGNIZFDO77I     1     3  IBUBd1Mxn1WtEUULim1Z51mT3oc   qd   \n 2  2FPMTQIP45HI3LODGNIZFDO77I     1     4  IBUBd9E0G7Z51UgbrcPup3GC3iY  [â€¦]   \n 3  2FPMTQIP45HI3LODGNIZFDO77I     1     6  IBUBd9Ae0nMTckagsrRI5pGbPxQ  [â€¦]   \n 4  2FPMTQIP45HI3LODGNIZFDO77I     1     7  IBUBd5FjpDQ83E60jslkgkHSx4Y  [â€¢]   \n \n   norm unicode_word lemma_id   cf    pos mask     sense inst reading  \\\n 0  [â€¦]          [â€¦]                                                    \n 1   qd       ğ“ªğ“‚§ğ“Œğ“²ğ“»ğ“‘•   162450  qdd   VERB       schlafen                \n 2  [â€¦]          [â€¦]                                                    \n 3  [â€¦]          [â€¦]                                                    \n 4    â€¢            â€¢             â€¢  PUNCT                               \n \n    break_perc                    unicode  \\\n 0         1.0                      ['â€¦']   \n 1         0.1  ['ğ“ª', 'ğ“‚§', 'ğ“Œ', 'ğ“²', 'ğ“»']   \n 2         1.0                      ['â€¦']   \n 3         1.0                      ['â€¦']   \n 4         1.0                      ['â€¢']   \n \n                                                break  \n 0                                        ['missing']  \n 1  ['complete', 'complete', 'complete', 'complete...  \n 2                                        ['missing']  \n 3                                        ['missing']  \n 4                                        ['missing']  ]\n\n\nFunction to convert the values from the text dataframe to a string of text with or without line breaks and word segmentation.\n\ndef df2str(df, column, break_perc=1, mask=True, segmentation=True):\n\n    # check if column exists in dataframe. If not, return empty text.\n    if column not in df.columns:\n        return (\"\", 0, 0)\n    else:\n        # remove rows that include duplicate values for compound words\n        if column not in [\"norm\", \"cf\", \"sense\", \"pos\"]:\n            df = df.drop_duplicates(\"ref\").copy()\n        # if column entry is empty string, replace with UNK (can happen with normalization or lemmatization)\n        mask_empty = df[column]==\"\"\n        df[column] = df[column].where(~mask_empty, other=\"UNK\")\n        # mask proper nouns\n        if mask and \"pos\" in df.columns:\n            mask_bool = df[\"pos\"].isin([\"PN\", \"RN\", \"DN\", \"GN\", \"MN\", \"SN\", \"n\"])\n            df[column] = df[column].where(~mask_bool, other=df[\"pos\"])\n\n        # change number masking from `n` to `NUM`\n        # !comment out for Egyptian\n        #if mask:\n        #    mask_num = df[column]==\"n\"\n        #    df[column] = df[column].where(~mask_num, other=\"NUM\")\n\n        # remove rows without break_perc (happens with non-Akkadian words)\n        if \"\" in df[\"break_perc\"].unique():\n            df = df[df[\"break_perc\"]!=\"\"].copy()\n        # filter according to break_perc\n        mask_break = df[\"break_perc\"] &lt;= break_perc\n        df[column] = df[column].where(mask_break, other=\"X\")\n        # calculate text length with and without UNK and x tokens\n        text_length_full = df.shape[0]\n        mask_partial = df[column].isin([\"UNK\", \"X\", \"x\"])\n        text_length_partial = text_length_full - sum(mask_partial)\n        # create text lines\n        text = \"\"\n        df_lines = split_df_by_column_value(df, \"line\")\n        for line in df_lines:\n            word_list = list(filter(None, line[column].to_list()))\n            if word_list != []:\n                text += \" \".join(map(str, word_list)).replace(\"x\", \"X\").strip() + \"\\n\"\n\n        if segmentation == False:\n            # remove all white spaces (word segmentation and line breaks)\n            text = re.sub(r\"[\\s\\u00A0]+\", \"\", text)\n\n        return (text, text_length_full, text_length_partial)\n\n\ndf2str(corpus[0], \"cf\")\n\n('UNK qdd UNK UNK â€¢\\nê½w â¸—ê½ UNK r gmiÌ¯ UNK\\nUNK n êœ¥ná¸« UNK têœ£ rmá¹¯.t â€¢\\nê½w â¸—s á¸¥r __ n â¸—ê½ UNK UNK qdd â€¢ ê½w â¸—ê½ priÌ¯ n â¸—á¹¯ r-bnr â€¢\\nê½w UNK â€¢\\nê½w UNK UNK zbiÌ¯ n â¸—ê½ m qdd â€¢\\nê½w â¸—s á¸¥r á¸d n â¸—ê½ â€¢\\nê½á¸« tr pw-tr ê½riÌ¯ â¸—á¹¯n r UNK â€¢ ê½ â¸—s á¸¥r á¸d n â¸—ê½\\nUNK pêœ£ UNK ê½riÌ¯ á¸¥r sá¸ â€¢\\nê½w â¸—f á¸¥r pnn znf r ê½tn â€¢\\nê½w â¸—ê½ UNK â€¢\\nê½w â¸—ê½ á¸¥r rmiÌ¯ r êœ¥êœ£.t wr â€¢\\nê½w pêœ£ á¸«r r-á¸r â¸—f UNK\\nUNK UNK â¸—s UNK\\n',\n 105,\n 85)\n\n\n\n\nTo convert to specific word levels and create dictionaries\nFunction to convert the dataframes into strings of lemmatized texts.\n\ndef get_lemmatized_texts(corpus, break_perc=1, mask=True):\n\n    texts_dict = {}\n    for df in corpus:\n        # get the text number from the dataframe \"text\" column\n        key = df[\"text\"].iloc[0]\n        text, text_length_full, text_length_partial = df2str(df, \"lemma_id\", break_perc, mask)\n        texts_dict[key] = (text, text_length_full, text_length_partial)\n    return texts_dict\n\n\nget_lemmatized_texts((split_df_by_column_value(corpus[0], \"text\")))\n\n{'2FPMTQIP45HI3LODGNIZFDO77I': ('UNK 162450 UNK UNK UNK\\n851513 10030 UNK 91909 167210 UNK\\nUNK 78900 38540 UNK 168850 94550 UNK\\n851513 10090 107529 850836 78870 10030 UNK UNK 162450 UNK 851512 10030 60920 78873 10120 91970 UNK\\n851513 UNK UNK\\n851513 UNK UNK 131460 78873 10030 64362 162650 UNK\\n851513 10090 107529 185810 400055 10030 UNK\\n30730 172720 500027 28550 10130 91902 UNK UNK 500024 10090 107529 185810 400055 10030\\nUNK 58770 UNK 28550 107529 150110 UNK\\n851513 10050 107529 60030 137250 91901 33120 UNK\\n851513 10030 UNK UNK\\n851513 10030 107529 94180 91903 34860 47271 UNK\\n851513 58770 119620 92500 10050 UNK\\nUNK UNK 10090 UNK\\n',\n  105,\n  73)}\n\n\nFunction to convert the dataframes into strings of normalized texts.\n\ndef get_normalized_texts(corpus, break_perc=1, mask=True):\n\n    texts_dict = {}\n    for df in corpus:\n        # get the text number from the dataframe \"text\" column\n        key = df[\"text\"].iloc[0]\n        text, text_length_full, text_length_partial = df2str(df, \"norm\", break_perc, mask)\n        texts_dict[key] = (text, text_length_full, text_length_partial)\n    return texts_dict\n\n\nget_normalized_texts((split_df_by_column_value(corpus[0], \"text\")))\n\n{'2FPMTQIP45HI3LODGNIZFDO77I': ('[â€¦] qd [â€¦] [â€¦] â€¢\\nê½w â¸—ê½ [â€¦] r gmiÌ¯ [â€¦]\\n[â€¦] n êœ¥ná¸« [â€¦] têœ£ rmá¹¯.t â€¢\\nê½w â¸—s á¸¥r __ n â¸—ê½ [â€¦] [â€¦] qd â€¢ ê½w â¸—ê½ priÌ¯ n â¸—t r-bw~n~r â€¢\\nê½w [â€¦] â€¢\\nê½w [â€¦] [â€¦] sbê½.t n â¸—ê½ m qd â€¢\\nê½w â¸—s á¸¥r á¸d n â¸—ê½ â€¢\\nê½á¸« trê½ ptê½ ê½rr â¸—tn r [â€¦] â€¢ ê½ â¸—s á¸¥r á¸d n â¸—ê½\\n[â€¦] pêœ£ [â€¦] ê½riÌ¯.t á¸¥r sd.t â€¢\\nê½w â¸—f á¸¥r pnw snf r ê½wtn â€¢\\nê½w â¸—ê½ [â€¦] â€¢\\nê½w â¸—ê½ á¸¥r rmiÌ¯ r êœ¥êœ£.t wr.t â€¢\\nê½w pêœ£ á¸«r r-á¸r â¸—f [â€¦]\\n[â€¦] __ â¸—s [â€¦]\\n',\n  105,\n  105)}\n\n\nFunction to convert the dataframes into strings of segmented unicode texts.\n\ndef get_segmented_unicode_texts(corpus, break_perc=1, mask=True):\n\n    texts_dict = {}\n    for df in corpus:\n        # get the text number from the dataframe \"text\" column\n        key = df[\"text\"].iloc[0]\n        text, text_length_full, text_length_partial = df2str(df, \"unicode_word\", break_perc, mask)\n        texts_dict[key] = (text, text_length_full, text_length_partial)\n    return texts_dict\n\n\nget_segmented_unicode_texts((split_df_by_column_value(corpus[0], \"text\")))\n\n{'2FPMTQIP45HI3LODGNIZFDO77I': ('[â€¦] ğ“ªğ“‚§ğ“Œğ“²ğ“»\\U00013455 [â€¦] [â€¦] â€¢\\nğ“‡‹ğ“² ğ“€€ [â€¦] ğ“‚‹ ğ“… ğ“…“[â€¦] [â€¦]\\n[â€¦] ğ“ˆ– ğ“‹¹ğ“ˆ–ğ“ [â€¦] ğ“ğ“„¿ ğ“‚‹ğ“€ğ“€€ğ“ğ“¥ â€¢\\nğ“‡‹ğ“² ğ“‹´ ğ“·ğ“¤ [â€¦] ğ“ˆ– ğ“€€ [â€¦] [â€¦] [â€¦]ğ“‚§\\U00013455ğ“Œ\\U00013455ğ“²ğ“» â€¢ ğ“‡‹ğ“² ğ“€€ ğ“‰ğ“‚‹ğ“‚» ğ“ˆ– ğ“€€ ğ“‚‹ğ“ƒ€ğ“²ğ“ˆ–ğ“¥ğ“‚‹ğ“ˆğ“‚» â€¢\\nğ“‡‹ğ“² [â€¦] â€¢\\nğ“‡‹ğ“² [â€¦] [â€¦] [â€¦]ğ“ƒ€ğ“‡‹ğ“ğ“‚» ğ“ˆ– ğ“€€ ğ“…“ ğ“ªğ“‚§ğ“Œğ“²ğ“» â€¢\\nğ“‡‹ğ“² ğ“‹´ ğ“·ğ“¤ ğ“†“ğ“‚§ ğ“ˆ– ğ“€€ â€¢\\nğ“‡‹ğ“ğ“› ğ“ğ“‚‹ğ“‡‹ğ“†µğ“› ğ“Šªğ“ğ“‡‹ğ“€ ğ“¹ğ“‚‹ ğ“ğ“ˆ–ğ“¥ ğ“‚‹ [â€¦] â€¢ ğ“‡‹ğ“€ ğ“‹´ ğ“·ğ“¤ ğ“†“ğ“‚§ ğ“ˆ– ğ“€€\\n[â€¦] ğ“…¯[â€¦] [â€¦] ğ“¹ğ“‚‹ğ“ ğ“·ğ“¤ ğ“‹´ğ“‚§ğ“ğ“´ğ“¥ â€¢\\nğ“‡‹ğ“² ğ“†‘ UNK ğ“Šªğ“ˆ–ğ“Œğ“²ğ“´ğ“›ğ“« ğ“Šƒğ“ˆ–ğ“†‘ğ“‚ğ“¥ ğ“‚‹ ğ“ƒ›ğ“²ğ“ğ“ˆ–[â€¦]ğ“ˆ‡ğ“¤ â€¢\\nğ“‡‹ğ“² ğ“€€ [â€¦] â€¢\\nğ“‡‹ğ“² ğ“€€ ğ“·ğ“¤ ğ“‚‹ğ“…“ğ“²ğ“¿ ğ“‚‹ ğ“‰»ğ“ğ“› ğ“…¨ğ“‚‹ğ“ â€¢\\nğ“‡‹ğ“² ğ“…¯ğ“„¿ ğ“ğ“‚‹ğ“ğ“‰ğ“¤ ğ“‚‹ğ“‡¥ğ“‚‹\\U00013455 [â€¦] [â€¦]\\n[â€¦] [â€¦]ğ“»\\U00013455[â€¦]ğ“«\\U00013455 ğ“‹´\\U00013455 [â€¦]\\n',\n  105,\n  104)}\n\n\n\n\nTo create the vector space model\n\nvectorizing texts\nConverts a list of texts into a term-document matrix based on TF-IDF scores.\n\nFigure 1. Example of a document-term matrix extracted from a corpus, see Fig. 3 in Karsdorp, F., Kestemont, M., & Riddell, A. (2021). Humanities Data Analysis: Case Studies with Python. Princeton University Press.\n\ndef vectorize(corpus, analyzer=\"word\", ngram_range=(1,1), max_df=1.0, min_df=1, max_features=None, stop_words=[\"UNK\", \"X\"]):\n\n    vectorizer = TfidfVectorizer(input=\"content\", lowercase=False, analyzer=analyzer,\n                                 # RegEx for Akkadian\n                                 #token_pattern=r\"(?u)\\b\\w+\\b\", ngram_range=ngram_range,\n                                 # RegEx for Egyptian\n                                 token_pattern=r\"(?u)\\b[\\w\\.]+\\b\", ngram_range=ngram_range,\n                                 max_df=max_df, min_df=min_df, max_features=max_features, stop_words=stop_words)\n\n    counts = vectorizer.fit_transform(corpus[\"text\"].tolist()).toarray()\n    stop_words = vectorizer.stop_words_\n\n    # saving the vocab used for vectorization, and switching the dictionary so that the feature index is the key\n    vocab = vectorizer.vocabulary_\n    switched_vocab = {value: key for key, value in vocab.items()}\n    # adding the vocab words to the counts dataframe for easier viewing.\n    column_names = []\n    x = 0\n    while x &lt; len(switched_vocab):\n        column_names.append(switched_vocab[x])\n        x += 1\n\n    counts_df = pd.DataFrame(counts, index=corpus.index, columns=column_names)\n\n    return (counts, counts_df, stop_words)\n\n\n\ncalculating distances between vectorized documents\nConverts a term-document matrix to a text similarity matrix.\n\ndef distance_calculator(counts, metric, text_ids):\n\n    return pd.DataFrame(squareform(pdist(counts, metric=metric)), index=text_ids, columns=text_ids)\n\n\n\nreducing dimensions with pca or tsne\nReduces multidimensional data into two dimensions using PCA.\n\ndef reduce_dimensions_pca(df, metadata):\n\n    pca = PCA(n_components=2)\n    reduced_data = pca.fit_transform(df)\n    reduced_df = pd.DataFrame(data=reduced_data, index=df.index, columns=[\"component 1\", \"component 2\"])\n    reduced_df_metadata = metadata.join(reduced_df)\n    return reduced_df_metadata\n\nReduces multidimensional data into two dimensions using TSNE.\n\ndef reduce_dimensions_tsne(df, perplexity, n_iter, metric, metadata):\n\n    tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=n_iter, metric=metric, init=\"pca\")\n    reduced_data = tsne.fit_transform(df)\n    reduced_df = pd.DataFrame(data=reduced_data, index=df.index, columns=[\"component 1\", \"component 2\"])\n    reduced_df_metadata = metadata.join(reduced_df)\n    return reduced_df_metadata"
  },
  {
    "objectID": "notebooks/09_Vectors_to_Networks.html#process-texts-from-dataframes-and-combine-results-with-metadata-dataframe",
    "href": "notebooks/09_Vectors_to_Networks.html#process-texts-from-dataframes-and-combine-results-with-metadata-dataframe",
    "title": "Intro",
    "section": "Process texts from dataframes and combine results with metadata dataframe",
    "text": "Process texts from dataframes and combine results with metadata dataframe\n\n# Function to combine processed texts with metadata\n\ndef get_corpus_metadata(texts_dict, metadata):\n  texts_df = pd.DataFrame(texts_dict, index=[\"text\", \"full_length\", \"partial_length\"]).transpose()\n  df = metadata.join(texts_df)\n  return df\n\n\n## vectorize lemma forms\ncorpus_dict = get_lemmatized_texts(corpus, break_perc=0)\n## vectorize normalized forms\n# corpus_dict = get_normalized_texts(corpus, break_perc=0)\n## vectorize Unicode cuneiform\n# corpus_dict = get_segmented_unicode_texts(corpus, break_perc=0)\n\ncorpus_metadata = get_corpus_metadata(corpus_dict, metadata)\n\n## For Akkadian\n## remove texts which have less than n words excluding UNK and X\n#n = 10\n#print(f\"Number of texts before filtering: {corpus_metadata.shape[0]}\")\n#corpus_metadata = corpus_metadata[corpus_metadata[\"partial_length\"]&gt;=n]\n#print(f\"Number of texts after filtering: {corpus_metadata.shape[0]}\")\n\n\n# For Egyptian use this instead, reset the index\nn = 50\nprint(f\"Number of texts before filtering: {corpus_metadata.shape[0]}\")\ncorpus_metadata = corpus_metadata[corpus_metadata[\"partial_length\"]&gt;=n].set_index(\"text_name\")\nprint(f\"Number of texts after filtering: {corpus_metadata.shape[0]}\")\n\nNumber of texts before filtering: 209\nNumber of texts after filtering: 124\n\n\n\ncorpus_metadata\n\n\n    \n\n\n\n\n\n\ncorpus_manual\npath\ndateEarliest\ndateLatest\nlanguage_manual\ntlaTextLangName\ntlaTextScriptName\ncontributors\ntext\nfull_length\npartial_length\n\n\ntext_name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npVandier = pLille 139 || Recto: Meryre und Sisobek\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nFrÃ¼hdemotisch\nFrÃ¼hdemotisch\nSpÃ¤thieratische Buchschrift\n['Lutz Popko', 'AltÃ¤gyptisches WÃ¶rterbuch', 'J...\n28160 600043 550055 X 64365 58770 X 78900 8806...\n3393\n1811\n\n\npBrooklyn 47.218.135 || Brooklyner Weisheitstext\nLehren\nsawlit || Literarische Texte || 3. Weisheitsle...\n\n\nFrÃ¼hdemotisch\nFrÃ¼hdemotisch\nSpÃ¤thieratische Buchschrift\n['Peter Dils', 'AltÃ¤gyptisches WÃ¶rterbuch', 'L...\nX X 851523 116230 X\\nX X 78030\\n55210 X X\\nX 1...\n2647\n1857\n\n\nhintere Innenwand || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nMittelÃ¤gyptisch\nfrÃ¼hes und klassisches MittelÃ¤gyptisch\nregulÃ¤re Hieroglyphenschrift\n['Lutz Popko', 'AltÃ¤gyptisches WÃ¶rterbuch', 'J...\n79090 116800 400007 94530 90260 203 69320 4000...\n234\n225\n\n\nhintere Innenwand || Rahmentext\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nMittelÃ¤gyptisch\nfrÃ¼hes und klassisches MittelÃ¤gyptisch\nregulÃ¤re Hieroglyphenschrift\n['Lutz Popko', 'AltÃ¤gyptisches WÃ¶rterbuch', 'J...\n38530 49461 88040 400038 500068 126020 72470 1...\n71\n71\n\n\nlinke Innenwand || Rahmentext\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nMittelÃ¤gyptisch\nfrÃ¼hes und klassisches MittelÃ¤gyptisch\nregulÃ¤re Hieroglyphenschrift\n['Lutz Popko', 'AltÃ¤gyptisches WÃ¶rterbuch', 'B...\nX 186050 400042 33040 111230 10050 64360 38540...\n79\n71\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16. oDeM 1013 || Die Lehre des Cheti 23.1-25.3\nLehren\nsawlit || Literarische Texte || 3. Weisheitsle...\n\n\nMittelÃ¤gyptisch\ntraditionelles MittelÃ¤gyptisch\nNeuhieratische Buchschrift\n['Peter Dils', 'AltÃ¤gyptisches WÃ¶rterbuch', 'B...\n185810 X X X X X 78030 X X 10110 95620\\n90120 ...\n129\n97\n\n\n17. oTurin CGT 57082 || Die Lehre des Cheti 25.1-29.1\nLehren\nsawlit || Literarische Texte || 3. Weisheitsle...\n\n\nMittelÃ¤gyptisch\ntraditionelles MittelÃ¤gyptisch\nNeuhieratische Buchschrift\n['Peter Dils', 'AltÃ¤gyptisches WÃ¶rterbuch', 'B...\nX X X X X 10110\\nX X X UNK 79800 X X\\nX X X X ...\n228\n106\n\n\npChassinat II = pLouvre E 25352 || Die Geistergeschichte des pChassinat II\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nTraditionsÃ¤gyptisch\ntraditionelles MittelÃ¤gyptisch\nSpÃ¤thieratische Buchschrift\n['Peter Dils', 'AltÃ¤gyptisches WÃ¶rterbuch', 'F...\nX X X 10050 X X UNK X X\\nX X X 96700 10050 X X...\n160\n87\n\n\n03. pChassinat I = pLouvre E 25351 || Die Geschichte von KÃ¶nig Neferkare und General Sasenet\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nTraditionsÃ¤gyptisch\ntraditionelles MittelÃ¤gyptisch\nSpÃ¤thieratische Buchschrift\n['Peter Dils', 'AltÃ¤gyptisches WÃ¶rterbuch', 'F...\nX X X X X 400043 X\\nX 10050 107529 77540 X X X...\n331\n254\n\n\nStele Louvre C 284 (\"Bentresch-Stele\") || Stele Louvre C 284 (\"Bentresch-Stele\")\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nTraditionsÃ¤gyptisch\ntraditionelles MittelÃ¤gyptisch\nregulÃ¤re Hieroglyphenschrift\n['Lutz Popko', 'AltÃ¤gyptisches WÃ¶rterbuch', 'J...\n850566 90360 400833\\n850566 90360 400833\\n8600...\n933\n904\n\n\n\n\n124 rows Ã— 11 columns"
  },
  {
    "objectID": "notebooks/09_Vectors_to_Networks.html#creating-a-network-visualization-from-the-distance-measures-of-the-tf-idf-matrix",
    "href": "notebooks/09_Vectors_to_Networks.html#creating-a-network-visualization-from-the-distance-measures-of-the-tf-idf-matrix",
    "title": "Intro",
    "section": "Creating a network visualization from the distance measures of the tf-idf matrix",
    "text": "Creating a network visualization from the distance measures of the tf-idf matrix\n\n## Imports\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap, to_hex\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n\ndef filter_top_similarities(distance_df, top_n=3):\n    filtered_edges = []\n    for text in matrix.index:\n        # Get the top N smallest distances (excluding zero distances to avoid self-loops)\n        top_similarities = matrix[text].nsmallest(top_n + 1).iloc[1:]  # Exclude the text itself\n        for similar_text, similarity in top_similarities.items():\n            filtered_edges.append((text, similar_text, similarity))\n    return filtered_edges\n\n# Assuming distance_df is your distance DataFrame\nfiltered_edges = filter_top_similarities(matrix)\n\nfiltered_edges\n\n[('pVandier = pLille 139 || Recto: Meryre und Sisobek',\n  'pBrooklyn 47.218.135 || Brooklyner Weisheitstext',\n  0.17552768662452056),\n ('pVandier = pLille 139 || Recto: Meryre und Sisobek',\n  'pMoskau 120 || Recto: Die Reise des Wenamun',\n  0.20860098412578387),\n ('pVandier = pLille 139 || Recto: Meryre und Sisobek',\n  'pChassinat III = pLouvre E 25353 || Text 1: Mythologische ErzÃ¤hlung',\n  0.2278307465446736),\n ('pBrooklyn 47.218.135 || Brooklyner Weisheitstext',\n  'pVandier = pLille 139 || Recto: Meryre und Sisobek',\n  0.17552768662452056),\n ('pBrooklyn 47.218.135 || Brooklyner Weisheitstext',\n  '1. pBM EA 10474 || Rto: Die Lehre des Amenemope',\n  0.1790645985832855),\n ('pBrooklyn 47.218.135 || Brooklyner Weisheitstext',\n  'pChassinat III = pLouvre E 25353 || Text 1: Mythologische ErzÃ¤hlung',\n  0.19336656873630553),\n ('hintere Innenwand || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)',\n  '01. sKairo CG 20538 des Sehetep-ib-Re || Die Loyalistische Lehre des Kairsu 1.1-6.10',\n  0.23484601748002765),\n ('hintere Innenwand || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)',\n  '03. pLouvre E 4864 || Recto: Die Loyalistische Lehre des Kairsu 3.6-14.11',\n  0.24235302998718633),\n ('hintere Innenwand || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)',\n  '01. pSallier II = pBM EA 10182 || Kol. 3-11: Die Lehre des Cheti 1.1-30.6',\n  0.24418709556773954),\n ('hintere Innenwand || Rahmentext',\n  'oDeM 1632 || Eine Sammlung von Verboten',\n  0.09879212266110149),\n ('hintere Innenwand || Rahmentext',\n  'oPetrie 11 = oLondon UC 39614 || Eine Sammlung von Verboten',\n  0.11379246347055172),\n ('hintere Innenwand || Rahmentext',\n  '09. pDeM 1 (D) || Recto: Die Lehre des Ani (Version D)',\n  0.19196937307864093),\n ('linke Innenwand || Rahmentext',\n  'pRamesseum A = pBerlin P 10499 (Bauer, R) || Recto: Der beredte Bauer (Version R)',\n  0.2666158516682342),\n ('linke Innenwand || Rahmentext',\n  '01. sKairo CG 20538 des Sehetep-ib-Re || Die Loyalistische Lehre des Kairsu 1.1-6.10',\n  0.27280299810784814),\n ('linke Innenwand || Rahmentext',\n  \"KV 9: Grab Ramses' VI., Korridor C, Nische am Ende der linken Wand || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\",\n  0.3136496902110715),\n (\"KV 17: Grab Sethos' I., vordere rechte Seitenkammer der Sargkammer || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\",\n  'pRamesseum A = pBerlin P 10499 (Bauer, R) || Recto: Der beredte Bauer (Version R)',\n  0.14937480669172876),\n (\"KV 17: Grab Sethos' I., vordere rechte Seitenkammer der Sargkammer || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\",\n  'pMoskau o.Nr. + pMoskau 167 || Mythologische Geschichte',\n  0.17983328688801659),\n (\"KV 17: Grab Sethos' I., vordere rechte Seitenkammer der Sargkammer || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\",\n  'oAshmolean Museum 1945.40 aus Deir el Medineh (AOS) || Sinuhe Koch 01-79',\n  0.19367620005713126),\n (\"KV 9: Grab Ramses' VI., Korridor C, Nische am Ende der linken Wand || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\",\n  '01. sKairo CG 20538 des Sehetep-ib-Re || Die Loyalistische Lehre des Kairsu 1.1-6.10',\n  0.16938358525531327),\n (\"KV 9: Grab Ramses' VI., Korridor C, Nische am Ende der linken Wand || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\",\n  '08. oAshmolean 1938.912 || Die Loyalistische Lehre des Kairsu 5.1-8.1',\n  0.19675135303318336),\n (\"KV 9: Grab Ramses' VI., Korridor C, Nische am Ende der linken Wand || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\",\n  'pCarlsberg VI || Die Lehre fÃ¼r Merikare',\n  0.21150172940808387),\n ('pWestcar = pBerlin P 3033 || Die ErzÃ¤hlungen des pWestcar',\n  'Stele Louvre C 284 (\"Bentresch-Stele\") || Stele Louvre C 284 (\"Bentresch-Stele\")',\n  0.15020921968958967),\n ('pWestcar = pBerlin P 3033 || Die ErzÃ¤hlungen des pWestcar',\n  'pLythgoe = pMMA 09.180.535 || Die Geschichte des pLythgoe',\n  0.1531979048005746),\n ('pWestcar = pBerlin P 3033 || Die ErzÃ¤hlungen des pWestcar',\n  'pRamesseum A = pBerlin P 10499 (Bauer, R) || Recto: Der beredte Bauer (Version R)',\n  0.2330292342516631),\n ('pLythgoe = pMMA 09.180.535 || Die Geschichte des pLythgoe',\n  'pWestcar = pBerlin P 3033 || Die ErzÃ¤hlungen des pWestcar',\n  0.1531979048005746),\n ('pLythgoe = pMMA 09.180.535 || Die Geschichte des pLythgoe',\n  'Stele Louvre C 284 (\"Bentresch-Stele\") || Stele Louvre C 284 (\"Bentresch-Stele\")',\n  0.25125878382929623),\n ('pLythgoe = pMMA 09.180.535 || Die Geschichte des pLythgoe',\n  'pLondon UC 32157 = pKahun LV.1 || Verso: Die Geschichte von Hay',\n  0.2789398933983388),\n ('pPetersburg 1115 || Die Geschichte des SchiffbrÃ¼chigen',\n  'oAshmolean Museum 1945.40 aus Deir el Medineh (AOS) || Sinuhe Koch 01-79',\n  0.10744325997426307),\n ('pPetersburg 1115 || Die Geschichte des SchiffbrÃ¼chigen',\n  'pBerlin P 3022 und Fragmente pAmherst m-q (B) || Sinuhe Koch 04-81',\n  0.12081084945309728),\n ('pPetersburg 1115 || Die Geschichte des SchiffbrÃ¼chigen',\n  'pAmherst 3 + pBerlin P 3024 || Der LebensmÃ¼de',\n  0.13977558765037024),\n ('pBerlin P 3022 und Fragmente pAmherst m-q (B) || Sinuhe Koch 04-81',\n  'oAshmolean Museum 1945.40 aus Deir el Medineh (AOS) || Sinuhe Koch 01-79',\n  0.009154979039167954),\n ('pBerlin P 3022 und Fragmente pAmherst m-q (B) || Sinuhe Koch 04-81',\n  'pBerlin P 10499 aus Theben-West (R) || Verso: Sinuhe Koch 01-58',\n  0.05673339971248825),\n ('pBerlin P 3022 und Fragmente pAmherst m-q (B) || Sinuhe Koch 04-81',\n  'oKairo CG 25216 aus dem Grab des Sennedjem TT 1 (C) || Recto Sinuhe Koch 01-22',\n  0.10696143954589288),\n ('pLondon UC 32157 = pKahun LV.1 || Verso: Die Geschichte von Hay',\n  '15. oBodmer Inv. 12 || Die Lehre des Cheti 16.1-18.2',\n  0.09130881280694036),\n ('pLondon UC 32157 = pKahun LV.1 || Verso: Die Geschichte von Hay',\n  'pChassinat II = pLouvre E 25352 || Die Geistergeschichte des pChassinat II',\n  0.12884760999391454),\n ('pLondon UC 32157 = pKahun LV.1 || Verso: Die Geschichte von Hay',\n  '12. oKairo CG 25217 || AuÃŸen: Die Lehre des Cheti 13.1-14.1',\n  0.1368729540135647),\n ('pKahun VI.12 = pUC 32158+32150A+32148B || Recto: Die Geschichte von Horus und Seth',\n  'pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1)',\n  0.14079392283577996),\n ('pKahun VI.12 = pUC 32158+32150A+32148B || Recto: Die Geschichte von Horus und Seth',\n  'pRamesseum A = pBerlin P 10499 (Bauer, R) || Recto: Der beredte Bauer (Version R)',\n  0.1471620498709958),\n ('pKahun VI.12 = pUC 32158+32150A+32148B || Recto: Die Geschichte von Horus und Seth',\n  'pLondon UC 32107A || Recto: Die Geschichte von/Ã¼ber Nachti',\n  0.1642086377009253),\n ('pLondon UC 32107A || Recto: Die Geschichte von/Ã¼ber Nachti',\n  'pKahun VI.12 = pUC 32158+32150A+32148B || Recto: Die Geschichte von Horus und Seth',\n  0.1642086377009253),\n ('pLondon UC 32107A || Recto: Die Geschichte von/Ã¼ber Nachti',\n  'pRamesseum A = pBerlin P 10499 (Bauer, R) || Recto: Der beredte Bauer (Version R)',\n  0.26706678546231233),\n ('pLondon UC 32107A || Recto: Die Geschichte von/Ã¼ber Nachti',\n  'pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1)',\n  0.26767466255912886),\n ('pBerlin P 3024 || Die Hirtengeschichte',\n  'pMoskau o.Nr. + pMoskau 167 || Mythologische Geschichte',\n  0.1879714763607424),\n ('pBerlin P 3024 || Die Hirtengeschichte',\n  'pAmherst 3 + pBerlin P 3024 || Der LebensmÃ¼de',\n  0.23784883101555832),\n ('pBerlin P 3024 || Die Hirtengeschichte',\n  'pMoskau o.Nr. (Sporting King) || The Sporting King',\n  0.23913786230363743),\n ('pBM EA 10475 || Verso: ErzÃ¤hlung von Palast und Lebenshaus',\n  'pBerlin P 3024 || Die Hirtengeschichte',\n  0.2656651323242395),\n ('pBM EA 10475 || Verso: ErzÃ¤hlung von Palast und Lebenshaus',\n  '03. pLouvre E 4864 || Recto: Die Loyalistische Lehre des Kairsu 3.6-14.11',\n  0.2816215780851745),\n ('pBM EA 10475 || Verso: ErzÃ¤hlung von Palast und Lebenshaus',\n  'pMoskau o.Nr. (Sporting King) || The Sporting King',\n  0.29337724283596156),\n ('pMoskau o.Nr. + pMoskau 167 || Mythologische Geschichte',\n  \"KV 17: Grab Sethos' I., vordere rechte Seitenkammer der Sargkammer || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\",\n  0.17983328688801659),\n ('pMoskau o.Nr. + pMoskau 167 || Mythologische Geschichte',\n  'pBerlin P 3024 || Die Hirtengeschichte',\n  0.1879714763607424),\n ('pMoskau o.Nr. + pMoskau 167 || Mythologische Geschichte',\n  'pPetersburg 1115 || Die Geschichte des SchiffbrÃ¼chigen',\n  0.19176996665576285),\n ('pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1)',\n  'pBerlin P 3025 + pAmherst II (Bauer, B2) || Der beredte Bauer (Version B2)',\n  0.0619253533694637),\n ('pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1)',\n  'pPetersburg 1116 A || Verso: Die Lehre fÃ¼r Merikare',\n  0.06678931490155882),\n ('pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1)',\n  'pBM EA 10509 (Ptahhotep, Version L2+L2G) || Die Lehre des Ptahhotep (Version L2+L2G)',\n  0.07595641621472426),\n ('pBerlin P 3025 + pAmherst II (Bauer, B2) || Der beredte Bauer (Version B2)',\n  'pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1)',\n  0.0619253533694637),\n ('pBerlin P 3025 + pAmherst II (Bauer, B2) || Der beredte Bauer (Version B2)',\n  'pPetersburg 1116 A || Verso: Die Lehre fÃ¼r Merikare',\n  0.12260820185588839),\n ('pBerlin P 3025 + pAmherst II (Bauer, B2) || Der beredte Bauer (Version B2)',\n  'pPrisse = pBN 186-194 (Ptahhotep, Version P) || Die Lehre des Ptahhotep',\n  0.1455895355909591),\n ('pRamesseum A = pBerlin P 10499 (Bauer, R) || Recto: Der beredte Bauer (Version R)',\n  'pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1)',\n  0.07687913744519326),\n ('pRamesseum A = pBerlin P 10499 (Bauer, R) || Recto: Der beredte Bauer (Version R)',\n  'pPetersburg 1116 A || Verso: Die Lehre fÃ¼r Merikare',\n  0.1163455962094343),\n ('pRamesseum A = pBerlin P 10499 (Bauer, R) || Recto: Der beredte Bauer (Version R)',\n  'pPrisse = pBN 186-194 (Ptahhotep, Version P) || Die Lehre des Ptahhotep',\n  0.11915760047599866),\n ('pLeiden I 344 = AMS 27 || Recto: Admonitions = Ipuwer',\n  '03. pLouvre E 4864 || Recto: Die Loyalistische Lehre des Kairsu 3.6-14.11',\n  0.1832831623179395),\n ('pLeiden I 344 = AMS 27 || Recto: Admonitions = Ipuwer',\n  'pPetersburg 1116 B || Verso: Die Prophezeiungen des Neferti',\n  0.19435258075298312),\n ('pLeiden I 344 = AMS 27 || Recto: Admonitions = Ipuwer',\n  'pCarlsberg VI || Die Lehre fÃ¼r Merikare',\n  0.20362706929971386),\n ('pAmherst 3 + pBerlin P 3024 || Der LebensmÃ¼de',\n  'pSallier II = pBM EA 10182 || Kol. 1-3: Die Lehre des Amenemhet',\n  0.09625210333292322),\n ('pAmherst 3 + pBerlin P 3024 || Der LebensmÃ¼de',\n  'pBerlin P 3022 und Fragmente pAmherst m-q (B) || Sinuhe Koch 04-81',\n  0.1188726051192942),\n ('pAmherst 3 + pBerlin P 3024 || Der LebensmÃ¼de',\n  'oAshmolean Museum 1945.40 aus Deir el Medineh (AOS) || Sinuhe Koch 01-79',\n  0.12058528144084835),\n ('tBM EA 5645 || Die Klagen des Chacheperreseneb',\n  'pAmherst 3 + pBerlin P 3024 || Der LebensmÃ¼de',\n  0.15036158838064595),\n ('tBM EA 5645 || Die Klagen des Chacheperreseneb',\n  'pBerlin P 3022 und Fragmente pAmherst m-q (B) || Sinuhe Koch 04-81',\n  0.16856929393481213),\n ('tBM EA 5645 || Die Klagen des Chacheperreseneb',\n  'pMillingen || Die Lehre des Amenemhet',\n  0.1886518605118277),\n ('pPetersburg 1116 B || Verso: Die Prophezeiungen des Neferti',\n  'pLeiden I 344 = AMS 27 || Recto: Admonitions = Ipuwer',\n  0.19435258075298312),\n ('pPetersburg 1116 B || Verso: Die Prophezeiungen des Neferti',\n  \"KV 17: Grab Sethos' I., vordere rechte Seitenkammer der Sargkammer || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\",\n  0.19847842193777998),\n ('pPetersburg 1116 B || Verso: Die Prophezeiungen des Neferti',\n  '03. pLouvre E 4864 || Recto: Die Loyalistische Lehre des Kairsu 3.6-14.11',\n  0.20909384555587296),\n ('pRamesseum I = pBM EA 10754 || Die Rede des Sasobek',\n  'pPetersburg 1116 A || Verso: Die Lehre fÃ¼r Merikare',\n  0.09528300446508309),\n ('pRamesseum I = pBM EA 10754 || Die Rede des Sasobek',\n  'pMoskau 4658 || Die Lehre fÃ¼r Merikare',\n  0.10996491576469958),\n ('pRamesseum I = pBM EA 10754 || Die Rede des Sasobek',\n  'pPrisse = pBN 186-194 (Ptahhotep, Version P) || Die Lehre des Ptahhotep',\n  0.11075662108034967),\n ('pButler = pBM EA 10274 || Verso: Die Rede des VogelfÃ¤ngers Juru',\n  'pBerlin P 3025 + pAmherst II (Bauer, B2) || Der beredte Bauer (Version B2)',\n  0.155212500105526),\n ('pButler = pBM EA 10274 || Verso: Die Rede des VogelfÃ¤ngers Juru',\n  'pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1)',\n  0.16166810707005164),\n ('pButler = pBM EA 10274 || Verso: Die Rede des VogelfÃ¤ngers Juru',\n  'pKahun VI.12 = pUC 32158+32150A+32148B || Recto: Die Geschichte von Horus und Seth',\n  0.19600946144418663),\n ('pMoskau o.Nr. (Pleasures of Fishing) || The Pleasures of Fishing and Fowling',\n  'pMillingen || Die Lehre des Amenemhet',\n  0.07908163710882199),\n ('pMoskau o.Nr. (Pleasures of Fishing) || The Pleasures of Fishing and Fowling',\n  'pSallier II = pBM EA 10182 || Kol. 1-3: Die Lehre des Amenemhet',\n  0.09458896880318401),\n ('pMoskau o.Nr. (Pleasures of Fishing) || The Pleasures of Fishing and Fowling',\n  'oKairo CG 25216 aus dem Grab des Sennedjem TT 1 (C) || Recto Sinuhe Koch 01-22',\n  0.1224178205824259),\n ('pMoskau o.Nr. (Sporting King) || The Sporting King',\n  'pBerlin P 3024 || Die Hirtengeschichte',\n  0.23913786230363743),\n ('pMoskau o.Nr. (Sporting King) || The Sporting King',\n  'pMoskau o.Nr. + pMoskau 167 || Mythologische Geschichte',\n  0.28909625654516813),\n ('pMoskau o.Nr. (Sporting King) || The Sporting King',\n  'pBM EA 10475 || Verso: ErzÃ¤hlung von Palast und Lebenshaus',\n  0.29337724283596156),\n ('tAshmolean 1964.489 || Der Oxforder Weisheitstext',\n  'pPetersburg 1116 A || Verso: Die Lehre fÃ¼r Merikare',\n  0.16192965979382234),\n ('tAshmolean 1964.489 || Der Oxforder Weisheitstext',\n  'pBM EA 10509 (Ptahhotep, Version L2+L2G) || Die Lehre des Ptahhotep (Version L2+L2G)',\n  0.16604272339116777),\n ('tAshmolean 1964.489 || Der Oxforder Weisheitstext',\n  'pPrisse = pBN 186-194 (Ptahhotep, Version P) || Die Lehre des Ptahhotep',\n  0.19049654528996984),\n ('pMillingen || Die Lehre des Amenemhet',\n  'pSallier II = pBM EA 10182 || Kol. 1-3: Die Lehre des Amenemhet',\n  0.06388867314892821),\n ('pMillingen || Die Lehre des Amenemhet',\n  'pMoskau o.Nr. (Pleasures of Fishing) || The Pleasures of Fishing and Fowling',\n  0.07908163710882199),\n ('pMillingen || Die Lehre des Amenemhet',\n  'pAmherst 3 + pBerlin P 3024 || Der LebensmÃ¼de',\n  0.12710601587498094),\n ('01. oMÃ¼nchen 3400 || Die Lehre des Hordjedef 1.1-3.1',\n  'oDeM 1632 || Eine Sammlung von Verboten',\n  0.1732264123459556),\n ('01. oMÃ¼nchen 3400 || Die Lehre des Hordjedef 1.1-3.1',\n  'oPetrie 11 = oLondon UC 39614 || Eine Sammlung von Verboten',\n  0.1786389235366801),\n ('01. oMÃ¼nchen 3400 || Die Lehre des Hordjedef 1.1-3.1',\n  'hintere Innenwand || Rahmentext',\n  0.2107446787565559),\n ('06. oGardiner 12 || Die Lehre des Hordjedef 1.4-9.1',\n  'pBM EA 10371+10435 (Ptahhotep, Version L1) || Die Lehre des Ptahhotep (Version L1)',\n  0.2169184299190794),\n ('06. oGardiner 12 || Die Lehre des Hordjedef 1.4-9.1',\n  'pBM EA 10509 (Ptahhotep, Version L2+L2G) || Die Lehre des Ptahhotep (Version L2+L2G)',\n  0.22618542603137692),\n ('06. oGardiner 12 || Die Lehre des Hordjedef 1.4-9.1',\n  'pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1)',\n  0.22812223633935136),\n ('pRamesseum II = pBM EA 10755 || Die Lehre des pRamesseum II',\n  'pRamesseum I = pBM EA 10754 || Die Rede des Sasobek',\n  0.12232937069583505),\n ('pRamesseum II = pBM EA 10755 || Die Lehre des pRamesseum II',\n  'pMoskau 4658 || Die Lehre fÃ¼r Merikare',\n  0.17417669266027214),\n ('pRamesseum II = pBM EA 10755 || Die Lehre des pRamesseum II',\n  '03. pLouvre E 4864 || Recto: Die Loyalistische Lehre des Kairsu 3.6-14.11',\n  0.1802316141795418),\n ('pBM EA 10371+10435 (Ptahhotep, Version L1) || Die Lehre des Ptahhotep (Version L1)',\n  'pBM EA 10509 (Ptahhotep, Version L2+L2G) || Die Lehre des Ptahhotep (Version L2+L2G)',\n  0.024620198668114268),\n ('pBM EA 10371+10435 (Ptahhotep, Version L1) || Die Lehre des Ptahhotep (Version L1)',\n  'pPrisse = pBN 186-194 (Ptahhotep, Version P) || Die Lehre des Ptahhotep',\n  0.02849413264234768),\n ('pBM EA 10371+10435 (Ptahhotep, Version L1) || Die Lehre des Ptahhotep (Version L1)',\n  'pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1)',\n  0.08940820488937429),\n ('pBM EA 10509 (Ptahhotep, Version L2+L2G) || Die Lehre des Ptahhotep (Version L2+L2G)',\n  'pPrisse = pBN 186-194 (Ptahhotep, Version P) || Die Lehre des Ptahhotep',\n  0.009353395133025244),\n ('pBM EA 10509 (Ptahhotep, Version L2+L2G) || Die Lehre des Ptahhotep (Version L2+L2G)',\n  'pBM EA 10371+10435 (Ptahhotep, Version L1) || Die Lehre des Ptahhotep (Version L1)',\n  0.024620198668114268),\n ('pBM EA 10509 (Ptahhotep, Version L2+L2G) || Die Lehre des Ptahhotep (Version L2+L2G)',\n  'pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1)',\n  0.07595641621472426),\n ('pPrisse = pBN 186-194 (Ptahhotep, Version P) || Die Lehre des Ptahhotep',\n  'pBM EA 10509 (Ptahhotep, Version L2+L2G) || Die Lehre des Ptahhotep (Version L2+L2G)',\n  0.009353395133025244),\n ('pPrisse = pBN 186-194 (Ptahhotep, Version P) || Die Lehre des Ptahhotep',\n  'pBM EA 10371+10435 (Ptahhotep, Version L1) || Die Lehre des Ptahhotep (Version L1)',\n  0.02849413264234768),\n ('pPrisse = pBN 186-194 (Ptahhotep, Version P) || Die Lehre des Ptahhotep',\n  'pPetersburg 1116 A || Verso: Die Lehre fÃ¼r Merikare',\n  0.07535484195508368),\n ('tCarnarvon 1 || Verso: Die Lehre des Ptahhotep (Version C)',\n  'Topf 1: oTurin Suppl. 6619 + oGardiner 306 + oDeM 1251 I+II + oTurin Suppl. 6806 || Chonsemhab und der Geist, Kolumne 2',\n  0.25435206367605956),\n ('tCarnarvon 1 || Verso: Die Lehre des Ptahhotep (Version C)',\n  'oAshmolean Museum 1945.40 aus Deir el Medineh (AOS) || Sinuhe Koch 01-79',\n  0.26811793553873664),\n ('tCarnarvon 1 || Verso: Die Lehre des Ptahhotep (Version C)',\n  '02. oBM EA 41541 || Die Lehre des Amunnacht',\n  0.2697290880994865),\n ('01. Lederrolle BM EA 10258 || Die Lehre eines Mannes fÃ¼r seinen Sohn 0.1-6.8',\n  '02. oDeM 1665 + oGardiner 1 || Die Lehre eines Mannes fÃ¼r seinen Sohn 1.2-8.7',\n  0.12258017107126284),\n ('01. Lederrolle BM EA 10258 || Die Lehre eines Mannes fÃ¼r seinen Sohn 0.1-6.8',\n  '01. pSallier II = pBM EA 10182 || Kol. 3-11: Die Lehre des Cheti 1.1-30.6',\n  0.12959898131741843),\n ('01. Lederrolle BM EA 10258 || Die Lehre eines Mannes fÃ¼r seinen Sohn 0.1-6.8',\n  'pRamesseum I = pBM EA 10754 || Die Rede des Sasobek',\n  0.1583022410385999),\n ('02. oDeM 1665 + oGardiner 1 || Die Lehre eines Mannes fÃ¼r seinen Sohn 1.2-8.7',\n  '06. oIFAO OL 416 (vorher: ohne Nummer (18. Dyn.)) || Die Lehre eines Mannes fÃ¼r seinen Sohn 4.5-8.2',\n  0.09871536248952317),\n ('02. oDeM 1665 + oGardiner 1 || Die Lehre eines Mannes fÃ¼r seinen Sohn 1.2-8.7',\n  '04. oTurin CGT 57063 || Die Lehre eines Mannes fÃ¼r seinen Sohn 3.4-4.4',\n  0.11543494720264946),\n ('02. oDeM 1665 + oGardiner 1 || Die Lehre eines Mannes fÃ¼r seinen Sohn 1.2-8.7',\n  '01. Lederrolle BM EA 10258 || Die Lehre eines Mannes fÃ¼r seinen Sohn 0.1-6.8',\n  0.12258017107126284),\n ('03. oGardiner 35 || Die Lehre eines Mannes fÃ¼r seinen Sohn 3.4-4.5',\n  '04. oTurin CGT 57063 || Die Lehre eines Mannes fÃ¼r seinen Sohn 3.4-4.4',\n  0.08939648667594002),\n ('03. oGardiner 35 || Die Lehre eines Mannes fÃ¼r seinen Sohn 3.4-4.5',\n  '06. oIFAO OL 416 (vorher: ohne Nummer (18. Dyn.)) || Die Lehre eines Mannes fÃ¼r seinen Sohn 4.5-8.2',\n  0.13522543748162752),\n ('03. oGardiner 35 || Die Lehre eines Mannes fÃ¼r seinen Sohn 3.4-4.5',\n  '02. oDeM 1665 + oGardiner 1 || Die Lehre eines Mannes fÃ¼r seinen Sohn 1.2-8.7',\n  0.16543559430711252),\n ('04. oTurin CGT 57063 || Die Lehre eines Mannes fÃ¼r seinen Sohn 3.4-4.4',\n  '03. oGardiner 35 || Die Lehre eines Mannes fÃ¼r seinen Sohn 3.4-4.5',\n  0.08939648667594002),\n ('04. oTurin CGT 57063 || Die Lehre eines Mannes fÃ¼r seinen Sohn 3.4-4.4',\n  '02. oDeM 1665 + oGardiner 1 || Die Lehre eines Mannes fÃ¼r seinen Sohn 1.2-8.7',\n  0.11543494720264946),\n ('04. oTurin CGT 57063 || Die Lehre eines Mannes fÃ¼r seinen Sohn 3.4-4.4',\n  '06. oIFAO OL 416 (vorher: ohne Nummer (18. Dyn.)) || Die Lehre eines Mannes fÃ¼r seinen Sohn 4.5-8.2',\n  0.1583473518306906),\n ('05. oLouvre 23561 || Die Lehre eines Mannes fÃ¼r seinen Sohn 4.5-6.1',\n  '06. oIFAO OL 416 (vorher: ohne Nummer (18. Dyn.)) || Die Lehre eines Mannes fÃ¼r seinen Sohn 4.5-8.2',\n  0.1805792303649041),\n ('05. oLouvre 23561 || Die Lehre eines Mannes fÃ¼r seinen Sohn 4.5-6.1',\n  'pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1)',\n  0.21961653588296437),\n ('05. oLouvre 23561 || Die Lehre eines Mannes fÃ¼r seinen Sohn 4.5-6.1',\n  '03. oGardiner 35 || Die Lehre eines Mannes fÃ¼r seinen Sohn 3.4-4.5',\n  0.23357426323266117),\n ('06. oIFAO OL 416 (vorher: ohne Nummer (18. Dyn.)) || Die Lehre eines Mannes fÃ¼r seinen Sohn 4.5-8.2',\n  '02. oDeM 1665 + oGardiner 1 || Die Lehre eines Mannes fÃ¼r seinen Sohn 1.2-8.7',\n  0.09871536248952317),\n ('06. oIFAO OL 416 (vorher: ohne Nummer (18. Dyn.)) || Die Lehre eines Mannes fÃ¼r seinen Sohn 4.5-8.2',\n  '02. pRifeh = pLondon UC 32781 || Recto: Die Loyalistische Lehre des Kairsu 1.1-4.8',\n  0.12461301318208617),\n ('06. oIFAO OL 416 (vorher: ohne Nummer (18. Dyn.)) || Die Lehre eines Mannes fÃ¼r seinen Sohn 4.5-8.2',\n  '03. oGardiner 35 || Die Lehre eines Mannes fÃ¼r seinen Sohn 3.4-4.5',\n  0.13522543748162752),\n ('07. oMMA 29.2.25 || Die Lehre eines Mannes fÃ¼r seinen Sohn 5.4-6.4',\n  '16. oDeM 1013 || Die Lehre des Cheti 23.1-25.3',\n  0.37332275724060937),\n ('07. oMMA 29.2.25 || Die Lehre eines Mannes fÃ¼r seinen Sohn 5.4-6.4',\n  '18. oOIC 16999 || Die Lehre eines Mannes fÃ¼r seinen Sohn 15.x+1-17.3',\n  0.44195303762063043),\n ('07. oMMA 29.2.25 || Die Lehre eines Mannes fÃ¼r seinen Sohn 5.4-6.4',\n  '6. tTurin CGT 58005 || Die Lehre des Amenemope 24.1-25.9',\n  0.5202975923305964),\n ('08. oBerlin P 14356 || Die Lehre eines Mannes fÃ¼r seinen Sohn 7.1-9.1',\n  '02. oDeM 1665 + oGardiner 1 || Die Lehre eines Mannes fÃ¼r seinen Sohn 1.2-8.7',\n  0.12984506053587375),\n ('08. oBerlin P 14356 || Die Lehre eines Mannes fÃ¼r seinen Sohn 7.1-9.1',\n  '15. oBodmer Inv. 12 || Die Lehre des Cheti 16.1-18.2',\n  0.13488395016666233),\n ('08. oBerlin P 14356 || Die Lehre eines Mannes fÃ¼r seinen Sohn 7.1-9.1',\n  '04. oTurin CGT 57063 || Die Lehre eines Mannes fÃ¼r seinen Sohn 3.4-4.4',\n  0.1699940390704462),\n ('14. oIFAO Inv. 2010 || Die Lehre eines Mannes fÃ¼r seinen Sohn 9.1-12.6',\n  'pPrisse = pBN 186-194 (Ptahhotep, Version P) || Die Lehre des Ptahhotep',\n  0.19685504558035494),\n ('14. oIFAO Inv. 2010 || Die Lehre eines Mannes fÃ¼r seinen Sohn 9.1-12.6',\n  'pRamesseum I = pBM EA 10754 || Die Rede des Sasobek',\n  0.21322916864095176),\n ('14. oIFAO Inv. 2010 || Die Lehre eines Mannes fÃ¼r seinen Sohn 9.1-12.6',\n  '01. Lederrolle BM EA 10258 || Die Lehre eines Mannes fÃ¼r seinen Sohn 0.1-6.8',\n  0.2142510513508974),\n ('18. oOIC 16999 || Die Lehre eines Mannes fÃ¼r seinen Sohn 15.x+1-17.3',\n  '23. oIFAO Inv. 2536 || Die Lehre eines Mannes fÃ¼r seinen Sohn 19.1-24.10',\n  0.12249038408814295),\n ('18. oOIC 16999 || Die Lehre eines Mannes fÃ¼r seinen Sohn 15.x+1-17.3',\n  'pBM EA 10509 (Ptahhotep, Version L2+L2G) || Die Lehre des Ptahhotep (Version L2+L2G)',\n  0.17944110751059295),\n ('18. oOIC 16999 || Die Lehre eines Mannes fÃ¼r seinen Sohn 15.x+1-17.3',\n  'pPrisse = pBN 186-194 (Ptahhotep, Version P) || Die Lehre des Ptahhotep',\n  0.18489686501455538),\n ('22. oMichaelides 16 = oLos Angeles M.80.203.209 || Die Lehre eines Mannes fÃ¼r seinen Sohn 19.1-21.3',\n  '14. oIFAO Inv. 2010 || Die Lehre eines Mannes fÃ¼r seinen Sohn 9.1-12.6',\n  0.2798958094520352),\n ('22. oMichaelides 16 = oLos Angeles M.80.203.209 || Die Lehre eines Mannes fÃ¼r seinen Sohn 19.1-21.3',\n  '23. oIFAO Inv. 2536 || Die Lehre eines Mannes fÃ¼r seinen Sohn 19.1-24.10',\n  0.32320680088928555),\n ('22. oMichaelides 16 = oLos Angeles M.80.203.209 || Die Lehre eines Mannes fÃ¼r seinen Sohn 19.1-21.3',\n  'pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1)',\n  0.3261010562001928),\n ('23. oIFAO Inv. 2536 || Die Lehre eines Mannes fÃ¼r seinen Sohn 19.1-24.10',\n  '18. oOIC 16999 || Die Lehre eines Mannes fÃ¼r seinen Sohn 15.x+1-17.3',\n  0.12249038408814295),\n ('23. oIFAO Inv. 2536 || Die Lehre eines Mannes fÃ¼r seinen Sohn 19.1-24.10',\n  'pBM EA 10509 (Ptahhotep, Version L2+L2G) || Die Lehre des Ptahhotep (Version L2+L2G)',\n  0.15641044551959116),\n ('23. oIFAO Inv. 2536 || Die Lehre eines Mannes fÃ¼r seinen Sohn 19.1-24.10',\n  'pBM EA 10371+10435 (Ptahhotep, Version L1) || Die Lehre des Ptahhotep (Version L1)',\n  0.15710113067460907),\n ('25. oMoskau 4478 + oBerlin P 9026 || Verso: Die Lehre eines Mannes fÃ¼r seinen Sohn 24.1-10',\n  'pPrisse = pBN 186-194 (Ptahhotep, Version P) || Die Lehre des Ptahhotep',\n  0.22945995447053136),\n ('25. oMoskau 4478 + oBerlin P 9026 || Verso: Die Lehre eines Mannes fÃ¼r seinen Sohn 24.1-10',\n  '23. oIFAO Inv. 2536 || Die Lehre eines Mannes fÃ¼r seinen Sohn 19.1-24.10',\n  0.24415196582436383),\n ('25. oMoskau 4478 + oBerlin P 9026 || Verso: Die Lehre eines Mannes fÃ¼r seinen Sohn 24.1-10',\n  '01. pSallier II = pBM EA 10182 || Kol. 3-11: Die Lehre des Cheti 1.1-30.6',\n  0.2508570144057397),\n ('26. oDeM 1056 + oGardiner 347 || Die Lehre eines Mannes fÃ¼r seinen Sohn 24.1-24.10, Kolophon',\n  'pPrisse = pBN 186-194 (Ptahhotep, Version P) || Die Lehre des Ptahhotep',\n  0.27444566970189055),\n ('26. oDeM 1056 + oGardiner 347 || Die Lehre eines Mannes fÃ¼r seinen Sohn 24.1-24.10, Kolophon',\n  'pMoskau 4658 || Die Lehre fÃ¼r Merikare',\n  0.27529751511842526),\n ('26. oDeM 1056 + oGardiner 347 || Die Lehre eines Mannes fÃ¼r seinen Sohn 24.1-24.10, Kolophon',\n  'pPetersburg 1116 A || Verso: Die Lehre fÃ¼r Merikare',\n  0.2794929755966843),\n ('pPrisse = pBN 183 (Kagemni) || Kol. 1-2: Die Lehre fÃ¼r Kagemni',\n  'pPrisse = pBN 186-194 (Ptahhotep, Version P) || Die Lehre des Ptahhotep',\n  0.15682487312048288),\n ('pPrisse = pBN 183 (Kagemni) || Kol. 1-2: Die Lehre fÃ¼r Kagemni',\n  'pBM EA 10509 (Ptahhotep, Version L2+L2G) || Die Lehre des Ptahhotep (Version L2+L2G)',\n  0.17928212586316994),\n ('pPrisse = pBN 183 (Kagemni) || Kol. 1-2: Die Lehre fÃ¼r Kagemni',\n  'pBM EA 10371+10435 (Ptahhotep, Version L1) || Die Lehre des Ptahhotep (Version L1)',\n  0.19671222655800746),\n ('pCarlsberg VI || Die Lehre fÃ¼r Merikare',\n  'pMoskau 4658 || Die Lehre fÃ¼r Merikare',\n  0.06928367859512619),\n ('pCarlsberg VI || Die Lehre fÃ¼r Merikare',\n  'pPetersburg 1116 A || Verso: Die Lehre fÃ¼r Merikare',\n  0.07509899758921645),\n ('pCarlsberg VI || Die Lehre fÃ¼r Merikare',\n  'pRamesseum I = pBM EA 10754 || Die Rede des Sasobek',\n  0.12779829434417345),\n ('pMoskau 4658 || Die Lehre fÃ¼r Merikare',\n  'pCarlsberg VI || Die Lehre fÃ¼r Merikare',\n  0.06928367859512619),\n ('pMoskau 4658 || Die Lehre fÃ¼r Merikare',\n  'pPetersburg 1116 A || Verso: Die Lehre fÃ¼r Merikare',\n  0.07232726939293632),\n ('pMoskau 4658 || Die Lehre fÃ¼r Merikare',\n  'pPrisse = pBN 186-194 (Ptahhotep, Version P) || Die Lehre des Ptahhotep',\n  0.09360982454755462),\n ('pPetersburg 1116 A || Verso: Die Lehre fÃ¼r Merikare',\n  'pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1)',\n  0.06678931490155882),\n ('pPetersburg 1116 A || Verso: Die Lehre fÃ¼r Merikare',\n  'pMoskau 4658 || Die Lehre fÃ¼r Merikare',\n  0.07232726939293632),\n ('pPetersburg 1116 A || Verso: Die Lehre fÃ¼r Merikare',\n  'pCarlsberg VI || Die Lehre fÃ¼r Merikare',\n  0.07509899758921645),\n ('01. sKairo CG 20538 des Sehetep-ib-Re || Die Loyalistische Lehre des Kairsu 1.1-6.10',\n  \"KV 9: Grab Ramses' VI., Korridor C, Nische am Ende der linken Wand || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\",\n  0.16938358525531327),\n ('01. sKairo CG 20538 des Sehetep-ib-Re || Die Loyalistische Lehre des Kairsu 1.1-6.10',\n  '03. pLouvre E 4864 || Recto: Die Loyalistische Lehre des Kairsu 3.6-14.11',\n  0.18011461052821764),\n ('01. sKairo CG 20538 des Sehetep-ib-Re || Die Loyalistische Lehre des Kairsu 1.1-6.10',\n  'pCarlsberg VI || Die Lehre fÃ¼r Merikare',\n  0.18011516412262474),\n ('02. pRifeh = pLondon UC 32781 || Recto: Die Loyalistische Lehre des Kairsu 1.1-4.8',\n  '07. oSchweiz Privatsammlung Inv. 086 || Die Loyalistische Lehre des Kairsu 3.1-3.12',\n  0.06777918369785252),\n ('02. pRifeh = pLondon UC 32781 || Recto: Die Loyalistische Lehre des Kairsu 1.1-4.8',\n  '06. oDeM 1235 || Die Loyalistische Lehre des Kairsu 1.1-4.1',\n  0.1241656847510042),\n ('02. pRifeh = pLondon UC 32781 || Recto: Die Loyalistische Lehre des Kairsu 1.1-4.8',\n  '06. oIFAO OL 416 (vorher: ohne Nummer (18. Dyn.)) || Die Lehre eines Mannes fÃ¼r seinen Sohn 4.5-8.2',\n  0.12461301318208617),\n ('03. pLouvre E 4864 || Recto: Die Loyalistische Lehre des Kairsu 3.6-14.11',\n  '04. tCarnarvon 2 || Verso: Die Loyalistische Lehre des Kairsu 8.1-14.12',\n  0.12144738165333613),\n ('03. pLouvre E 4864 || Recto: Die Loyalistische Lehre des Kairsu 3.6-14.11',\n  'pMoskau 4658 || Die Lehre fÃ¼r Merikare',\n  0.13236045006213104),\n ('03. pLouvre E 4864 || Recto: Die Loyalistische Lehre des Kairsu 3.6-14.11',\n  'pCarlsberg VI || Die Lehre fÃ¼r Merikare',\n  0.13553003647991702),\n ('04. tCarnarvon 2 || Verso: Die Loyalistische Lehre des Kairsu 8.1-14.12',\n  '03. pLouvre E 4864 || Recto: Die Loyalistische Lehre des Kairsu 3.6-14.11',\n  0.12144738165333613),\n ('04. tCarnarvon 2 || Verso: Die Loyalistische Lehre des Kairsu 8.1-14.12',\n  'pCarlsberg VI || Die Lehre fÃ¼r Merikare',\n  0.1538441147296994),\n ('04. tCarnarvon 2 || Verso: Die Loyalistische Lehre des Kairsu 8.1-14.12',\n  'pMoskau 4658 || Die Lehre fÃ¼r Merikare',\n  0.17187258762484348),\n ('06. oDeM 1235 || Die Loyalistische Lehre des Kairsu 1.1-4.1',\n  '02. pRifeh = pLondon UC 32781 || Recto: Die Loyalistische Lehre des Kairsu 1.1-4.8',\n  0.1241656847510042),\n ('06. oDeM 1235 || Die Loyalistische Lehre des Kairsu 1.1-4.1',\n  '07. oSchweiz Privatsammlung Inv. 086 || Die Loyalistische Lehre des Kairsu 3.1-3.12',\n  0.18533276076566507),\n ('06. oDeM 1235 || Die Loyalistische Lehre des Kairsu 1.1-4.1',\n  '01. sKairo CG 20538 des Sehetep-ib-Re || Die Loyalistische Lehre des Kairsu 1.1-6.10',\n  0.2349145977930317),\n ('07. oSchweiz Privatsammlung Inv. 086 || Die Loyalistische Lehre des Kairsu 3.1-3.12',\n  '02. pRifeh = pLondon UC 32781 || Recto: Die Loyalistische Lehre des Kairsu 1.1-4.8',\n  0.06777918369785252),\n ('07. oSchweiz Privatsammlung Inv. 086 || Die Loyalistische Lehre des Kairsu 3.1-3.12',\n  '15. oBodmer Inv. 12 || Die Lehre des Cheti 16.1-18.2',\n  0.1547821405042824),\n ('07. oSchweiz Privatsammlung Inv. 086 || Die Loyalistische Lehre des Kairsu 3.1-3.12',\n  '08. oBerlin P 14356 || Die Lehre eines Mannes fÃ¼r seinen Sohn 7.1-9.1',\n  0.17308151461973598),\n ('08. oAshmolean 1938.912 || Die Loyalistische Lehre des Kairsu 5.1-8.1',\n  '03. pLouvre E 4864 || Recto: Die Loyalistische Lehre des Kairsu 3.6-14.11',\n  0.1413975481883647),\n ('08. oAshmolean 1938.912 || Die Loyalistische Lehre des Kairsu 5.1-8.1',\n  '01. sKairo CG 20538 des Sehetep-ib-Re || Die Loyalistische Lehre des Kairsu 1.1-6.10',\n  0.19419260630660284),\n ('08. oAshmolean 1938.912 || Die Loyalistische Lehre des Kairsu 5.1-8.1',\n  \"KV 9: Grab Ramses' VI., Korridor C, Nische am Ende der linken Wand || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\",\n  0.19675135303318336),\n ('09. oBM EA 5632 || Recto: Die Loyalistische Lehre des Kairsu 8.1-9.1',\n  '01. Lederrolle BM EA 10258 || Die Lehre eines Mannes fÃ¼r seinen Sohn 0.1-6.8',\n  0.2913085621229471),\n ('09. oBM EA 5632 || Recto: Die Loyalistische Lehre des Kairsu 8.1-9.1',\n  'pAmherst 3 + pBerlin P 3024 || Der LebensmÃ¼de',\n  0.301432989850025),\n ('09. oBM EA 5632 || Recto: Die Loyalistische Lehre des Kairsu 8.1-9.1',\n  'pSallier II = pBM EA 10182 || Kol. 1-3: Die Lehre des Amenemhet',\n  0.3201346955753681),\n ('pSallier I = pBM EA 10185 || Recto 1-3: Apophis und Seqenenre',\n  'pHarris 500 = pBM EA 10060 || Verso 4-8: Das PrinzenmÃ¤rchen',\n  0.17614894046379792),\n ('pSallier I = pBM EA 10185 || Recto 1-3: Apophis und Seqenenre',\n  'pChester Beatty II = pBM EA 10682 || Wahrheit und LÃ¼ge',\n  0.18296348043916866),\n ('pSallier I = pBM EA 10185 || Recto 1-3: Apophis und Seqenenre',\n  'pChester Beatty I || Recto: Der Streit zwischen Horus und Seth',\n  0.18639458363742478),\n ('Topf 1: oTurin Suppl. 6619 + oGardiner 306 + oDeM 1251 I+II + oTurin Suppl. 6806 || Chonsemhab und der Geist, Kolumne 2',\n  'Topf 2: oFlorenz 2616 || Chonsemhab und der Geist, Kolumne 3',\n  0.14462779630604927),\n ('Topf 1: oTurin Suppl. 6619 + oGardiner 306 + oDeM 1251 I+II + oTurin Suppl. 6806 || Chonsemhab und der Geist, Kolumne 2',\n  'Topf 2: oTurin Cat. 6620+6838 + oDeM 1252 I+II + oTurin Cat. 6851 + oFlorenz 2617 || Chonsemhab und der Geist, Kolumne 3',\n  0.15560624134305345),\n ('Topf 1: oTurin Suppl. 6619 + oGardiner 306 + oDeM 1251 I+II + oTurin Suppl. 6806 || Chonsemhab und der Geist, Kolumne 2',\n  'pChester Beatty II = pBM EA 10682 || Wahrheit und LÃ¼ge',\n  0.17309802788398942),\n ('Topf 2: oTurin Cat. 6620+6838 + oDeM 1252 I+II + oTurin Cat. 6851 + oFlorenz 2617 || Chonsemhab und der Geist, Kolumne 3',\n  'Topf 2: oFlorenz 2616 || Chonsemhab und der Geist, Kolumne 3',\n  0.037665425065587566),\n ('Topf 2: oTurin Cat. 6620+6838 + oDeM 1252 I+II + oTurin Cat. 6851 + oFlorenz 2617 || Chonsemhab und der Geist, Kolumne 3',\n  'Topf 1: oTurin Suppl. 6619 + oGardiner 306 + oDeM 1251 I+II + oTurin Suppl. 6806 || Chonsemhab und der Geist, Kolumne 2',\n  0.15560624134305345),\n ('Topf 2: oTurin Cat. 6620+6838 + oDeM 1252 I+II + oTurin Cat. 6851 + oFlorenz 2617 || Chonsemhab und der Geist, Kolumne 3',\n  'oAshmolean Museum 1945.40 aus Deir el Medineh (AOS) || Sinuhe Koch 01-79',\n  0.22801055435304485),\n ('Topf 2: oFlorenz 2616 || Chonsemhab und der Geist, Kolumne 3',\n  'Topf 2: oTurin Cat. 6620+6838 + oDeM 1252 I+II + oTurin Cat. 6851 + oFlorenz 2617 || Chonsemhab und der Geist, Kolumne 3',\n  0.037665425065587566),\n ('Topf 2: oFlorenz 2616 || Chonsemhab und der Geist, Kolumne 3',\n  'Topf 1: oTurin Suppl. 6619 + oGardiner 306 + oDeM 1251 I+II + oTurin Suppl. 6806 || Chonsemhab und der Geist, Kolumne 2',\n  0.14462779630604927),\n ('Topf 2: oFlorenz 2616 || Chonsemhab und der Geist, Kolumne 3',\n  'pMoskau 120 || Recto: Die Reise des Wenamun',\n  0.20630771654048963),\n ('Topf 3: oLouvre N 667 + N 700 || Chonsemhab und der Geist, Kolumne 4',\n  'pdâ€™Orbiney = pBM EA 10183 || Recto: Das ZweibrÃ¼dermÃ¤rchen',\n  0.13651144380031044),\n ('Topf 3: oLouvre N 667 + N 700 || Chonsemhab und der Geist, Kolumne 4',\n  'pDeM 39 || Herischef und General Meryre',\n  0.1481120028775199),\n ('Topf 3: oLouvre N 667 + N 700 || Chonsemhab und der Geist, Kolumne 4',\n  'pHarris 500 = pBM EA 10060 || Verso 4-8: Das PrinzenmÃ¤rchen',\n  0.1535233503673964),\n ('pdâ€™Orbiney = pBM EA 10183 || Recto: Das ZweibrÃ¼dermÃ¤rchen',\n  'pHarris 500 = pBM EA 10060 || Verso 4-8: Das PrinzenmÃ¤rchen',\n  0.04099488175872956),\n ('pdâ€™Orbiney = pBM EA 10183 || Recto: Das ZweibrÃ¼dermÃ¤rchen',\n  'pChester Beatty II = pBM EA 10682 || Wahrheit und LÃ¼ge',\n  0.04223161077779647),\n ('pdâ€™Orbiney = pBM EA 10183 || Recto: Das ZweibrÃ¼dermÃ¤rchen',\n  'pChester Beatty I || Recto: Der Streit zwischen Horus und Seth',\n  0.0693176472067848),\n ('tTurin Cat. 6238 = tTurin CGT 58004 || Der Streit zwischen Kopf und Leib',\n  'pChester Beatty I || Recto: Der Streit zwischen Horus und Seth',\n  0.17042199330412056),\n ('tTurin Cat. 6238 = tTurin CGT 58004 || Der Streit zwischen Kopf und Leib',\n  'pMoskau 120 || Recto: Die Reise des Wenamun',\n  0.20752677389079788),\n ('tTurin Cat. 6238 = tTurin CGT 58004 || Der Streit zwischen Kopf und Leib',\n  'pChester Beatty II = pBM EA 10682 || Wahrheit und LÃ¼ge',\n  0.21236123768342507),\n ('pBN 202 + pAmherst 9 || Die GÃ¶tter und das Meer (\"Astartelegende\")',\n  'pHarris 500 = pBM EA 10060 || Verso 4-8: Das PrinzenmÃ¤rchen',\n  0.09448730607764766),\n ('pBN 202 + pAmherst 9 || Die GÃ¶tter und das Meer (\"Astartelegende\")',\n  'pChester Beatty I || Recto: Der Streit zwischen Horus und Seth',\n  0.10785773227722462),\n ('pBN 202 + pAmherst 9 || Die GÃ¶tter und das Meer (\"Astartelegende\")',\n  'pdâ€™Orbiney = pBM EA 10183 || Recto: Das ZweibrÃ¼dermÃ¤rchen',\n  0.12854169025154505),\n ('pMoskau 120 || Recto: Die Reise des Wenamun',\n  'pChester Beatty II = pBM EA 10682 || Wahrheit und LÃ¼ge',\n  0.1261103600590634),\n ('pMoskau 120 || Recto: Die Reise des Wenamun',\n  'pHarris 500 = pBM EA 10060 || Verso 1-3: Die Einnahme von Joppe',\n  0.13787133418350694),\n ('pMoskau 120 || Recto: Die Reise des Wenamun',\n  'pHarris 500 = pBM EA 10060 || Verso 4-8: Das PrinzenmÃ¤rchen',\n  0.14266021839755516),\n ('pTurin CGT 54033 (alias: pTurin CGT 54031) || Verso: ErzÃ¤hlung von einem GÃ¶tterstreit',\n  '11. pAnastasi VII = pBM EA 10222 || Kol. 1-7: Die Lehre des Cheti 10.1-30.6',\n  0.1295185259282089),\n ('pTurin CGT 54033 (alias: pTurin CGT 54031) || Verso: ErzÃ¤hlung von einem GÃ¶tterstreit',\n  '01. pSallier II = pBM EA 10182 || Kol. 3-11: Die Lehre des Cheti 1.1-30.6',\n  0.13480013799912727),\n ('pTurin CGT 54033 (alias: pTurin CGT 54031) || Verso: ErzÃ¤hlung von einem GÃ¶tterstreit',\n  'pChassinat II = pLouvre E 25352 || Die Geistergeschichte des pChassinat II',\n  0.14774903958953634),\n ('pBoulaq 13 = pCairo CG 58041 || Eine Kriminalgeschichte (?)',\n  '01. pSallier II = pBM EA 10182 || Kol. 3-11: Die Lehre des Cheti 1.1-30.6',\n  0.1748887954175231),\n ('pBoulaq 13 = pCairo CG 58041 || Eine Kriminalgeschichte (?)',\n  '11. pAnastasi VII = pBM EA 10222 || Kol. 1-7: Die Lehre des Cheti 10.1-30.6',\n  0.18343516059044507),\n ('pBoulaq 13 = pCairo CG 58041 || Eine Kriminalgeschichte (?)',\n  'pTurin CGT 54033 (alias: pTurin CGT 54031) || Verso: ErzÃ¤hlung von einem GÃ¶tterstreit',\n  0.19517979238523187),\n ('oTurin Cat. 9587 = oTurin CGT 57561 || Eine Traumgeschichte',\n  'pDeM 39 || Herischef und General Meryre',\n  0.23672439895953123),\n ('oTurin Cat. 9587 = oTurin CGT 57561 || Eine Traumgeschichte',\n  'pMoskau 120 || Recto: Die Reise des Wenamun',\n  0.26580540049086565),\n ('oTurin Cat. 9587 = oTurin CGT 57561 || Eine Traumgeschichte',\n  'pHarris 500 = pBM EA 10060 || Verso 1-3: Die Einnahme von Joppe',\n  0.3028342031471041),\n ('pAshmolean 1960.1265a-g || ErzÃ¤hlung vom KÃ¶nig und dem Vorsteher der JÃ¤ger',\n  'pBerlin P 3020 + pWien 36 || ErzÃ¤hlung von einem KÃ¶nig und einer GÃ¶ttin',\n  0.2544353407026728),\n ('pAshmolean 1960.1265a-g || ErzÃ¤hlung vom KÃ¶nig und dem Vorsteher der JÃ¤ger',\n  'pdâ€™Orbiney = pBM EA 10183 || Recto: Das ZweibrÃ¼dermÃ¤rchen',\n  0.32037028062283346),\n ('pAshmolean 1960.1265a-g || ErzÃ¤hlung vom KÃ¶nig und dem Vorsteher der JÃ¤ger',\n  'Topf 3: oLouvre N 667 + N 700 || Chonsemhab und der Geist, Kolumne 4',\n  0.39004110550180626),\n ('pBerlin P 3020 + pWien 36 || ErzÃ¤hlung von einem KÃ¶nig und einer GÃ¶ttin',\n  'pChester Beatty I || Recto: Der Streit zwischen Horus und Seth',\n  0.10930594491116696),\n ('pBerlin P 3020 + pWien 36 || ErzÃ¤hlung von einem KÃ¶nig und einer GÃ¶ttin',\n  'pdâ€™Orbiney = pBM EA 10183 || Recto: Das ZweibrÃ¼dermÃ¤rchen',\n  0.12370434552734222),\n ('pBerlin P 3020 + pWien 36 || ErzÃ¤hlung von einem KÃ¶nig und einer GÃ¶ttin',\n  'pChester Beatty II = pBM EA 10682 || Wahrheit und LÃ¼ge',\n  0.13119825836513532),\n ('pDeM 39 || Herischef und General Meryre',\n  'pHarris 500 = pBM EA 10060 || Verso 4-8: Das PrinzenmÃ¤rchen',\n  0.08398595980987666),\n ('pDeM 39 || Herischef und General Meryre',\n  'pdâ€™Orbiney = pBM EA 10183 || Recto: Das ZweibrÃ¼dermÃ¤rchen',\n  0.09059399836482251),\n ('pDeM 39 || Herischef und General Meryre',\n  'pChester Beatty II = pBM EA 10682 || Wahrheit und LÃ¼ge',\n  0.13305297492997048),\n ('pHarris 500 = pBM EA 10060 || Verso 1-3: Die Einnahme von Joppe',\n  'pHarris 500 = pBM EA 10060 || Verso 4-8: Das PrinzenmÃ¤rchen',\n  0.1128910953229848),\n ('pHarris 500 = pBM EA 10060 || Verso 1-3: Die Einnahme von Joppe',\n  'pdâ€™Orbiney = pBM EA 10183 || Recto: Das ZweibrÃ¼dermÃ¤rchen',\n  0.13208360165126543),\n ('pHarris 500 = pBM EA 10060 || Verso 1-3: Die Einnahme von Joppe',\n  'pChester Beatty II = pBM EA 10682 || Wahrheit und LÃ¼ge',\n  0.13717012251495708),\n ('pHarris 500 = pBM EA 10060 || Verso 4-8: Das PrinzenmÃ¤rchen',\n  'pdâ€™Orbiney = pBM EA 10183 || Recto: Das ZweibrÃ¼dermÃ¤rchen',\n  0.04099488175872956),\n ('pHarris 500 = pBM EA 10060 || Verso 4-8: Das PrinzenmÃ¤rchen',\n  'pChester Beatty II = pBM EA 10682 || Wahrheit und LÃ¼ge',\n  0.048121508574797356),\n ('pHarris 500 = pBM EA 10060 || Verso 4-8: Das PrinzenmÃ¤rchen',\n  'pDeM 39 || Herischef und General Meryre',\n  0.08398595980987666),\n (\"pLouvre N 3136 || Kriegsgeschichte Ramses' III.\",\n  'pHarris 500 = pBM EA 10060 || Verso 1-3: Die Einnahme von Joppe',\n  0.2159676330382796),\n (\"pLouvre N 3136 || Kriegsgeschichte Ramses' III.\",\n  'pMoskau 120 || Recto: Die Reise des Wenamun',\n  0.21782216456988135),\n (\"pLouvre N 3136 || Kriegsgeschichte Ramses' III.\",\n  'pChester Beatty II = pBM EA 10682 || Wahrheit und LÃ¼ge',\n  0.33795688480228103),\n ('pChassinat III = pLouvre E 25353 || Text 1: Mythologische ErzÃ¤hlung',\n  '1. pBM EA 10474 || Rto: Die Lehre des Amenemope',\n  0.1256111497855753),\n ('pChassinat III = pLouvre E 25353 || Text 1: Mythologische ErzÃ¤hlung',\n  '6. tTurin CGT 58005 || Die Lehre des Amenemope 24.1-25.9',\n  0.15653376484096093),\n ('pChassinat III = pLouvre E 25353 || Text 1: Mythologische ErzÃ¤hlung',\n  '5. pStockholm MM 18416 || Die Lehre des Amenemope 10.18-14.5',\n  0.17165858560116642),\n (\"pTurin Cat. 1940+1941+o.Nr. || Syrienfeldzug Thutmosis' III.\",\n  'pMoskau 127 = pPuschkin I, b, 127 || Recto: Der Moskauer literarische Brief (\"A Tale of Woe\")',\n  0.20567764314270787),\n (\"pTurin Cat. 1940+1941+o.Nr. || Syrienfeldzug Thutmosis' III.\",\n  '06. oLacau || Die Lehre des Amunnacht',\n  0.21822726110909607),\n (\"pTurin Cat. 1940+1941+o.Nr. || Syrienfeldzug Thutmosis' III.\",\n  'Topf 2: oFlorenz 2616 || Chonsemhab und der Geist, Kolumne 3',\n  0.21965940023177433),\n ('pChester Beatty II = pBM EA 10682 || Wahrheit und LÃ¼ge',\n  'pdâ€™Orbiney = pBM EA 10183 || Recto: Das ZweibrÃ¼dermÃ¤rchen',\n  0.04223161077779647),\n ('pChester Beatty II = pBM EA 10682 || Wahrheit und LÃ¼ge',\n  'pHarris 500 = pBM EA 10060 || Verso 4-8: Das PrinzenmÃ¤rchen',\n  0.048121508574797356),\n ('pChester Beatty II = pBM EA 10682 || Wahrheit und LÃ¼ge',\n  'pChester Beatty I || Recto: Der Streit zwischen Horus und Seth',\n  0.07089232409082058),\n ('pMoskau 127 = pPuschkin I, b, 127 || Recto: Der Moskauer literarische Brief (\"A Tale of Woe\")',\n  'oAshmolean Museum 1945.40 aus Deir el Medineh (AOS) || Sinuhe Koch 01-79',\n  0.1173595538847958),\n ('pMoskau 127 = pPuschkin I, b, 127 || Recto: Der Moskauer literarische Brief (\"A Tale of Woe\")',\n  'pBerlin P 3022 und Fragmente pAmherst m-q (B) || Sinuhe Koch 04-81',\n  0.1332315837800978),\n ('pMoskau 127 = pPuschkin I, b, 127 || Recto: Der Moskauer literarische Brief (\"A Tale of Woe\")',\n  'pMoskau o.Nr. (Pleasures of Fishing) || The Pleasures of Fishing and Fowling',\n  0.16258209574584026),\n ('oChicago OIC 12074 + oIFAO Inv. 2188 || Die \"Lehre\" des Menena',\n  '01. oKV 18/3.614+627 || Die Lehre des Amunnacht',\n  0.10804436105079462),\n ('oChicago OIC 12074 + oIFAO Inv. 2188 || Die \"Lehre\" des Menena',\n  '09. pDeM 1 (D) || Recto: Die Lehre des Ani (Version D)',\n  0.16065661376710483),\n ('oChicago OIC 12074 + oIFAO Inv. 2188 || Die \"Lehre\" des Menena',\n  '06. oLacau || Die Lehre des Amunnacht',\n  0.16967505752787715),\n ('1. pBM EA 10474 || Rto: Die Lehre des Amenemope',\n  '02. pBoulaq 4, Rto (B) || Recto: Die Lehre des Ani (Version B)',\n  0.0840527890078756),\n ('1. pBM EA 10474 || Rto: Die Lehre des Amenemope',\n  '6. tTurin CGT 58005 || Die Lehre des Amenemope 24.1-25.9',\n  0.10896391747285128),\n ('1. pBM EA 10474 || Rto: Die Lehre des Amenemope',\n  '09. pDeM 1 (D) || Recto: Die Lehre des Ani (Version D)',\n  0.11211758419714479),\n ('3. oKairo SR 1840 || Die Lehre des Amenemope 3.11-4.10',\n  'oDeM 1632 || Eine Sammlung von Verboten',\n  0.17245034926765945),\n ('3. oKairo SR 1840 || Die Lehre des Amenemope 3.11-4.10',\n  'oChicago OIC 12074 + oIFAO Inv. 2188 || Die \"Lehre\" des Menena',\n  0.2130948024904714),\n ('3. oKairo SR 1840 || Die Lehre des Amenemope 3.11-4.10',\n  '06. pChester Beatty V = pBM EA 10685 || Verso 2.6-11: Die Lehre des Ani (Version L)',\n  0.21913237859321677),\n ('5. pStockholm MM 18416 || Die Lehre des Amenemope 10.18-14.5',\n  '1. pBM EA 10474 || Rto: Die Lehre des Amenemope',\n  0.15051480421397756),\n ('5. pStockholm MM 18416 || Die Lehre des Amenemope 10.18-14.5',\n  'pChassinat III = pLouvre E 25353 || Text 1: Mythologische ErzÃ¤hlung',\n  0.17165858560116642),\n ('5. pStockholm MM 18416 || Die Lehre des Amenemope 10.18-14.5',\n  '02. pBoulaq 4, Rto (B) || Recto: Die Lehre des Ani (Version B)',\n  0.2062452990770638),\n ('6. tTurin CGT 58005 || Die Lehre des Amenemope 24.1-25.9',\n  '1. pBM EA 10474 || Rto: Die Lehre des Amenemope',\n  0.10896391747285128),\n ('6. tTurin CGT 58005 || Die Lehre des Amenemope 24.1-25.9',\n  'pChassinat III = pLouvre E 25353 || Text 1: Mythologische ErzÃ¤hlung',\n  0.15653376484096093),\n ('6. tTurin CGT 58005 || Die Lehre des Amenemope 24.1-25.9',\n  '02. pBoulaq 4, Rto (B) || Recto: Die Lehre des Ani (Version B)',\n  0.20007374728790805),\n ('01. oKV 18/3.614+627 || Die Lehre des Amunnacht',\n  '02. oBM EA 41541 || Die Lehre des Amunnacht',\n  0.06929969453037055),\n ('01. oKV 18/3.614+627 || Die Lehre des Amunnacht',\n  'oChicago OIC 12074 + oIFAO Inv. 2188 || Die \"Lehre\" des Menena',\n  0.10804436105079462),\n ('01. oKV 18/3.614+627 || Die Lehre des Amunnacht',\n  '03. oGrdseloff || Die Lehre des Amunnacht',\n  0.17959542158363329),\n ('02. oBM EA 41541 || Die Lehre des Amunnacht',\n  '01. oKV 18/3.614+627 || Die Lehre des Amunnacht',\n  0.06929969453037055),\n ('02. oBM EA 41541 || Die Lehre des Amunnacht',\n  '03. oGrdseloff || Die Lehre des Amunnacht',\n  0.1560232510828472),\n ('02. oBM EA 41541 || Die Lehre des Amunnacht',\n  'oChicago OIC 12074 + oIFAO Inv. 2188 || Die \"Lehre\" des Menena',\n  0.2323133720545425),\n ('03. oGrdseloff || Die Lehre des Amunnacht',\n  '02. oBM EA 41541 || Die Lehre des Amunnacht',\n  0.1560232510828472),\n ('03. oGrdseloff || Die Lehre des Amunnacht',\n  '01. oKV 18/3.614+627 || Die Lehre des Amunnacht',\n  0.17959542158363329),\n ('03. oGrdseloff || Die Lehre des Amunnacht',\n  'oChicago OIC 12074 + oIFAO Inv. 2188 || Die \"Lehre\" des Menena',\n  0.30448234034722643),\n ('06. oLacau || Die Lehre des Amunnacht',\n  'oChicago OIC 12074 + oIFAO Inv. 2188 || Die \"Lehre\" des Menena',\n  0.16967505752787715),\n ('06. oLacau || Die Lehre des Amunnacht',\n  '02. pBoulaq 4, Rto (B) || Recto: Die Lehre des Ani (Version B)',\n  0.21538094054546542),\n ('06. oLacau || Die Lehre des Amunnacht',\n  \"pTurin Cat. 1940+1941+o.Nr. || Syrienfeldzug Thutmosis' III.\",\n  0.21822726110909607),\n ('02. pBoulaq 4, Rto (B) || Recto: Die Lehre des Ani (Version B)',\n  '09. pDeM 1 (D) || Recto: Die Lehre des Ani (Version D)',\n  0.02976798902951816),\n ('02. pBoulaq 4, Rto (B) || Recto: Die Lehre des Ani (Version B)',\n  '1. pBM EA 10474 || Rto: Die Lehre des Amenemope',\n  0.0840527890078756),\n ('02. pBoulaq 4, Rto (B) || Recto: Die Lehre des Ani (Version B)',\n  '06. pChester Beatty V = pBM EA 10685 || Verso 2.6-11: Die Lehre des Ani (Version L)',\n  0.09177697633869375),\n ('06. pChester Beatty V = pBM EA 10685 || Verso 2.6-11: Die Lehre des Ani (Version L)',\n  '09. pDeM 1 (D) || Recto: Die Lehre des Ani (Version D)',\n  0.09157594924602852),\n ('06. pChester Beatty V = pBM EA 10685 || Verso 2.6-11: Die Lehre des Ani (Version L)',\n  '02. pBoulaq 4, Rto (B) || Recto: Die Lehre des Ani (Version B)',\n  0.09177697633869375),\n ('06. pChester Beatty V = pBM EA 10685 || Verso 2.6-11: Die Lehre des Ani (Version L)',\n  '1. pBM EA 10474 || Rto: Die Lehre des Amenemope',\n  0.1851678291168679),\n ('09. pDeM 1 (D) || Recto: Die Lehre des Ani (Version D)',\n  '02. pBoulaq 4, Rto (B) || Recto: Die Lehre des Ani (Version B)',\n  0.02976798902951816),\n ('09. pDeM 1 (D) || Recto: Die Lehre des Ani (Version D)',\n  '06. pChester Beatty V = pBM EA 10685 || Verso 2.6-11: Die Lehre des Ani (Version L)',\n  0.09157594924602852),\n ('09. pDeM 1 (D) || Recto: Die Lehre des Ani (Version D)',\n  '1. pBM EA 10474 || Rto: Die Lehre des Amenemope',\n  0.11211758419714479),\n ('11. pSaqqara EES 75-S 45 = SCA 6319 (S) || Die Lehre des Ani (Version S)',\n  '02. pBoulaq 4, Rto (B) || Recto: Die Lehre des Ani (Version B)',\n  0.14725496452872167),\n ('11. pSaqqara EES 75-S 45 = SCA 6319 (S) || Die Lehre des Ani (Version S)',\n  '09. pDeM 1 (D) || Recto: Die Lehre des Ani (Version D)',\n  0.19751927314999485),\n ('11. pSaqqara EES 75-S 45 = SCA 6319 (S) || Die Lehre des Ani (Version S)',\n  '1. pBM EA 10474 || Rto: Die Lehre des Amenemope',\n  0.226790004736372),\n ('12. pGuimet 16959 = pLouvre E 30144 (G) || Die Lehre des Ani (Version G)',\n  '02. pBoulaq 4, Rto (B) || Recto: Die Lehre des Ani (Version B)',\n  0.12944044145457367),\n ('12. pGuimet 16959 = pLouvre E 30144 (G) || Die Lehre des Ani (Version G)',\n  '09. pDeM 1 (D) || Recto: Die Lehre des Ani (Version D)',\n  0.17074741353968648),\n ('12. pGuimet 16959 = pLouvre E 30144 (G) || Die Lehre des Ani (Version G)',\n  '11. pAnastasi VII = pBM EA 10222 || Kol. 1-7: Die Lehre des Cheti 10.1-30.6',\n  0.24032277453794093),\n ('oGardiner 2 || Die Lehre des Hori',\n  '09. pDeM 1 (D) || Recto: Die Lehre des Ani (Version D)',\n  0.15989410312744567),\n ('oGardiner 2 || Die Lehre des Hori',\n  '02. pBoulaq 4, Rto (B) || Recto: Die Lehre des Ani (Version B)',\n  0.160859038090854),\n ('oGardiner 2 || Die Lehre des Hori',\n  '1. pBM EA 10474 || Rto: Die Lehre des Amenemope',\n  0.20329155067380578),\n ('oDeM 1632 || Eine Sammlung von Verboten',\n  'oPetrie 11 = oLondon UC 39614 || Eine Sammlung von Verboten',\n  0.07864528316235864),\n ('oDeM 1632 || Eine Sammlung von Verboten',\n  'hintere Innenwand || Rahmentext',\n  0.09879212266110149),\n ('oDeM 1632 || Eine Sammlung von Verboten',\n  '3. oKairo SR 1840 || Die Lehre des Amenemope 3.11-4.10',\n  0.17245034926765945),\n ('oPetrie 11 = oLondon UC 39614 || Eine Sammlung von Verboten',\n  'oDeM 1632 || Eine Sammlung von Verboten',\n  0.07864528316235864),\n ('oPetrie 11 = oLondon UC 39614 || Eine Sammlung von Verboten',\n  'hintere Innenwand || Rahmentext',\n  0.11379246347055172),\n ('oPetrie 11 = oLondon UC 39614 || Eine Sammlung von Verboten',\n  '09. pDeM 1 (D) || Recto: Die Lehre des Ani (Version D)',\n  0.15295536086894101),\n ('pBerlin P 10499 aus Theben-West (R) || Verso: Sinuhe Koch 01-58',\n  'pBerlin P 3022 und Fragmente pAmherst m-q (B) || Sinuhe Koch 04-81',\n  0.05673339971248825),\n ('pBerlin P 10499 aus Theben-West (R) || Verso: Sinuhe Koch 01-58',\n  'oAshmolean Museum 1945.40 aus Deir el Medineh (AOS) || Sinuhe Koch 01-79',\n  0.07572323120208901),\n ('pBerlin P 10499 aus Theben-West (R) || Verso: Sinuhe Koch 01-58',\n  'pMoskau 4657 (Golenischeff) (G) || Sinuhe Koch 01-37',\n  0.09140607322390293),\n ('pChester Beatty I || Recto: Der Streit zwischen Horus und Seth',\n  'pdâ€™Orbiney = pBM EA 10183 || Recto: Das ZweibrÃ¼dermÃ¤rchen',\n  0.0693176472067848),\n ('pChester Beatty I || Recto: Der Streit zwischen Horus und Seth',\n  'pChester Beatty II = pBM EA 10682 || Wahrheit und LÃ¼ge',\n  0.07089232409082058),\n ('pChester Beatty I || Recto: Der Streit zwischen Horus und Seth',\n  'pHarris 500 = pBM EA 10060 || Verso 4-8: Das PrinzenmÃ¤rchen',\n  0.08675540954459326),\n ('oAshmolean Museum 1945.40 aus Deir el Medineh (AOS) || Sinuhe Koch 01-79',\n  'pBerlin P 3022 und Fragmente pAmherst m-q (B) || Sinuhe Koch 04-81',\n  0.009154979039167954),\n ('oAshmolean Museum 1945.40 aus Deir el Medineh (AOS) || Sinuhe Koch 01-79',\n  'pBerlin P 10499 aus Theben-West (R) || Verso: Sinuhe Koch 01-58',\n  0.07572323120208901),\n ('oAshmolean Museum 1945.40 aus Deir el Medineh (AOS) || Sinuhe Koch 01-79',\n  'pPetersburg 1115 || Die Geschichte des SchiffbrÃ¼chigen',\n  0.10744325997426307),\n ('oBM EA 5629 aus Theben-West (L) || Sinuhe Koch 80-81',\n  'pBerlin P 3022 und Fragmente pAmherst m-q (B) || Sinuhe Koch 04-81',\n  0.17617803642067853),\n ('oBM EA 5629 aus Theben-West (L) || Sinuhe Koch 80-81',\n  'pBerlin P 10499 aus Theben-West (R) || Verso: Sinuhe Koch 01-58',\n  0.20709482118179068),\n ('oBM EA 5629 aus Theben-West (L) || Sinuhe Koch 80-81',\n  'oAshmolean Museum 1945.40 aus Deir el Medineh (AOS) || Sinuhe Koch 01-79',\n  0.20890658451664268),\n ('oKairo CG 25216 aus dem Grab des Sennedjem TT 1 (C) || Recto Sinuhe Koch 01-22',\n  'pBerlin P 10499 aus Theben-West (R) || Verso: Sinuhe Koch 01-58',\n  0.10027191775411881),\n ('oKairo CG 25216 aus dem Grab des Sennedjem TT 1 (C) || Recto Sinuhe Koch 01-22',\n  'pBerlin P 3022 und Fragmente pAmherst m-q (B) || Sinuhe Koch 04-81',\n  0.10696143954589288),\n ('oKairo CG 25216 aus dem Grab des Sennedjem TT 1 (C) || Recto Sinuhe Koch 01-22',\n  'oAshmolean Museum 1945.40 aus Deir el Medineh (AOS) || Sinuhe Koch 01-79',\n  0.11203493320230862),\n ('pMoskau 4657 (Golenischeff) (G) || Sinuhe Koch 01-37',\n  'pBerlin P 10499 aus Theben-West (R) || Verso: Sinuhe Koch 01-58',\n  0.09140607322390293),\n ('pMoskau 4657 (Golenischeff) (G) || Sinuhe Koch 01-37',\n  'oKairo CG 25216 aus dem Grab des Sennedjem TT 1 (C) || Recto Sinuhe Koch 01-22',\n  0.1241286828088296),\n ('pMoskau 4657 (Golenischeff) (G) || Sinuhe Koch 01-37',\n  'pBerlin P 3022 und Fragmente pAmherst m-q (B) || Sinuhe Koch 04-81',\n  0.1577361655624817),\n ('pSallier II = pBM EA 10182 || Kol. 1-3: Die Lehre des Amenemhet',\n  'pMillingen || Die Lehre des Amenemhet',\n  0.06388867314892821),\n ('pSallier II = pBM EA 10182 || Kol. 1-3: Die Lehre des Amenemhet',\n  'pMoskau o.Nr. (Pleasures of Fishing) || The Pleasures of Fishing and Fowling',\n  0.09458896880318401),\n ('pSallier II = pBM EA 10182 || Kol. 1-3: Die Lehre des Amenemhet',\n  'pAmherst 3 + pBerlin P 3024 || Der LebensmÃ¼de',\n  0.09625210333292322),\n ('01. pSallier II = pBM EA 10182 || Kol. 3-11: Die Lehre des Cheti 1.1-30.6',\n  '11. pAnastasi VII = pBM EA 10222 || Kol. 1-7: Die Lehre des Cheti 10.1-30.6',\n  0.014539136290222299),\n ('01. pSallier II = pBM EA 10182 || Kol. 3-11: Die Lehre des Cheti 1.1-30.6',\n  '13. tLouvre N 693 || Die Lehre des Cheti 13.4-22.2 und 30.1-6; Nilhymnus 1.1-2.6',\n  0.11742270565667412),\n ('01. pSallier II = pBM EA 10182 || Kol. 3-11: Die Lehre des Cheti 1.1-30.6',\n  '02. oDeM 1014+1478 || Die Lehre des Cheti 1.1-6.2 und 29.1-30.6',\n  0.11978302852505296),\n ('02. oDeM 1014+1478 || Die Lehre des Cheti 1.1-6.2 und 29.1-30.6',\n  'pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1)',\n  0.10092805821534523),\n ('02. oDeM 1014+1478 || Die Lehre des Cheti 1.1-6.2 und 29.1-30.6',\n  '01. pSallier II = pBM EA 10182 || Kol. 3-11: Die Lehre des Cheti 1.1-30.6',\n  0.11978302852505296),\n ('02. oDeM 1014+1478 || Die Lehre des Cheti 1.1-6.2 und 29.1-30.6',\n  '11. pAnastasi VII = pBM EA 10222 || Kol. 1-7: Die Lehre des Cheti 10.1-30.6',\n  0.13360316403874284),\n ('03. oGenÃ¨ve 12551 || Die Lehre des Cheti 1.1-3.6',\n  '11. pAnastasi VII = pBM EA 10222 || Kol. 1-7: Die Lehre des Cheti 10.1-30.6',\n  0.19040447064838606),\n ('03. oGenÃ¨ve 12551 || Die Lehre des Cheti 1.1-3.6',\n  '01. pSallier II = pBM EA 10182 || Kol. 3-11: Die Lehre des Cheti 1.1-30.6',\n  0.19157932913732623),\n ('03. oGenÃ¨ve 12551 || Die Lehre des Cheti 1.1-3.6',\n  '02. oDeM 1014+1478 || Die Lehre des Cheti 1.1-6.2 und 29.1-30.6',\n  0.2083382079061129),\n ('09. oPetrie 40 = oLondon UC 39639 || Die Lehre des Cheti 9.1-10.1',\n  '12. oKairo CG 25217 || AuÃŸen: Die Lehre des Cheti 13.1-14.1',\n  0.1066700223675453),\n ('09. oPetrie 40 = oLondon UC 39639 || Die Lehre des Cheti 9.1-10.1',\n  '14. oBM EA 29550 + oDeM 1546 || Die Lehre des Cheti 14.1-21.2',\n  0.14292762944361836),\n ('09. oPetrie 40 = oLondon UC 39639 || Die Lehre des Cheti 9.1-10.1',\n  '13. tLouvre N 693 || Die Lehre des Cheti 13.4-22.2 und 30.1-6; Nilhymnus 1.1-2.6',\n  0.15083989259980135),\n ('10. pAmherst 14 || Die Lehre des Cheti 9.5-13.1',\n  '09. oPetrie 40 = oLondon UC 39639 || Die Lehre des Cheti 9.1-10.1',\n  0.1762361073773736),\n ('10. pAmherst 14 || Die Lehre des Cheti 9.5-13.1',\n  '11. pAnastasi VII = pBM EA 10222 || Kol. 1-7: Die Lehre des Cheti 10.1-30.6',\n  0.17674646201746325),\n ('10. pAmherst 14 || Die Lehre des Cheti 9.5-13.1',\n  '14. oBM EA 29550 + oDeM 1546 || Die Lehre des Cheti 14.1-21.2',\n  0.1910465493841914),\n ('11. pAnastasi VII = pBM EA 10222 || Kol. 1-7: Die Lehre des Cheti 10.1-30.6',\n  '01. pSallier II = pBM EA 10182 || Kol. 3-11: Die Lehre des Cheti 1.1-30.6',\n  0.014539136290222299),\n ('11. pAnastasi VII = pBM EA 10222 || Kol. 1-7: Die Lehre des Cheti 10.1-30.6',\n  '13. tLouvre N 693 || Die Lehre des Cheti 13.4-22.2 und 30.1-6; Nilhymnus 1.1-2.6',\n  0.12767917043654298),\n ('11. pAnastasi VII = pBM EA 10222 || Kol. 1-7: Die Lehre des Cheti 10.1-30.6',\n  'pTurin CGT 54033 (alias: pTurin CGT 54031) || Verso: ErzÃ¤hlung von einem GÃ¶tterstreit',\n  0.1295185259282089),\n ('12. oKairo CG 25217 || AuÃŸen: Die Lehre des Cheti 13.1-14.1',\n  '14. oBM EA 29550 + oDeM 1546 || Die Lehre des Cheti 14.1-21.2',\n  0.10662033210814359),\n ('12. oKairo CG 25217 || AuÃŸen: Die Lehre des Cheti 13.1-14.1',\n  '09. oPetrie 40 = oLondon UC 39639 || Die Lehre des Cheti 9.1-10.1',\n  0.1066700223675453),\n ('12. oKairo CG 25217 || AuÃŸen: Die Lehre des Cheti 13.1-14.1',\n  '15. oBodmer Inv. 12 || Die Lehre des Cheti 16.1-18.2',\n  0.11322813035886936),\n ('13. tLouvre N 693 || Die Lehre des Cheti 13.4-22.2 und 30.1-6; Nilhymnus 1.1-2.6',\n  '14. oBM EA 29550 + oDeM 1546 || Die Lehre des Cheti 14.1-21.2',\n  0.04977367146557221),\n ('13. tLouvre N 693 || Die Lehre des Cheti 13.4-22.2 und 30.1-6; Nilhymnus 1.1-2.6',\n  '01. pSallier II = pBM EA 10182 || Kol. 3-11: Die Lehre des Cheti 1.1-30.6',\n  0.11742270565667412),\n ('13. tLouvre N 693 || Die Lehre des Cheti 13.4-22.2 und 30.1-6; Nilhymnus 1.1-2.6',\n  '11. pAnastasi VII = pBM EA 10222 || Kol. 1-7: Die Lehre des Cheti 10.1-30.6',\n  0.12767917043654298),\n ('14. oBM EA 29550 + oDeM 1546 || Die Lehre des Cheti 14.1-21.2',\n  '13. tLouvre N 693 || Die Lehre des Cheti 13.4-22.2 und 30.1-6; Nilhymnus 1.1-2.6',\n  0.04977367146557221),\n ('14. oBM EA 29550 + oDeM 1546 || Die Lehre des Cheti 14.1-21.2',\n  '12. oKairo CG 25217 || AuÃŸen: Die Lehre des Cheti 13.1-14.1',\n  0.10662033210814359),\n ('14. oBM EA 29550 + oDeM 1546 || Die Lehre des Cheti 14.1-21.2',\n  '09. oPetrie 40 = oLondon UC 39639 || Die Lehre des Cheti 9.1-10.1',\n  0.14292762944361836),\n ('15. oBodmer Inv. 12 || Die Lehre des Cheti 16.1-18.2',\n  'pLondon UC 32157 = pKahun LV.1 || Verso: Die Geschichte von Hay',\n  0.09130881280694036),\n ('15. oBodmer Inv. 12 || Die Lehre des Cheti 16.1-18.2',\n  '12. oKairo CG 25217 || AuÃŸen: Die Lehre des Cheti 13.1-14.1',\n  0.11322813035886936),\n ('15. oBodmer Inv. 12 || Die Lehre des Cheti 16.1-18.2',\n  '08. oBerlin P 14356 || Die Lehre eines Mannes fÃ¼r seinen Sohn 7.1-9.1',\n  0.13488395016666233),\n ('16. oDeM 1013 || Die Lehre des Cheti 23.1-25.3',\n  '17. oTurin CGT 57082 || Die Lehre des Cheti 25.1-29.1',\n  0.17010805074450563),\n ('16. oDeM 1013 || Die Lehre des Cheti 23.1-25.3',\n  '18. oOIC 16999 || Die Lehre eines Mannes fÃ¼r seinen Sohn 15.x+1-17.3',\n  0.18504020723706516),\n ('16. oDeM 1013 || Die Lehre des Cheti 23.1-25.3',\n  '09. pDeM 1 (D) || Recto: Die Lehre des Ani (Version D)',\n  0.2492101398475497),\n ('17. oTurin CGT 57082 || Die Lehre des Cheti 25.1-29.1',\n  '16. oDeM 1013 || Die Lehre des Cheti 23.1-25.3',\n  0.17010805074450563),\n ('17. oTurin CGT 57082 || Die Lehre des Cheti 25.1-29.1',\n  'pBM EA 10371+10435 (Ptahhotep, Version L1) || Die Lehre des Ptahhotep (Version L1)',\n  0.21049666409826218),\n ('17. oTurin CGT 57082 || Die Lehre des Cheti 25.1-29.1',\n  'pBM EA 10509 (Ptahhotep, Version L2+L2G) || Die Lehre des Ptahhotep (Version L2+L2G)',\n  0.24258848754119844),\n ('pChassinat II = pLouvre E 25352 || Die Geistergeschichte des pChassinat II',\n  'pLondon UC 32157 = pKahun LV.1 || Verso: Die Geschichte von Hay',\n  0.12884760999391454),\n ('pChassinat II = pLouvre E 25352 || Die Geistergeschichte des pChassinat II',\n  '01. pSallier II = pBM EA 10182 || Kol. 3-11: Die Lehre des Cheti 1.1-30.6',\n  0.1442519195311337),\n ('pChassinat II = pLouvre E 25352 || Die Geistergeschichte des pChassinat II',\n  'pTurin CGT 54033 (alias: pTurin CGT 54031) || Verso: ErzÃ¤hlung von einem GÃ¶tterstreit',\n  0.14774903958953634),\n ('03. pChassinat I = pLouvre E 25351 || Die Geschichte von KÃ¶nig Neferkare und General Sasenet',\n  'pLondon UC 32157 = pKahun LV.1 || Verso: Die Geschichte von Hay',\n  0.14987510106846547),\n ('03. pChassinat I = pLouvre E 25351 || Die Geschichte von KÃ¶nig Neferkare und General Sasenet',\n  '12. oKairo CG 25217 || AuÃŸen: Die Lehre des Cheti 13.1-14.1',\n  0.1700264190118077),\n ('03. pChassinat I = pLouvre E 25351 || Die Geschichte von KÃ¶nig Neferkare und General Sasenet',\n  '15. oBodmer Inv. 12 || Die Lehre des Cheti 16.1-18.2',\n  0.18238956451311195),\n ('Stele Louvre C 284 (\"Bentresch-Stele\") || Stele Louvre C 284 (\"Bentresch-Stele\")',\n  'pWestcar = pBerlin P 3033 || Die ErzÃ¤hlungen des pWestcar',\n  0.15020921968958967),\n ('Stele Louvre C 284 (\"Bentresch-Stele\") || Stele Louvre C 284 (\"Bentresch-Stele\")',\n  '03. pChassinat I = pLouvre E 25351 || Die Geschichte von KÃ¶nig Neferkare und General Sasenet',\n  0.1907124934252078),\n ('Stele Louvre C 284 (\"Bentresch-Stele\") || Stele Louvre C 284 (\"Bentresch-Stele\")',\n  'pLythgoe = pMMA 09.180.535 || Die Geschichte des pLythgoe',\n  0.25125878382929623)]\n\n\n\n# Create the NetworkX Graph\nG = nx.Graph()\n\n\n# Get a list of nodes\nlist_of_nodes = list(G.nodes())\nprint(\"List of Nodes:\")\nfor node in  list_of_nodes:\n  print(node)\n\nList of Nodes:\n\n\n\n# Get a list of edges\nlist_of_edges = list(G.edges())\nprint(\"List of Edges:\")\nfor edge in  list_of_edges:\n  print(edge)\n\nList of Edges:\n\n\n\n# Add edges to the graph (assuming filtered_edges is already defined)\nfor text, similar_text, similarity in filtered_edges:\n    G.add_edge(text, similar_text, weight=similarity)\n\n# Visualize the Network with improved aesthetics and more spread out\nplt.figure(figsize=(25, 25))\n\n# Use the spring_layout with adjusted parameters for better spacing\npos = nx.spring_layout(G, seed=42, k=1, iterations=5000, weight='weight')  # Adjust `k` to spread out the nodes\n\n# Calculate node degrees\ndegrees = dict(G.degree())\nnode_sizes = [200 + 100 * degrees[n] for n in G.nodes()]  # Node size based on degree\n\n# Normalize node degrees for colormap\nmax_degree = max(degrees.values())\nnode_colors = [degrees[n] / max_degree for n in G.nodes()]  # Normalized degree for colormap\n\n# Invert the node colors to ensure higher degrees are darker\n#node_colors = [color for color in node_colors]  # Invert the color mapping\n\n# Create a custom monochrome blue colormap\nmonochrome_cmap = LinearSegmentedColormap.from_list(\"monochrome_blue\", [\"lightblue\", \"darkblue\"])\n\n# Draw nodes\nnx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, cmap=monochrome_cmap)\n\n# Draw edges with varying widths based on similarity\nedges = G.edges(data=True)\n#weights = [1 / (edge[2]['weight'] * 2) if edge[2]['weight'] != 0 else 0 for edge in edges]  # Invert weights for better visualization\n\nnx.draw_networkx_edges(G, pos, edgelist=edges, edge_color='gray', alpha=0.5)\n\n# Draw labels\nnx.draw_networkx_labels(G, pos, font_size=6, font_family='sans-serif')\n\n# Add a colorbar for node colors\nsm = plt.cm.ScalarMappable(cmap=monochrome_cmap, norm=plt.Normalize(vmin=0, vmax=1))\nsm.set_array([])\nplt.colorbar(sm)\n\nplt.title(\"Network Visualization of Texts Based on Cosine Similarity of TF-IDF\")\nplt.show()\n#plt.savefig(\"Network_EgLiterature.png\", format=\"png\", dpi=300)\n\nMatplotlibDeprecationWarning: Unable to determine Axes to steal space for Colorbar. Using gca(), but will raise in the future. Either provide the *cax* argument to use as the Axes for the Colorbar, provide the *ax* argument to steal space from it, or add *mappable* to an Axes.\n  plt.colorbar(sm)\n\n\n\n\n\n\n\n\n\n\n## with genres for colour\n\n# Create the NetworkX Graph\nG = nx.Graph()\n\n# Add edges to the graph (assuming filtered_edges is already defined)\nfor text, similar_text, similarity in filtered_edges:\n  # Scale the similarity to increase its influence\n    scaled_similarity = similarity * 10  # Increase the influence of weights\n    G.add_edge(text, similar_text, weight=similarity)\n\n# Visualize the Network with improved aesthetics and more spread out\nplt.figure(figsize=(25, 25))\n\n# Use the spring_layout with adjusted parameters for better spacing\npos = nx.spring_layout(G, seed=42, k=0.1, iterations=600, weight='weight', scale=2.0)  # Adjust `k` to spread out the nodes\n\n# Calculate node degrees\ndegrees = dict(G.degree())\nnode_sizes = [200 + 100 * degrees[n] for n in G.nodes()]  # Node size based on degree\n\n# Map genres to colors\ngenres = metadata['corpus_manual'].unique()\nnum_genres = len(genres)\ncolor_map = plt.get_cmap('tab20', num_genres)  # Use a colormap with the right number of colors\ngenre_to_color = {genre: to_hex(color_map(i / len(genres))) for i, genre in enumerate(genres)}\n\n# Apply the color mapping\nnode_colors = [genre_to_color[corpus_metadata.at[node, 'corpus_manual']] for node in G.nodes()]\n\n# Create a custom monochrome colormap from light gray to black\nmonochrome_cmap = LinearSegmentedColormap.from_list(\"monochrome_blue\", [\"lightblue\", \"darkblue\"])\n\n# Draw nodes\nnx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, cmap=monochrome_cmap)\n\n# Draw edges with varying widths based on similarity\nedges = G.edges(data=True)\nweights = [1 / (edge[2]['weight'] * 2) if edge[2]['weight'] != 0 else 0 for edge in edges]  # Invert weights for better visualization\n\nnx.draw_networkx_edges(G, pos, edgelist=edges, width=weights, edge_color='gray', alpha=0.5)\n\n# Draw labels\nnx.draw_networkx_labels(G, pos, font_size=6, font_family='sans-serif')\n\n# Add a colorbar for node colors\nsm = plt.cm.ScalarMappable(cmap=color_map, norm=plt.Normalize(vmin=0, vmax=len(genres)))\nsm.set_array([])\nplt.colorbar(sm, ticks=range(len(genres)), format=lambda x, _: genres[int(x)])\n\nplt.title(\"Improved Network Visualization of Texts based on Cosine Similarity of TF-IDF\")\nplt.show()\n#plt.savefig(\"Network_EgLiterature.png\", format=\"png\", dpi=300)\n\n/usr/local/lib/python3.10/dist-packages/networkx/drawing/nx_pylab.py:450: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  node_collection = ax.scatter(\n&lt;ipython-input-36-6adc31c4d41e&gt;:49: MatplotlibDeprecationWarning: Unable to determine Axes to steal space for Colorbar. Using gca(), but will raise in the future. Either provide the *cax* argument to use as the Axes for the Colorbar, provide the *ax* argument to steal space from it, or add *mappable* to an Axes.\n  plt.colorbar(sm, ticks=range(len(genres)), format=lambda x, _: genres[int(x)])\n\n\n\n\n\n\n\n\n\n\nExploratory network analysis\nThe field of graph theory began in the Prussian city of KÃ¶nigsberg. Leonhard Euler, who did not live in the city, published in 1736 his solution to what is now considered the foundational problem of graph theory: the â€œKÃ¶nigsberg problem of seven bridgesâ€ (problema Regiomontanum de septem pontibus). The problem questioned whether one can find a single path crossing only once each of the seven bridges over the river Pregel. In the terminology of network analysis: is there a set of connected nodes that crosses every edge only once. This is easily applicable to the field of textual manuscripts: are there certain manuscripts or genres that could be crossed or had to be crossed to get from text A to text B, and then, what were the ramifications of that on or for scholarly agents or officials using the text?\n\n\n\nKonigsberg_problem.jpg\n\n\nEuler showed that in the case of the KÃ¶nigsberg multigraph, there isnâ€™t a path that crosses every bridge â€“ i.e.Â passes every edge (in graph theory also E) â€“ only once. This led to the general conclusion that when there are nodes or vertices (in graph theory V) with an odd number of edges, it affects the characteristics of the network as a whole. Modern social network analysis terms this â€œthe number of nodes of odd degreeâ€, meaning, the number of nodes that have an odd number of edges. If a network has either 0 or exactly 2 nodes of odd degree, then there exists a path which follows each edge only once. This is termed a Eulerian network or graph. But, if there is 1 node of odd degree, or 3 or more nodes of odd degree, it is impossible to follow every edge only once.\n\n\n\nNetworks.jpg\n\n\nFour types of graph representations from Perez and Germon (2016: Fig 7.3): The graph is composed of four nodes and three edges connecting between them; it is easy to see from the adjacency matrix and adjacency list that all nodes are of odd degree. An example of a path is {Aâ†’Bâ†’C}, which is the path from node A to node C.\nThe Eulerian example is just one of many known quantitative aspects and theoretical models applicable to many graphs and networks describing real world situations, from roads, to power grids, to social and economic relations. Thus, the mathematics of graph theory has the potential to reveal dynamic patterns in historical networks, understanding the movement of entities inside the graph and how they relate to each other, especially in graphs that are too big for humans to analyze visually. In other words, applying mathematical aspects of graph theory can achieve new insights into networks, instead of them remaining static data visualizations, as often happens in the humanities.\nGraph theory is a rich field in mathematics, with many underlying principles on the inherent nature of certain types of behaviors within a network, that is based on mathematical formulae. Like the example of Euler above, we can learn more about the nature of the relationships in our networks, if we understand even a little bit about the principles governing the relationships within the data. Here we will present a selection of fundamental terms and concepts in graph theory that are easily applicable to historical data: 1. Degree distribution 2. Scale free network 3. Betweeness centrality 4. Hubs 5. Bridges\n\ndef graph_info(G):\n    print(\"Graph information:\")\n    print(\"Type of graph:\", type(G).__name__)\n    print(\"Number of nodes:\", G.number_of_nodes())\n    print(\"Number of edges:\", G.number_of_edges())\n    print(\"Degree:\", G.degree())\n    print(\"Average degree:\", sum(dict(G.degree()).values()) / G.number_of_nodes())\n    print(\"Betweenness centrality:\", nx.betweenness_centrality(G))\n    print(\"Degree centrality:\", nx.degree_centrality(G))\n    print(\"Eigenvector centrality:\", nx.eigenvector_centrality(G, max_iter=500))\n\n\n# Get a summary of the graph\ninfo = graph_info(G)\nprint(info)\n\nGraph information:\nType of graph: Graph\nNumber of nodes: 124\nNumber of edges: 289\nDegree: [('pVandier = pLille 139 || Recto: Meryre und Sisobek', 3), ('pBrooklyn 47.218.135 || Brooklyner Weisheitstext', 3), ('pMoskau 120 || Recto: Die Reise des Wenamun', 8), ('pChassinat III = pLouvre E 25353 || Text 1: Mythologische ErzÃ¤hlung', 5), ('1. pBM EA 10474 || Rto: Die Lehre des Amenemope', 9), ('hintere Innenwand || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)', 3), ('01. sKairo CG 20538 des Sehetep-ib-Re || Die Loyalistische Lehre des Kairsu 1.1-6.10', 7), ('03. pLouvre E 4864 || Recto: Die Loyalistische Lehre des Kairsu 3.6-14.11', 10), ('01. pSallier II = pBM EA 10182 || Kol. 3-11: Die Lehre des Cheti 1.1-30.6', 10), ('hintere Innenwand || Rahmentext', 4), ('oDeM 1632 || Eine Sammlung von Verboten', 4), ('oPetrie 11 = oLondon UC 39614 || Eine Sammlung von Verboten', 4), ('09. pDeM 1 (D) || Recto: Die Lehre des Ani (Version D)', 10), ('linke Innenwand || Rahmentext', 3), ('pRamesseum A = pBerlin P 10499 (Bauer, R) || Recto: Der beredte Bauer (Version R)', 8), (\"KV 9: Grab Ramses' VI., Korridor C, Nische am Ende der linken Wand || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\", 4), (\"KV 17: Grab Sethos' I., vordere rechte Seitenkammer der Sargkammer || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\", 4), ('pMoskau o.Nr. + pMoskau 167 || Mythologische Geschichte', 4), ('oAshmolean Museum 1945.40 aus Deir el Medineh (AOS) || Sinuhe Koch 01-79', 10), ('08. oAshmolean 1938.912 || Die Loyalistische Lehre des Kairsu 5.1-8.1', 3), ('pCarlsberg VI || Die Lehre fÃ¼r Merikare', 8), ('pWestcar = pBerlin P 3033 || Die ErzÃ¤hlungen des pWestcar', 3), ('Stele Louvre C 284 (\"Bentresch-Stele\") || Stele Louvre C 284 (\"Bentresch-Stele\")', 3), ('pLythgoe = pMMA 09.180.535 || Die Geschichte des pLythgoe', 3), ('pLondon UC 32157 = pKahun LV.1 || Verso: Die Geschichte von Hay', 5), ('pPetersburg 1115 || Die Geschichte des SchiffbrÃ¼chigen', 4), ('pBerlin P 3022 und Fragmente pAmherst m-q (B) || Sinuhe Koch 04-81', 9), ('pAmherst 3 + pBerlin P 3024 || Der LebensmÃ¼de', 8), ('pBerlin P 10499 aus Theben-West (R) || Verso: Sinuhe Koch 01-58', 5), ('oKairo CG 25216 aus dem Grab des Sennedjem TT 1 (C) || Recto Sinuhe Koch 01-22', 5), ('15. oBodmer Inv. 12 || Die Lehre des Cheti 16.1-18.2', 5), ('pChassinat II = pLouvre E 25352 || Die Geistergeschichte des pChassinat II', 3), ('12. oKairo CG 25217 || AuÃŸen: Die Lehre des Cheti 13.1-14.1', 5), ('pKahun VI.12 = pUC 32158+32150A+32148B || Recto: Die Geschichte von Horus und Seth', 4), ('pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1)', 12), ('pLondon UC 32107A || Recto: Die Geschichte von/Ã¼ber Nachti', 3), ('pBerlin P 3024 || Die Hirtengeschichte', 4), ('pMoskau o.Nr. (Sporting King) || The Sporting King', 3), ('pBM EA 10475 || Verso: ErzÃ¤hlung von Palast und Lebenshaus', 3), ('pBerlin P 3025 + pAmherst II (Bauer, B2) || Der beredte Bauer (Version B2)', 4), ('pPetersburg 1116 A || Verso: Die Lehre fÃ¼r Merikare', 9), ('pBM EA 10509 (Ptahhotep, Version L2+L2G) || Die Lehre des Ptahhotep (Version L2+L2G)', 9), ('pPrisse = pBN 186-194 (Ptahhotep, Version P) || Die Lehre des Ptahhotep', 13), ('pLeiden I 344 = AMS 27 || Recto: Admonitions = Ipuwer', 3), ('pPetersburg 1116 B || Verso: Die Prophezeiungen des Neferti', 3), ('pSallier II = pBM EA 10182 || Kol. 1-3: Die Lehre des Amenemhet', 4), ('tBM EA 5645 || Die Klagen des Chacheperreseneb', 3), ('pMillingen || Die Lehre des Amenemhet', 4), ('pRamesseum I = pBM EA 10754 || Die Rede des Sasobek', 7), ('pMoskau 4658 || Die Lehre fÃ¼r Merikare', 8), ('pButler = pBM EA 10274 || Verso: Die Rede des VogelfÃ¤ngers Juru', 3), ('pMoskau o.Nr. (Pleasures of Fishing) || The Pleasures of Fishing and Fowling', 4), ('tAshmolean 1964.489 || Der Oxforder Weisheitstext', 3), ('01. oMÃ¼nchen 3400 || Die Lehre des Hordjedef 1.1-3.1', 3), ('06. oGardiner 12 || Die Lehre des Hordjedef 1.4-9.1', 3), ('pBM EA 10371+10435 (Ptahhotep, Version L1) || Die Lehre des Ptahhotep (Version L1)', 7), ('pRamesseum II = pBM EA 10755 || Die Lehre des pRamesseum II', 3), ('tCarnarvon 1 || Verso: Die Lehre des Ptahhotep (Version C)', 3), ('Topf 1: oTurin Suppl. 6619 + oGardiner 306 + oDeM 1251 I+II + oTurin Suppl. 6806 || Chonsemhab und der Geist, Kolumne 2', 4), ('02. oBM EA 41541 || Die Lehre des Amunnacht', 4), ('01. Lederrolle BM EA 10258 || Die Lehre eines Mannes fÃ¼r seinen Sohn 0.1-6.8', 5), ('02. oDeM 1665 + oGardiner 1 || Die Lehre eines Mannes fÃ¼r seinen Sohn 1.2-8.7', 5), ('06. oIFAO OL 416 (vorher: ohne Nummer (18. Dyn.)) || Die Lehre eines Mannes fÃ¼r seinen Sohn 4.5-8.2', 5), ('04. oTurin CGT 57063 || Die Lehre eines Mannes fÃ¼r seinen Sohn 3.4-4.4', 4), ('03. oGardiner 35 || Die Lehre eines Mannes fÃ¼r seinen Sohn 3.4-4.5', 4), ('05. oLouvre 23561 || Die Lehre eines Mannes fÃ¼r seinen Sohn 4.5-6.1', 3), ('02. pRifeh = pLondon UC 32781 || Recto: Die Loyalistische Lehre des Kairsu 1.1-4.8', 3), ('07. oMMA 29.2.25 || Die Lehre eines Mannes fÃ¼r seinen Sohn 5.4-6.4', 3), ('16. oDeM 1013 || Die Lehre des Cheti 23.1-25.3', 4), ('18. oOIC 16999 || Die Lehre eines Mannes fÃ¼r seinen Sohn 15.x+1-17.3', 5), ('6. tTurin CGT 58005 || Die Lehre des Amenemope 24.1-25.9', 4), ('08. oBerlin P 14356 || Die Lehre eines Mannes fÃ¼r seinen Sohn 7.1-9.1', 4), ('14. oIFAO Inv. 2010 || Die Lehre eines Mannes fÃ¼r seinen Sohn 9.1-12.6', 4), ('23. oIFAO Inv. 2536 || Die Lehre eines Mannes fÃ¼r seinen Sohn 19.1-24.10', 5), ('22. oMichaelides 16 = oLos Angeles M.80.203.209 || Die Lehre eines Mannes fÃ¼r seinen Sohn 19.1-21.3', 3), ('25. oMoskau 4478 + oBerlin P 9026 || Verso: Die Lehre eines Mannes fÃ¼r seinen Sohn 24.1-10', 3), ('26. oDeM 1056 + oGardiner 347 || Die Lehre eines Mannes fÃ¼r seinen Sohn 24.1-24.10, Kolophon', 3), ('pPrisse = pBN 183 (Kagemni) || Kol. 1-2: Die Lehre fÃ¼r Kagemni', 3), ('07. oSchweiz Privatsammlung Inv. 086 || Die Loyalistische Lehre des Kairsu 3.1-3.12', 4), ('06. oDeM 1235 || Die Loyalistische Lehre des Kairsu 1.1-4.1', 3), ('04. tCarnarvon 2 || Verso: Die Loyalistische Lehre des Kairsu 8.1-14.12', 3), ('09. oBM EA 5632 || Recto: Die Loyalistische Lehre des Kairsu 8.1-9.1', 3), ('pSallier I = pBM EA 10185 || Recto 1-3: Apophis und Seqenenre', 3), ('pHarris 500 = pBM EA 10060 || Verso 4-8: Das PrinzenmÃ¤rchen', 9), ('pChester Beatty II = pBM EA 10682 || Wahrheit und LÃ¼ge', 11), ('pChester Beatty I || Recto: Der Streit zwischen Horus und Seth', 7), ('Topf 2: oFlorenz 2616 || Chonsemhab und der Geist, Kolumne 3', 4), ('Topf 2: oTurin Cat. 6620+6838 + oDeM 1252 I+II + oTurin Cat. 6851 + oFlorenz 2617 || Chonsemhab und der Geist, Kolumne 3', 3), ('Topf 3: oLouvre N 667 + N 700 || Chonsemhab und der Geist, Kolumne 4', 4), ('pdâ€™Orbiney = pBM EA 10183 || Recto: Das ZweibrÃ¼dermÃ¤rchen', 9), ('pDeM 39 || Herischef und General Meryre', 5), ('tTurin Cat. 6238 = tTurin CGT 58004 || Der Streit zwischen Kopf und Leib', 3), ('pBN 202 + pAmherst 9 || Die GÃ¶tter und das Meer (\"Astartelegende\")', 3), ('pHarris 500 = pBM EA 10060 || Verso 1-3: Die Einnahme von Joppe', 6), ('pTurin CGT 54033 (alias: pTurin CGT 54031) || Verso: ErzÃ¤hlung von einem GÃ¶tterstreit', 4), ('11. pAnastasi VII = pBM EA 10222 || Kol. 1-7: Die Lehre des Cheti 10.1-30.6', 8), ('pBoulaq 13 = pCairo CG 58041 || Eine Kriminalgeschichte (?)', 3), ('oTurin Cat. 9587 = oTurin CGT 57561 || Eine Traumgeschichte', 3), ('pAshmolean 1960.1265a-g || ErzÃ¤hlung vom KÃ¶nig und dem Vorsteher der JÃ¤ger', 3), ('pBerlin P 3020 + pWien 36 || ErzÃ¤hlung von einem KÃ¶nig und einer GÃ¶ttin', 4), (\"pLouvre N 3136 || Kriegsgeschichte Ramses' III.\", 3), ('5. pStockholm MM 18416 || Die Lehre des Amenemope 10.18-14.5', 3), (\"pTurin Cat. 1940+1941+o.Nr. || Syrienfeldzug Thutmosis' III.\", 3), ('pMoskau 127 = pPuschkin I, b, 127 || Recto: Der Moskauer literarische Brief (\"A Tale of Woe\")', 4), ('06. oLacau || Die Lehre des Amunnacht', 3), ('oChicago OIC 12074 + oIFAO Inv. 2188 || Die \"Lehre\" des Menena', 6), ('01. oKV 18/3.614+627 || Die Lehre des Amunnacht', 3), ('02. pBoulaq 4, Rto (B) || Recto: Die Lehre des Ani (Version B)', 9), ('3. oKairo SR 1840 || Die Lehre des Amenemope 3.11-4.10', 3), ('06. pChester Beatty V = pBM EA 10685 || Verso 2.6-11: Die Lehre des Ani (Version L)', 4), ('03. oGrdseloff || Die Lehre des Amunnacht', 3), ('11. pSaqqara EES 75-S 45 = SCA 6319 (S) || Die Lehre des Ani (Version S)', 3), ('12. pGuimet 16959 = pLouvre E 30144 (G) || Die Lehre des Ani (Version G)', 3), ('oGardiner 2 || Die Lehre des Hori', 3), ('pMoskau 4657 (Golenischeff) (G) || Sinuhe Koch 01-37', 3), ('oBM EA 5629 aus Theben-West (L) || Sinuhe Koch 80-81', 3), ('13. tLouvre N 693 || Die Lehre des Cheti 13.4-22.2 und 30.1-6; Nilhymnus 1.1-2.6', 4), ('02. oDeM 1014+1478 || Die Lehre des Cheti 1.1-6.2 und 29.1-30.6', 4), ('03. oGenÃ¨ve 12551 || Die Lehre des Cheti 1.1-3.6', 3), ('09. oPetrie 40 = oLondon UC 39639 || Die Lehre des Cheti 9.1-10.1', 4), ('14. oBM EA 29550 + oDeM 1546 || Die Lehre des Cheti 14.1-21.2', 4), ('10. pAmherst 14 || Die Lehre des Cheti 9.5-13.1', 3), ('17. oTurin CGT 57082 || Die Lehre des Cheti 25.1-29.1', 3), ('03. pChassinat I = pLouvre E 25351 || Die Geschichte von KÃ¶nig Neferkare und General Sasenet', 4)]\nAverage degree: 4.661290322580645\nBetweenness centrality: {'pVandier = pLille 139 || Recto: Meryre und Sisobek': 0.06301342858629165, 'pBrooklyn 47.218.135 || Brooklyner Weisheitstext': 0.017679313998024304, 'pMoskau 120 || Recto: Die Reise des Wenamun': 0.09775998596398326, 'pChassinat III = pLouvre E 25353 || Text 1: Mythologische ErzÃ¤hlung': 0.04837385418125363, '1. pBM EA 10474 || Rto: Die Lehre des Amenemope': 0.049792658540687554, 'hintere Innenwand || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)': 0.035187994884275174, '01. sKairo CG 20538 des Sehetep-ib-Re || Die Loyalistische Lehre des Kairsu 1.1-6.10': 0.03981418344885483, '03. pLouvre E 4864 || Recto: Die Loyalistische Lehre des Kairsu 3.6-14.11': 0.07196074458185332, '01. pSallier II = pBM EA 10182 || Kol. 3-11: Die Lehre des Cheti 1.1-30.6': 0.12424107328626458, 'hintere Innenwand || Rahmentext': 0.012419884601012158, 'oDeM 1632 || Eine Sammlung von Verboten': 0.0015477133230032063, 'oPetrie 11 = oLondon UC 39614 || Eine Sammlung von Verboten': 0.012419884601012158, '09. pDeM 1 (D) || Recto: Die Lehre des Ani (Version D)': 0.14511446245045004, 'linke Innenwand || Rahmentext': 0.01760204915756555, 'pRamesseum A = pBerlin P 10499 (Bauer, R) || Recto: Der beredte Bauer (Version R)': 0.190108817193117, \"KV 9: Grab Ramses' VI., Korridor C, Nische am Ende der linken Wand || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\": 0.001157123638131235, \"KV 17: Grab Sethos' I., vordere rechte Seitenkammer der Sargkammer || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\": 0.19353921890000694, 'pMoskau o.Nr. + pMoskau 167 || Mythologische Geschichte': 0.013154552993617355, 'oAshmolean Museum 1945.40 aus Deir el Medineh (AOS) || Sinuhe Koch 01-79': 0.26608447472603347, '08. oAshmolean 1938.912 || Die Loyalistische Lehre des Kairsu 5.1-8.1': 0.0003087653827357947, 'pCarlsberg VI || Die Lehre fÃ¼r Merikare': 0.02117176907696193, 'pWestcar = pBerlin P 3033 || Die ErzÃ¤hlungen des pWestcar': 0.04545770355138248, 'Stele Louvre C 284 (\"Bentresch-Stele\") || Stele Louvre C 284 (\"Bentresch-Stele\")': 0.01268723770311061, 'pLythgoe = pMMA 09.180.535 || Die Geschichte des pLythgoe': 0.011323378497545987, 'pLondon UC 32157 = pKahun LV.1 || Verso: Die Geschichte von Hay': 0.019380966421009786, 'pPetersburg 1115 || Die Geschichte des SchiffbrÃ¼chigen': 0.006143213710186916, 'pBerlin P 3022 und Fragmente pAmherst m-q (B) || Sinuhe Koch 04-81': 0.025764326095063378, 'pAmherst 3 + pBerlin P 3024 || Der LebensmÃ¼de': 0.09295447518197138, 'pBerlin P 10499 aus Theben-West (R) || Verso: Sinuhe Koch 01-58': 0.003116274037470776, 'oKairo CG 25216 aus dem Grab des Sennedjem TT 1 (C) || Recto Sinuhe Koch 01-22': 0.005849466474781565, '15. oBodmer Inv. 12 || Die Lehre des Cheti 16.1-18.2': 0.014368047385687782, 'pChassinat II = pLouvre E 25352 || Die Geistergeschichte des pChassinat II': 0.01948937403537786, '12. oKairo CG 25217 || AuÃŸen: Die Lehre des Cheti 13.1-14.1': 0.011664396774356356, 'pKahun VI.12 = pUC 32158+32150A+32148B || Recto: Die Geschichte von Horus und Seth': 0.003262656531349055, 'pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1)': 0.10404185769822626, 'pLondon UC 32107A || Recto: Die Geschichte von/Ã¼ber Nachti': 0.0, 'pBerlin P 3024 || Die Hirtengeschichte': 0.015280482323665067, 'pMoskau o.Nr. (Sporting King) || The Sporting King': 0.0009599959919831971, 'pBM EA 10475 || Verso: ErzÃ¤hlung von Palast und Lebenshaus': 0.015420955235029602, 'pBerlin P 3025 + pAmherst II (Bauer, B2) || Der beredte Bauer (Version B2)': 0.0024503644355703532, 'pPetersburg 1116 A || Verso: Die Lehre fÃ¼r Merikare': 0.03284349333909347, 'pBM EA 10509 (Ptahhotep, Version L2+L2G) || Die Lehre des Ptahhotep (Version L2+L2G)': 0.02789554537313574, 'pPrisse = pBN 186-194 (Ptahhotep, Version P) || Die Lehre des Ptahhotep': 0.12622618477830883, 'pLeiden I 344 = AMS 27 || Recto: Admonitions = Ipuwer': 0.0016667815798163644, 'pPetersburg 1116 B || Verso: Die Prophezeiungen des Neferti': 0.037095707392334504, 'pSallier II = pBM EA 10182 || Kol. 1-3: Die Lehre des Amenemhet': 0.010305055466990695, 'tBM EA 5645 || Die Klagen des Chacheperreseneb': 0.0001888133635434715, 'pMillingen || Die Lehre des Amenemhet': 0.0006341907681371895, 'pRamesseum I = pBM EA 10754 || Die Rede des Sasobek': 0.027347665407691443, 'pMoskau 4658 || Die Lehre fÃ¼r Merikare': 0.027437127952917806, 'pButler = pBM EA 10274 || Verso: Die Rede des VogelfÃ¤ngers Juru': 6.66400106624017e-05, 'pMoskau o.Nr. (Pleasures of Fishing) || The Pleasures of Fishing and Fowling': 0.007903404294847715, 'tAshmolean 1964.489 || Der Oxforder Weisheitstext': 0.0002602532058984772, '01. oMÃ¼nchen 3400 || Die Lehre des Hordjedef 1.1-3.1': 0.0, '06. oGardiner 12 || Die Lehre des Hordjedef 1.4-9.1': 0.0, 'pBM EA 10371+10435 (Ptahhotep, Version L1) || Die Lehre des Ptahhotep (Version L1)': 0.012593697787632332, 'pRamesseum II = pBM EA 10755 || Die Lehre des pRamesseum II': 0.0007842365699222956, 'tCarnarvon 1 || Verso: Die Lehre des Ptahhotep (Version C)': 0.10234363887037472, 'Topf 1: oTurin Suppl. 6619 + oGardiner 306 + oDeM 1251 I+II + oTurin Suppl. 6806 || Chonsemhab und der Geist, Kolumne 2': 0.1166391903730762, '02. oBM EA 41541 || Die Lehre des Amunnacht': 0.04183825849875727, '01. Lederrolle BM EA 10258 || Die Lehre eines Mannes fÃ¼r seinen Sohn 0.1-6.8': 0.11509132682006609, '02. oDeM 1665 + oGardiner 1 || Die Lehre eines Mannes fÃ¼r seinen Sohn 1.2-8.7': 0.04868960984991642, '06. oIFAO OL 416 (vorher: ohne Nummer (18. Dyn.)) || Die Lehre eines Mannes fÃ¼r seinen Sohn 4.5-8.2': 0.01553940843905316, '04. oTurin CGT 57063 || Die Lehre eines Mannes fÃ¼r seinen Sohn 3.4-4.4': 0.0019082563053210082, '03. oGardiner 35 || Die Lehre eines Mannes fÃ¼r seinen Sohn 3.4-4.5': 0.0033217697840084185, '05. oLouvre 23561 || Die Lehre eines Mannes fÃ¼r seinen Sohn 4.5-6.1': 0.027185510827682075, '02. pRifeh = pLondon UC 32781 || Recto: Die Loyalistische Lehre des Kairsu 1.1-4.8': 0.004702664846588334, '07. oMMA 29.2.25 || Die Lehre eines Mannes fÃ¼r seinen Sohn 5.4-6.4': 0.03397049281203995, '16. oDeM 1013 || Die Lehre des Cheti 23.1-25.3': 0.06553357771344262, '18. oOIC 16999 || Die Lehre eines Mannes fÃ¼r seinen Sohn 15.x+1-17.3': 0.08865516737289082, '6. tTurin CGT 58005 || Die Lehre des Amenemope 24.1-25.9': 0.03672787375687263, '08. oBerlin P 14356 || Die Lehre eines Mannes fÃ¼r seinen Sohn 7.1-9.1': 0.013568180674932184, '14. oIFAO Inv. 2010 || Die Lehre eines Mannes fÃ¼r seinen Sohn 9.1-12.6': 0.010089369852685989, '23. oIFAO Inv. 2536 || Die Lehre eines Mannes fÃ¼r seinen Sohn 19.1-24.10': 0.00825999264323625, '22. oMichaelides 16 = oLos Angeles M.80.203.209 || Die Lehre eines Mannes fÃ¼r seinen Sohn 19.1-21.3': 0.002669186995888541, '25. oMoskau 4478 + oBerlin P 9026 || Verso: Die Lehre eines Mannes fÃ¼r seinen Sohn 24.1-10': 0.01633213460907339, '26. oDeM 1056 + oGardiner 347 || Die Lehre eines Mannes fÃ¼r seinen Sohn 24.1-24.10, Kolophon': 0.0, 'pPrisse = pBN 183 (Kagemni) || Kol. 1-2: Die Lehre fÃ¼r Kagemni': 0.0, '07. oSchweiz Privatsammlung Inv. 086 || Die Loyalistische Lehre des Kairsu 3.1-3.12': 0.008895461423273826, '06. oDeM 1235 || Die Loyalistische Lehre des Kairsu 1.1-4.1': 0.018464318865265162, '04. tCarnarvon 2 || Verso: Die Loyalistische Lehre des Kairsu 8.1-14.12': 0.0, '09. oBM EA 5632 || Recto: Die Loyalistische Lehre des Kairsu 8.1-9.1': 0.07615070606912594, 'pSallier I = pBM EA 10185 || Recto 1-3: Apophis und Seqenenre': 0.0, 'pHarris 500 = pBM EA 10060 || Verso 4-8: Das PrinzenmÃ¤rchen': 0.027576537023977145, 'pChester Beatty II = pBM EA 10682 || Wahrheit und LÃ¼ge': 0.11954648161625223, 'pChester Beatty I || Recto: Der Streit zwischen Horus und Seth': 0.0035610671556293317, 'Topf 2: oFlorenz 2616 || Chonsemhab und der Geist, Kolumne 3': 0.05341177896959187, 'Topf 2: oTurin Cat. 6620+6838 + oDeM 1252 I+II + oTurin Cat. 6851 + oFlorenz 2617 || Chonsemhab und der Geist, Kolumne 3': 0.09634123314864623, 'Topf 3: oLouvre N 667 + N 700 || Chonsemhab und der Geist, Kolumne 4': 0.0009959051039456076, 'pdâ€™Orbiney = pBM EA 10183 || Recto: Das ZweibrÃ¼dermÃ¤rchen': 0.015042303161897183, 'pDeM 39 || Herischef und General Meryre': 0.003487763143900688, 'tTurin Cat. 6238 = tTurin CGT 58004 || Der Streit zwischen Kopf und Leib': 0.0013675748165575424, 'pBN 202 + pAmherst 9 || Die GÃ¶tter und das Meer (\"Astartelegende\")': 0.0, 'pHarris 500 = pBM EA 10060 || Verso 1-3: Die Einnahme von Joppe': 0.003008407134802616, 'pTurin CGT 54033 (alias: pTurin CGT 54031) || Verso: ErzÃ¤hlung von einem GÃ¶tterstreit': 0.006536904217362336, '11. pAnastasi VII = pBM EA 10222 || Kol. 1-7: Die Lehre des Cheti 10.1-30.6': 0.12301834596217678, 'pBoulaq 13 = pCairo CG 58041 || Eine Kriminalgeschichte (?)': 0.0, 'oTurin Cat. 9587 = oTurin CGT 57561 || Eine Traumgeschichte': 0.0013860859306304319, 'pAshmolean 1960.1265a-g || ErzÃ¤hlung vom KÃ¶nig und dem Vorsteher der JÃ¤ger': 6.66400106624017e-05, 'pBerlin P 3020 + pWien 36 || ErzÃ¤hlung von einem KÃ¶nig und einer GÃ¶ttin': 0.0064251969444636585, \"pLouvre N 3136 || Kriegsgeschichte Ramses' III.\": 0.0, '5. pStockholm MM 18416 || Die Lehre des Amenemope 10.18-14.5': 0.004740079928731209, \"pTurin Cat. 1940+1941+o.Nr. || Syrienfeldzug Thutmosis' III.\": 0.04397795076032134, 'pMoskau 127 = pPuschkin I, b, 127 || Recto: Der Moskauer literarische Brief (\"A Tale of Woe\")': 0.03585118862507907, '06. oLacau || Die Lehre des Amunnacht': 0.03784364882933809, 'oChicago OIC 12074 + oIFAO Inv. 2188 || Die \"Lehre\" des Menena': 0.05551847424314732, '01. oKV 18/3.614+627 || Die Lehre des Amunnacht': 0.0, '02. pBoulaq 4, Rto (B) || Recto: Die Lehre des Ani (Version B)': 0.07415957884311694, '3. oKairo SR 1840 || Die Lehre des Amenemope 3.11-4.10': 0.007608521323035514, '06. pChester Beatty V = pBM EA 10685 || Verso 2.6-11: Die Lehre des Ani (Version L)': 0.006996383745684028, '03. oGrdseloff || Die Lehre des Amunnacht': 0.0, '11. pSaqqara EES 75-S 45 = SCA 6319 (S) || Die Lehre des Ani (Version S)': 0.0, '12. pGuimet 16959 = pLouvre E 30144 (G) || Die Lehre des Ani (Version G)': 0.10609087695287796, 'oGardiner 2 || Die Lehre des Hori': 0.0, 'pMoskau 4657 (Golenischeff) (G) || Sinuhe Koch 01-37': 0.0, 'oBM EA 5629 aus Theben-West (L) || Sinuhe Koch 80-81': 0.0, '13. tLouvre N 693 || Die Lehre des Cheti 13.4-22.2 und 30.1-6; Nilhymnus 1.1-2.6': 0.02836645641088536, '02. oDeM 1014+1478 || Die Lehre des Cheti 1.1-6.2 und 29.1-30.6': 0.029448097058349245, '03. oGenÃ¨ve 12551 || Die Lehre des Cheti 1.1-3.6': 0.0, '09. oPetrie 40 = oLondon UC 39639 || Die Lehre des Cheti 9.1-10.1': 0.0076968928035516145, '14. oBM EA 29550 + oDeM 1546 || Die Lehre des Cheti 14.1-21.2': 0.0076968928035516145, '10. pAmherst 14 || Die Lehre des Cheti 9.5-13.1': 0.01231920378837234, '17. oTurin CGT 57082 || Die Lehre des Cheti 25.1-29.1': 0.013680905704096429, '03. pChassinat I = pLouvre E 25351 || Die Geschichte von KÃ¶nig Neferkare und General Sasenet': 0.007030194376293088}\nDegree centrality: {'pVandier = pLille 139 || Recto: Meryre und Sisobek': 0.024390243902439025, 'pBrooklyn 47.218.135 || Brooklyner Weisheitstext': 0.024390243902439025, 'pMoskau 120 || Recto: Die Reise des Wenamun': 0.06504065040650407, 'pChassinat III = pLouvre E 25353 || Text 1: Mythologische ErzÃ¤hlung': 0.04065040650406505, '1. pBM EA 10474 || Rto: Die Lehre des Amenemope': 0.07317073170731708, 'hintere Innenwand || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)': 0.024390243902439025, '01. sKairo CG 20538 des Sehetep-ib-Re || Die Loyalistische Lehre des Kairsu 1.1-6.10': 0.05691056910569106, '03. pLouvre E 4864 || Recto: Die Loyalistische Lehre des Kairsu 3.6-14.11': 0.0813008130081301, '01. pSallier II = pBM EA 10182 || Kol. 3-11: Die Lehre des Cheti 1.1-30.6': 0.0813008130081301, 'hintere Innenwand || Rahmentext': 0.032520325203252036, 'oDeM 1632 || Eine Sammlung von Verboten': 0.032520325203252036, 'oPetrie 11 = oLondon UC 39614 || Eine Sammlung von Verboten': 0.032520325203252036, '09. pDeM 1 (D) || Recto: Die Lehre des Ani (Version D)': 0.0813008130081301, 'linke Innenwand || Rahmentext': 0.024390243902439025, 'pRamesseum A = pBerlin P 10499 (Bauer, R) || Recto: Der beredte Bauer (Version R)': 0.06504065040650407, \"KV 9: Grab Ramses' VI., Korridor C, Nische am Ende der linken Wand || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\": 0.032520325203252036, \"KV 17: Grab Sethos' I., vordere rechte Seitenkammer der Sargkammer || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\": 0.032520325203252036, 'pMoskau o.Nr. + pMoskau 167 || Mythologische Geschichte': 0.032520325203252036, 'oAshmolean Museum 1945.40 aus Deir el Medineh (AOS) || Sinuhe Koch 01-79': 0.0813008130081301, '08. oAshmolean 1938.912 || Die Loyalistische Lehre des Kairsu 5.1-8.1': 0.024390243902439025, 'pCarlsberg VI || Die Lehre fÃ¼r Merikare': 0.06504065040650407, 'pWestcar = pBerlin P 3033 || Die ErzÃ¤hlungen des pWestcar': 0.024390243902439025, 'Stele Louvre C 284 (\"Bentresch-Stele\") || Stele Louvre C 284 (\"Bentresch-Stele\")': 0.024390243902439025, 'pLythgoe = pMMA 09.180.535 || Die Geschichte des pLythgoe': 0.024390243902439025, 'pLondon UC 32157 = pKahun LV.1 || Verso: Die Geschichte von Hay': 0.04065040650406505, 'pPetersburg 1115 || Die Geschichte des SchiffbrÃ¼chigen': 0.032520325203252036, 'pBerlin P 3022 und Fragmente pAmherst m-q (B) || Sinuhe Koch 04-81': 0.07317073170731708, 'pAmherst 3 + pBerlin P 3024 || Der LebensmÃ¼de': 0.06504065040650407, 'pBerlin P 10499 aus Theben-West (R) || Verso: Sinuhe Koch 01-58': 0.04065040650406505, 'oKairo CG 25216 aus dem Grab des Sennedjem TT 1 (C) || Recto Sinuhe Koch 01-22': 0.04065040650406505, '15. oBodmer Inv. 12 || Die Lehre des Cheti 16.1-18.2': 0.04065040650406505, 'pChassinat II = pLouvre E 25352 || Die Geistergeschichte des pChassinat II': 0.024390243902439025, '12. oKairo CG 25217 || AuÃŸen: Die Lehre des Cheti 13.1-14.1': 0.04065040650406505, 'pKahun VI.12 = pUC 32158+32150A+32148B || Recto: Die Geschichte von Horus und Seth': 0.032520325203252036, 'pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1)': 0.0975609756097561, 'pLondon UC 32107A || Recto: Die Geschichte von/Ã¼ber Nachti': 0.024390243902439025, 'pBerlin P 3024 || Die Hirtengeschichte': 0.032520325203252036, 'pMoskau o.Nr. (Sporting King) || The Sporting King': 0.024390243902439025, 'pBM EA 10475 || Verso: ErzÃ¤hlung von Palast und Lebenshaus': 0.024390243902439025, 'pBerlin P 3025 + pAmherst II (Bauer, B2) || Der beredte Bauer (Version B2)': 0.032520325203252036, 'pPetersburg 1116 A || Verso: Die Lehre fÃ¼r Merikare': 0.07317073170731708, 'pBM EA 10509 (Ptahhotep, Version L2+L2G) || Die Lehre des Ptahhotep (Version L2+L2G)': 0.07317073170731708, 'pPrisse = pBN 186-194 (Ptahhotep, Version P) || Die Lehre des Ptahhotep': 0.10569105691056911, 'pLeiden I 344 = AMS 27 || Recto: Admonitions = Ipuwer': 0.024390243902439025, 'pPetersburg 1116 B || Verso: Die Prophezeiungen des Neferti': 0.024390243902439025, 'pSallier II = pBM EA 10182 || Kol. 1-3: Die Lehre des Amenemhet': 0.032520325203252036, 'tBM EA 5645 || Die Klagen des Chacheperreseneb': 0.024390243902439025, 'pMillingen || Die Lehre des Amenemhet': 0.032520325203252036, 'pRamesseum I = pBM EA 10754 || Die Rede des Sasobek': 0.05691056910569106, 'pMoskau 4658 || Die Lehre fÃ¼r Merikare': 0.06504065040650407, 'pButler = pBM EA 10274 || Verso: Die Rede des VogelfÃ¤ngers Juru': 0.024390243902439025, 'pMoskau o.Nr. (Pleasures of Fishing) || The Pleasures of Fishing and Fowling': 0.032520325203252036, 'tAshmolean 1964.489 || Der Oxforder Weisheitstext': 0.024390243902439025, '01. oMÃ¼nchen 3400 || Die Lehre des Hordjedef 1.1-3.1': 0.024390243902439025, '06. oGardiner 12 || Die Lehre des Hordjedef 1.4-9.1': 0.024390243902439025, 'pBM EA 10371+10435 (Ptahhotep, Version L1) || Die Lehre des Ptahhotep (Version L1)': 0.05691056910569106, 'pRamesseum II = pBM EA 10755 || Die Lehre des pRamesseum II': 0.024390243902439025, 'tCarnarvon 1 || Verso: Die Lehre des Ptahhotep (Version C)': 0.024390243902439025, 'Topf 1: oTurin Suppl. 6619 + oGardiner 306 + oDeM 1251 I+II + oTurin Suppl. 6806 || Chonsemhab und der Geist, Kolumne 2': 0.032520325203252036, '02. oBM EA 41541 || Die Lehre des Amunnacht': 0.032520325203252036, '01. Lederrolle BM EA 10258 || Die Lehre eines Mannes fÃ¼r seinen Sohn 0.1-6.8': 0.04065040650406505, '02. oDeM 1665 + oGardiner 1 || Die Lehre eines Mannes fÃ¼r seinen Sohn 1.2-8.7': 0.04065040650406505, '06. oIFAO OL 416 (vorher: ohne Nummer (18. Dyn.)) || Die Lehre eines Mannes fÃ¼r seinen Sohn 4.5-8.2': 0.04065040650406505, '04. oTurin CGT 57063 || Die Lehre eines Mannes fÃ¼r seinen Sohn 3.4-4.4': 0.032520325203252036, '03. oGardiner 35 || Die Lehre eines Mannes fÃ¼r seinen Sohn 3.4-4.5': 0.032520325203252036, '05. oLouvre 23561 || Die Lehre eines Mannes fÃ¼r seinen Sohn 4.5-6.1': 0.024390243902439025, '02. pRifeh = pLondon UC 32781 || Recto: Die Loyalistische Lehre des Kairsu 1.1-4.8': 0.024390243902439025, '07. oMMA 29.2.25 || Die Lehre eines Mannes fÃ¼r seinen Sohn 5.4-6.4': 0.024390243902439025, '16. oDeM 1013 || Die Lehre des Cheti 23.1-25.3': 0.032520325203252036, '18. oOIC 16999 || Die Lehre eines Mannes fÃ¼r seinen Sohn 15.x+1-17.3': 0.04065040650406505, '6. tTurin CGT 58005 || Die Lehre des Amenemope 24.1-25.9': 0.032520325203252036, '08. oBerlin P 14356 || Die Lehre eines Mannes fÃ¼r seinen Sohn 7.1-9.1': 0.032520325203252036, '14. oIFAO Inv. 2010 || Die Lehre eines Mannes fÃ¼r seinen Sohn 9.1-12.6': 0.032520325203252036, '23. oIFAO Inv. 2536 || Die Lehre eines Mannes fÃ¼r seinen Sohn 19.1-24.10': 0.04065040650406505, '22. oMichaelides 16 = oLos Angeles M.80.203.209 || Die Lehre eines Mannes fÃ¼r seinen Sohn 19.1-21.3': 0.024390243902439025, '25. oMoskau 4478 + oBerlin P 9026 || Verso: Die Lehre eines Mannes fÃ¼r seinen Sohn 24.1-10': 0.024390243902439025, '26. oDeM 1056 + oGardiner 347 || Die Lehre eines Mannes fÃ¼r seinen Sohn 24.1-24.10, Kolophon': 0.024390243902439025, 'pPrisse = pBN 183 (Kagemni) || Kol. 1-2: Die Lehre fÃ¼r Kagemni': 0.024390243902439025, '07. oSchweiz Privatsammlung Inv. 086 || Die Loyalistische Lehre des Kairsu 3.1-3.12': 0.032520325203252036, '06. oDeM 1235 || Die Loyalistische Lehre des Kairsu 1.1-4.1': 0.024390243902439025, '04. tCarnarvon 2 || Verso: Die Loyalistische Lehre des Kairsu 8.1-14.12': 0.024390243902439025, '09. oBM EA 5632 || Recto: Die Loyalistische Lehre des Kairsu 8.1-9.1': 0.024390243902439025, 'pSallier I = pBM EA 10185 || Recto 1-3: Apophis und Seqenenre': 0.024390243902439025, 'pHarris 500 = pBM EA 10060 || Verso 4-8: Das PrinzenmÃ¤rchen': 0.07317073170731708, 'pChester Beatty II = pBM EA 10682 || Wahrheit und LÃ¼ge': 0.0894308943089431, 'pChester Beatty I || Recto: Der Streit zwischen Horus und Seth': 0.05691056910569106, 'Topf 2: oFlorenz 2616 || Chonsemhab und der Geist, Kolumne 3': 0.032520325203252036, 'Topf 2: oTurin Cat. 6620+6838 + oDeM 1252 I+II + oTurin Cat. 6851 + oFlorenz 2617 || Chonsemhab und der Geist, Kolumne 3': 0.024390243902439025, 'Topf 3: oLouvre N 667 + N 700 || Chonsemhab und der Geist, Kolumne 4': 0.032520325203252036, 'pdâ€™Orbiney = pBM EA 10183 || Recto: Das ZweibrÃ¼dermÃ¤rchen': 0.07317073170731708, 'pDeM 39 || Herischef und General Meryre': 0.04065040650406505, 'tTurin Cat. 6238 = tTurin CGT 58004 || Der Streit zwischen Kopf und Leib': 0.024390243902439025, 'pBN 202 + pAmherst 9 || Die GÃ¶tter und das Meer (\"Astartelegende\")': 0.024390243902439025, 'pHarris 500 = pBM EA 10060 || Verso 1-3: Die Einnahme von Joppe': 0.04878048780487805, 'pTurin CGT 54033 (alias: pTurin CGT 54031) || Verso: ErzÃ¤hlung von einem GÃ¶tterstreit': 0.032520325203252036, '11. pAnastasi VII = pBM EA 10222 || Kol. 1-7: Die Lehre des Cheti 10.1-30.6': 0.06504065040650407, 'pBoulaq 13 = pCairo CG 58041 || Eine Kriminalgeschichte (?)': 0.024390243902439025, 'oTurin Cat. 9587 = oTurin CGT 57561 || Eine Traumgeschichte': 0.024390243902439025, 'pAshmolean 1960.1265a-g || ErzÃ¤hlung vom KÃ¶nig und dem Vorsteher der JÃ¤ger': 0.024390243902439025, 'pBerlin P 3020 + pWien 36 || ErzÃ¤hlung von einem KÃ¶nig und einer GÃ¶ttin': 0.032520325203252036, \"pLouvre N 3136 || Kriegsgeschichte Ramses' III.\": 0.024390243902439025, '5. pStockholm MM 18416 || Die Lehre des Amenemope 10.18-14.5': 0.024390243902439025, \"pTurin Cat. 1940+1941+o.Nr. || Syrienfeldzug Thutmosis' III.\": 0.024390243902439025, 'pMoskau 127 = pPuschkin I, b, 127 || Recto: Der Moskauer literarische Brief (\"A Tale of Woe\")': 0.032520325203252036, '06. oLacau || Die Lehre des Amunnacht': 0.024390243902439025, 'oChicago OIC 12074 + oIFAO Inv. 2188 || Die \"Lehre\" des Menena': 0.04878048780487805, '01. oKV 18/3.614+627 || Die Lehre des Amunnacht': 0.024390243902439025, '02. pBoulaq 4, Rto (B) || Recto: Die Lehre des Ani (Version B)': 0.07317073170731708, '3. oKairo SR 1840 || Die Lehre des Amenemope 3.11-4.10': 0.024390243902439025, '06. pChester Beatty V = pBM EA 10685 || Verso 2.6-11: Die Lehre des Ani (Version L)': 0.032520325203252036, '03. oGrdseloff || Die Lehre des Amunnacht': 0.024390243902439025, '11. pSaqqara EES 75-S 45 = SCA 6319 (S) || Die Lehre des Ani (Version S)': 0.024390243902439025, '12. pGuimet 16959 = pLouvre E 30144 (G) || Die Lehre des Ani (Version G)': 0.024390243902439025, 'oGardiner 2 || Die Lehre des Hori': 0.024390243902439025, 'pMoskau 4657 (Golenischeff) (G) || Sinuhe Koch 01-37': 0.024390243902439025, 'oBM EA 5629 aus Theben-West (L) || Sinuhe Koch 80-81': 0.024390243902439025, '13. tLouvre N 693 || Die Lehre des Cheti 13.4-22.2 und 30.1-6; Nilhymnus 1.1-2.6': 0.032520325203252036, '02. oDeM 1014+1478 || Die Lehre des Cheti 1.1-6.2 und 29.1-30.6': 0.032520325203252036, '03. oGenÃ¨ve 12551 || Die Lehre des Cheti 1.1-3.6': 0.024390243902439025, '09. oPetrie 40 = oLondon UC 39639 || Die Lehre des Cheti 9.1-10.1': 0.032520325203252036, '14. oBM EA 29550 + oDeM 1546 || Die Lehre des Cheti 14.1-21.2': 0.032520325203252036, '10. pAmherst 14 || Die Lehre des Cheti 9.5-13.1': 0.024390243902439025, '17. oTurin CGT 57082 || Die Lehre des Cheti 25.1-29.1': 0.024390243902439025, '03. pChassinat I = pLouvre E 25351 || Die Geschichte von KÃ¶nig Neferkare und General Sasenet': 0.032520325203252036}\nEigenvector centrality: {'pVandier = pLille 139 || Recto: Meryre und Sisobek': 0.0014362160009398113, 'pBrooklyn 47.218.135 || Brooklyner Weisheitstext': 0.002454283705435061, 'pMoskau 120 || Recto: Die Reise des Wenamun': 0.002496036488160371, 'pChassinat III = pLouvre E 25353 || Text 1: Mythologische ErzÃ¤hlung': 0.004149139694894572, '1. pBM EA 10474 || Rto: Die Lehre des Amenemope': 0.009979466174146304, 'hintere Innenwand || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)': 0.05066967009277731, '01. sKairo CG 20538 des Sehetep-ib-Re || Die Loyalistische Lehre des Kairsu 1.1-6.10': 0.09607428959331978, '03. pLouvre E 4864 || Recto: Die Loyalistische Lehre des Kairsu 3.6-14.11': 0.15926387236158754, '01. pSallier II = pBM EA 10182 || Kol. 3-11: Die Lehre des Cheti 1.1-30.6': 0.06605282676784865, 'hintere Innenwand || Rahmentext': 0.0035240719480350113, 'oDeM 1632 || Eine Sammlung von Verboten': 0.0016085733642569196, 'oPetrie 11 = oLondon UC 39614 || Eine Sammlung von Verboten': 0.0035240719480350113, '09. pDeM 1 (D) || Recto: Die Lehre des Ani (Version D)': 0.015855004770683828, 'linke Innenwand || Rahmentext': 0.05912474720816896, 'pRamesseum A = pBerlin P 10499 (Bauer, R) || Recto: Der beredte Bauer (Version R)': 0.2137583318625122, \"KV 9: Grab Ramses' VI., Korridor C, Nische am Ende der linken Wand || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\": 0.065187795745684, \"KV 17: Grab Sethos' I., vordere rechte Seitenkammer der Sargkammer || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\": 0.044624293577813626, 'pMoskau o.Nr. + pMoskau 167 || Mythologische Geschichte': 0.010830099854720634, 'oAshmolean Museum 1945.40 aus Deir el Medineh (AOS) || Sinuhe Koch 01-79': 0.016138291800429017, '08. oAshmolean 1938.912 || Die Loyalistische Lehre des Kairsu 5.1-8.1': 0.05053329035408668, 'pCarlsberg VI || Die Lehre fÃ¼r Merikare': 0.20774521694932674, 'pWestcar = pBerlin P 3033 || Die ErzÃ¤hlungen des pWestcar': 0.03607166340268448, 'Stele Louvre C 284 (\"Bentresch-Stele\") || Stele Louvre C 284 (\"Bentresch-Stele\")': 0.007381408977588103, 'pLythgoe = pMMA 09.180.535 || Die Geschichte des pLythgoe': 0.00765802132601639, 'pLondon UC 32157 = pKahun LV.1 || Verso: Die Geschichte von Hay': 0.005120738196152394, 'pPetersburg 1115 || Die Geschichte des SchiffbrÃ¼chigen': 0.007595345847986915, 'pBerlin P 3022 und Fragmente pAmherst m-q (B) || Sinuhe Koch 04-81': 0.010262174059881503, 'pAmherst 3 + pBerlin P 3024 || Der LebensmÃ¼de': 0.010944676122086723, 'pBerlin P 10499 aus Theben-West (R) || Verso: Sinuhe Koch 01-58': 0.006540928438401458, 'oKairo CG 25216 aus dem Grab des Sennedjem TT 1 (C) || Recto Sinuhe Koch 01-22': 0.0062572035243545175, '15. oBodmer Inv. 12 || Die Lehre des Cheti 16.1-18.2': 0.0035263009224859912, 'pChassinat II = pLouvre E 25352 || Die Geistergeschichte des pChassinat II': 0.014637686489244912, '12. oKairo CG 25217 || AuÃŸen: Die Lehre des Cheti 13.1-14.1': 0.003568540600168596, 'pKahun VI.12 = pUC 32158+32150A+32148B || Recto: Die Geschichte von Horus und Seth': 0.10814985558690648, 'pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1)': 0.2866538477616272, 'pLondon UC 32107A || Recto: Die Geschichte von/Ã¼ber Nachti': 0.09594431278161311, 'pBerlin P 3024 || Die Hirtengeschichte': 0.008982235142307806, 'pMoskau o.Nr. (Sporting King) || The Sporting King': 0.0074916933527333475, 'pBM EA 10475 || Verso: ErzÃ¤hlung von Palast und Lebenshaus': 0.027706375519211124, 'pBerlin P 3025 + pAmherst II (Bauer, B2) || Der beredte Bauer (Version B2)': 0.1736674275319884, 'pPetersburg 1116 A || Verso: Die Lehre fÃ¼r Merikare': 0.3298554890276473, 'pBM EA 10509 (Ptahhotep, Version L2+L2G) || Die Lehre des Ptahhotep (Version L2+L2G)': 0.26244064997918387, 'pPrisse = pBN 186-194 (Ptahhotep, Version P) || Die Lehre des Ptahhotep': 0.3954164446639575, 'pLeiden I 344 = AMS 27 || Recto: Admonitions = Ipuwer': 0.06453355659970827, 'pPetersburg 1116 B || Verso: Die Prophezeiungen des Neferti': 0.042318686725597174, 'pSallier II = pBM EA 10182 || Kol. 1-3: Die Lehre des Amenemhet': 0.004982077468278339, 'tBM EA 5645 || Die Klagen des Chacheperreseneb': 0.003913985053756533, 'pMillingen || Die Lehre des Amenemhet': 0.003618473561661634, 'pRamesseum I = pBM EA 10754 || Die Rede des Sasobek': 0.23657676279032924, 'pMoskau 4658 || Die Lehre fÃ¼r Merikare': 0.2663541895456376, 'pButler = pBM EA 10274 || Verso: Die Rede des VogelfÃ¤ngers Juru': 0.08962368243492476, 'pMoskau o.Nr. (Pleasures of Fishing) || The Pleasures of Fishing and Fowling': 0.003110315382799575, 'tAshmolean 1964.489 || Der Oxforder Weisheitstext': 0.15572020058561267, '01. oMÃ¼nchen 3400 || Die Lehre des Hordjedef 1.1-3.1': 0.0013648121582704427, '06. oGardiner 12 || Die Lehre des Hordjedef 1.4-9.1': 0.12171011429979439, 'pBM EA 10371+10435 (Ptahhotep, Version L1) || Die Lehre des Ptahhotep (Version L1)': 0.222896616393292, 'pRamesseum II = pBM EA 10755 || Die Lehre des pRamesseum II': 0.10439991600910116, 'tCarnarvon 1 || Verso: Die Lehre des Ptahhotep (Version C)': 0.0030349390707466346, 'Topf 1: oTurin Suppl. 6619 + oGardiner 306 + oDeM 1251 I+II + oTurin Suppl. 6806 || Chonsemhab und der Geist, Kolumne 2': 0.0017504607920871824, '02. oBM EA 41541 || Die Lehre des Amunnacht': 0.0013578147758737942, '01. Lederrolle BM EA 10258 || Die Lehre eines Mannes fÃ¼r seinen Sohn 0.1-6.8': 0.07240873945213622, '02. oDeM 1665 + oGardiner 1 || Die Lehre eines Mannes fÃ¼r seinen Sohn 1.2-8.7': 0.01832756562474197, '06. oIFAO OL 416 (vorher: ohne Nummer (18. Dyn.)) || Die Lehre eines Mannes fÃ¼r seinen Sohn 4.5-8.2': 0.015300606252558852, '04. oTurin CGT 57063 || Die Lehre eines Mannes fÃ¼r seinen Sohn 3.4-4.4': 0.008466466090594892, '03. oGardiner 35 || Die Lehre eines Mannes fÃ¼r seinen Sohn 3.4-4.5': 0.014502336201224202, '05. oLouvre 23561 || Die Lehre eines Mannes fÃ¼r seinen Sohn 4.5-6.1': 0.049891755903586875, '02. pRifeh = pLondon UC 32781 || Recto: Die Loyalistische Lehre des Kairsu 1.1-4.8': 0.005861591156569924, '07. oMMA 29.2.25 || Die Lehre eines Mannes fÃ¼r seinen Sohn 5.4-6.4': 0.029098970951561158, '16. oDeM 1013 || Die Lehre des Cheti 23.1-25.3': 0.04140806972902685, '18. oOIC 16999 || Die Lehre eines Mannes fÃ¼r seinen Sohn 15.x+1-17.3': 0.13464638612894242, '6. tTurin CGT 58005 || Die Lehre des Amenemope 24.1-25.9': 0.00851625708934243, '08. oBerlin P 14356 || Die Lehre eines Mannes fÃ¼r seinen Sohn 7.1-9.1': 0.005571161824231068, '14. oIFAO Inv. 2010 || Die Lehre eines Mannes fÃ¼r seinen Sohn 9.1-12.6': 0.12439503798189537, '23. oIFAO Inv. 2536 || Die Lehre eines Mannes fÃ¼r seinen Sohn 19.1-24.10': 0.12567996721499095, '22. oMichaelides 16 = oLos Angeles M.80.203.209 || Die Lehre eines Mannes fÃ¼r seinen Sohn 19.1-21.3': 0.0846192773614272, '25. oMoskau 4478 + oBerlin P 9026 || Verso: Die Lehre eines Mannes fÃ¼r seinen Sohn 24.1-10': 0.09256842530687082, '26. oDeM 1056 + oGardiner 347 || Die Lehre eines Mannes fÃ¼r seinen Sohn 24.1-24.10, Kolophon': 0.15633719855509157, 'pPrisse = pBN 183 (Kagemni) || Kol. 1-2: Die Lehre fÃ¼r Kagemni': 0.13885734264066985, '07. oSchweiz Privatsammlung Inv. 086 || Die Loyalistische Lehre des Kairsu 3.1-3.12': 0.005016806294131473, '06. oDeM 1235 || Die Loyalistische Lehre des Kairsu 1.1-4.1': 0.016861882848088598, '04. tCarnarvon 2 || Verso: Die Loyalistische Lehre des Kairsu 8.1-14.12': 0.09985440931582663, '09. oBM EA 5632 || Recto: Die Loyalistische Lehre des Kairsu 8.1-9.1': 0.013926787855028773, 'pSallier I = pBM EA 10185 || Recto 1-3: Apophis und Seqenenre': 0.001477215692710939, 'pHarris 500 = pBM EA 10060 || Verso 4-8: Das PrinzenmÃ¤rchen': 0.0032032380130898406, 'pChester Beatty II = pBM EA 10682 || Wahrheit und LÃ¼ge': 0.0036445987984184613, 'pChester Beatty I || Recto: Der Streit zwischen Horus und Seth': 0.0024819869658102337, 'Topf 2: oFlorenz 2616 || Chonsemhab und der Geist, Kolumne 3': 0.0013676927992423674, 'Topf 2: oTurin Cat. 6620+6838 + oDeM 1252 I+II + oTurin Cat. 6851 + oFlorenz 2617 || Chonsemhab und der Geist, Kolumne 3': 0.0030368288992633336, 'Topf 3: oLouvre N 667 + N 700 || Chonsemhab und der Geist, Kolumne 4': 0.0014460428557432276, 'pdâ€™Orbiney = pBM EA 10183 || Recto: Das ZweibrÃ¼dermÃ¤rchen': 0.0030098565767864805, 'pDeM 39 || Herischef und General Meryre': 0.001960110711103634, 'tTurin Cat. 6238 = tTurin CGT 58004 || Der Streit zwischen Kopf und Leib': 0.0013650170621396789, 'pBN 202 + pAmherst 9 || Die GÃ¶tter und das Meer (\"Astartelegende\")': 0.001376852679696999, 'pHarris 500 = pBM EA 10060 || Verso 1-3: Die Einnahme von Joppe': 0.002338618919587044, 'pTurin CGT 54033 (alias: pTurin CGT 54031) || Verso: ErzÃ¤hlung von einem GÃ¶tterstreit': 0.021671327337825664, '11. pAnastasi VII = pBM EA 10222 || Kol. 1-7: Die Lehre des Cheti 10.1-30.6': 0.03708994344106931, 'pBoulaq 13 = pCairo CG 58041 || Eine Kriminalgeschichte (?)': 0.01967787065340595, 'oTurin Cat. 9587 = oTurin CGT 57561 || Eine Traumgeschichte': 0.0010756633332520257, 'pAshmolean 1960.1265a-g || ErzÃ¤hlung vom KÃ¶nig und dem Vorsteher der JÃ¤ger': 0.0009587373377519512, 'pBerlin P 3020 + pWien 36 || ErzÃ¤hlung von einem KÃ¶nig und einer GÃ¶ttin': 0.0015984605090569373, \"pLouvre N 3136 || Kriegsgeschichte Ramses' III.\": 0.0013422670656170084, '5. pStockholm MM 18416 || Die Lehre des Amenemope 10.18-14.5': 0.0039285838255290325, \"pTurin Cat. 1940+1941+o.Nr. || Syrienfeldzug Thutmosis' III.\": 0.0013779269693086598, 'pMoskau 127 = pPuschkin I, b, 127 || Recto: Der Moskauer literarische Brief (\"A Tale of Woe\")': 0.004870033970568234, '06. oLacau || Die Lehre des Amunnacht': 0.0024997330562686643, 'oChicago OIC 12074 + oIFAO Inv. 2188 || Die \"Lehre\" des Menena': 0.003687892235105851, '01. oKV 18/3.614+627 || Die Lehre des Amunnacht': 0.0009444278517487296, '02. pBoulaq 4, Rto (B) || Recto: Die Lehre des Ani (Version B)': 0.010788952694799549, '3. oKairo SR 1840 || Die Lehre des Amenemope 3.11-4.10': 0.0017898753553980953, '06. pChester Beatty V = pBM EA 10685 || Verso 2.6-11: Die Lehre des Ani (Version L)': 0.0060562540055318055, '03. oGrdseloff || Die Lehre des Amunnacht': 0.0009444278517487296, '11. pSaqqara EES 75-S 45 = SCA 6319 (S) || Die Lehre des Ani (Version S)': 0.0057740612573407455, '12. pGuimet 16959 = pLouvre E 30144 (G) || Die Lehre des Ani (Version G)': 0.01004818453383421, 'oGardiner 2 || Die Lehre des Hori': 0.0057740612573407455, 'pMoskau 4657 (Golenischeff) (G) || Sinuhe Koch 01-37': 0.003635734600679985, 'oBM EA 5629 aus Theben-West (L) || Sinuhe Koch 80-81': 0.0051936081782683755, '13. tLouvre N 693 || Die Lehre des Cheti 13.4-22.2 und 30.1-6; Nilhymnus 1.1-2.6': 0.01797939550066261, '02. oDeM 1014+1478 || Die Lehre des Cheti 1.1-6.2 und 29.1-30.6': 0.06564981163773322, '03. oGenÃ¨ve 12551 || Die Lehre des Cheti 1.1-3.6': 0.026611404205160255, '09. oPetrie 40 = oLondon UC 39639 || Die Lehre des Cheti 9.1-10.1': 0.005449063180375267, '14. oBM EA 29550 + oDeM 1546 || Die Lehre des Cheti 14.1-21.2': 0.005449063180375267, '10. pAmherst 14 || Die Lehre des Cheti 9.5-13.1': 0.0075656772548528, '17. oTurin CGT 57082 || Die Lehre des Cheti 25.1-29.1': 0.08304530737879667, '03. pChassinat I = pLouvre E 25351 || Die Geschichte von KÃ¶nig Neferkare und General Sasenet': 0.003089610618037005}\nNone\n\n\nA simple network measure is a nodeâ€™s â€˜degreeâ€™, which is the total number of connections (edges) it has with other nodes. For a variety of data sets and social scientific problems, the distribution of degree values across all nodes in a network can be particularly revealing.\nDegree distribution &gt; Defined as the probability distribution of all degrees over the whole network. The measure is commonly used to compare the structure of networks since the publication of the â€œscale-freeâ€ network structure. In scale-free networks, the degree distribution follows a power-law.\n\nP. 20 in Collar, A., Coward, F., Brughmans, T. et al.Â Networks in Archaeology: Phenomena, Abstraction, Representation. J Archaeol Method Theory 22, 1â€“32 (2015). https://doi.org/10.1007/s10816-014-9235-6\n\n\n# Calculate the degree for each node\ndegree_dict = dict(G.degree())\ndegree_series = pd.Series(degree_dict, name='degree')\n\n# Create a DataFrame with the degree values\ndf_degree = pd.DataFrame(degree_series)\n\n# Create a histogram using Plotly Express\nfig = px.histogram(df_degree, x='degree', nbins=15,\n                   title='Histogram of Degree Distribution in the Egyptian Texts Network',\n                   labels={'degree': 'Degree'})\n\n# Show the plot\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\nimport heapq\n\n# Calculate the degree for each node\ndegree_dict = dict(G.degree())\n\n# Find the node with the highest degree\n# max_degree_node = max(degree_dict, key=degree_dict.get)\n# max_degree_value = degree_dict[max_degree_node]\n\n# Find the top n nodes with the highest degrees\ntop_3_nodes = heapq.nlargest(3, degree_dict.items(), key=lambda x: x[1])\n\n# Print the results\nfor node, degree in top_3_nodes:\n    print(f\"Node {node} has a degree of {degree}.\")\n\nNode pPrisse = pBN 186-194 (Ptahhotep, Version P) || Die Lehre des Ptahhotep has a degree of 13.\nNode pBerlin P 3023 + pAmherst I (Bauer, B1) || Der beredte Bauer (Version B1) has a degree of 12.\nNode pChester Beatty II = pBM EA 10682 || Wahrheit und LÃ¼ge has a degree of 11.\n\n\n\nDegree centrality\n\nDefined as the centrality of a node based on the number of edges incident to this node. According to the degree centrality measure, a node is important or prominent if it has edges to a high number of other nodes.\n\nP. 20 in Collar, A., Coward, F., Brughmans, T. et al.Â Networks in Archaeology: Phenomena, Abstraction, Representation. J Archaeol Method Theory 22, 1â€“32 (2015). https://doi.org/10.1007/s10816-014-9235-6\n\n# Calculate the degree centrality for each node\ndegree_centrality_dict = nx.degree_centrality(G)\ndegree_centrality_series = pd.Series(degree_centrality_dict, name='degree_centrality')\n\n# Create a DataFrame with the degree centrality values\ndf_degree_centrality = pd.DataFrame(degree_centrality_series)\n\n# Create a histogram using Plotly Express\nfig = px.histogram(df_degree_centrality, x='degree_centrality', nbins=20,\n                   title='Histogram of Degree Centrality',\n                   labels={'degree_centrality': 'Degree Centrality'})\n\nfig.update_layout(\n    yaxis_title=\"Number of Texts\")  # Setze den Titel der Y-Achse\n\n# Show the plot\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\nIdentifying Hubs and Bridges: Node degree vs Betweenness centrality\nBetweenness centrality indicates mathematically how central a node is. For every pair of nodes in the graph, there is a shortest path to get from one to the other. Their path can cross other nodes along the way. The more shortest paths that pass through a node, the higher its betweennes centrality score. Thus, these nodes function as bridges in the network.\n\n# Ploting relationship between degree and betweenness centrality\n\n# Calculate the degree for each node\ndegree_dict = dict(G.degree())\ndegree_series = pd.Series(degree_dict, name='degree')\n\n# Calculate the betweenness centrality for each node\nbetweenness_dict = nx.betweenness_centrality(G)\nbetweenness_series = pd.Series(betweenness_dict, name='betweenness')\n\n# Combine the degree and betweenness centrality into a DataFrame\ndf = pd.DataFrame({'degree': degree_series, 'betweenness': betweenness_series})\ndf.index.name = 'text_name' # Set the name of the index\ndf = df.merge(metadata[['text_name', 'corpus_manual']], on='text_name', how='left')\n\n# Calculate the average betweenness centrality for each degree\naverage_betweenness = df.groupby('degree')['betweenness'].mean().reset_index()\n\n# Create a scatter plot using Plotly Express\nfig = px.scatter(df.reset_index(), x='degree', y='betweenness', title='Degree vs Betweenness Centrality',\n                 color = 'corpus_manual',\n                 hover_data=['text_name', 'degree'],\n                 labels={'degree': 'Degree', 'betweenness': 'Betweenness Centrality'})\n\n# Add the average curve using Plotly Graph Objects\nfig.add_trace(go.Scatter(x=average_betweenness['degree'], y=average_betweenness['betweenness'],\n                         mode='lines', name='Average Betweenness', line=dict(color='red')))\n\n# Show the plot\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\nThe majority of texts in the network have both a low degree score â€“ meaning, they have a low number of edges connecting them to other places â€“ and also a low betweenness centrality score, meaning they do not function as bridges, connecting between different places. There are only a few placenames that have significantly higher degree, functioning as hubs.\n\n\nSocializing with the important nodes: Eigenvector centrality\nEigenvector centrality serves the purpose to identify influential nodes in a network based on the premise that connections to high-scoring nodes (= high eigenvector centrality) contribute more to a nodeâ€™s score.\nVery questionable from a moral point of view but well illustrating the concept is this example that ChatGPT-4 produced for us:\n\nThink of the eigenvector as a magical list that scores each person at the conference on their influence. This list isnâ€™t just made up by counting how many hands each person shakes (which would be a simpler measure = degree). Instead, itâ€™s like a loop where: * Each person gets points not only for every handshake but gains extra points if the people they shook hands with are themselves high-scorers (= high value in the eigenvector) on this magical list. * This creates a chain reaction. Being connected to influential people boosts your score, and your score helps boost the scores of those youâ€™re connected to, in a cycle. * Other real world examples: * Social Media: On platforms like Twitter or Facebook, some people are considered more influential not just because they have a lot of followers, but because many of their followers are themselves influential. * Academic citations: In academia, a researcherâ€™s importance is often measured not only by how many papers they have published but also by how often their papers are cited by other highly regarded scientists.\n\nâ€œA node with a high eigenvector centrality is a node that is connected to other nodes with a high eigenvector centrality.â€ (Collar, Anna et al.Â 2015, Networks in Archaeology: Phenomena, Abstraction, Representation, J Archaeol Method Theory 22, 1â€“32, DOI 10.1007/s10816-014-9235-6, p.21)\n\n# Calculate eigenvector centrality for each node\neigenvector_centrality_dict = nx.eigenvector_centrality(G, max_iter=500)\neigenvector_centrality_series = pd.Series(eigenvector_centrality_dict, name='eigenvector_centrality')\n\n# Create a DataFrame with the eigenvector centrality values\ndf_eigenvector_centrality = pd.DataFrame(eigenvector_centrality_series)\n# Set the name of the index\ndf_eigenvector_centrality.index.name = 'text_name'\n# Add genre info ('corpus_manual') as a column to the df\ndf_eigenvector_centrality = df_eigenvector_centrality.merge(metadata[['text_name', 'corpus_manual']], on='text_name', how='left')\n\n# Display the DataFrame\ndf_eigenvector_centrality.sort_values(by='eigenvector_centrality', ascending=False)\n\n\n    \n\n\n\n\n\n\ntext_name\neigenvector_centrality\ncorpus_manual\n\n\n\n\n42\npPrisse = pBN 186-194 (Ptahhotep, Version P) |...\n0.395416\nLehren\n\n\n40\npPetersburg 1116 A || Verso: Die Lehre fÃ¼r Mer...\n0.329855\nLehren\n\n\n34\npBerlin P 3023 + pAmherst I (Bauer, B1) || Der...\n0.286654\nReden\n\n\n49\npMoskau 4658 || Die Lehre fÃ¼r Merikare\n0.266354\nLehren\n\n\n41\npBM EA 10509 (Ptahhotep, Version L2+L2G) || Di...\n0.262441\nLehren\n\n\n...\n...\n...\n...\n\n\n100\npLouvre N 3136 || Kriegsgeschichte Ramses' III.\n0.001342\nErzÃ¤hlungen\n\n\n97\noTurin Cat. 9587 = oTurin CGT 57561 || Eine Tr...\n0.001076\nErzÃ¤hlungen\n\n\n98\npAshmolean 1960.1265a-g || ErzÃ¤hlung vom KÃ¶nig...\n0.000959\nErzÃ¤hlungen\n\n\n106\n01. oKV 18/3.614+627 || Die Lehre des Amunnacht\n0.000944\nLehren\n\n\n110\n03. oGrdseloff || Die Lehre des Amunnacht\n0.000944\nLehren\n\n\n\n\n124 rows Ã— 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# Create a histogram using Plotly Express\nfig = px.histogram(df_eigenvector_centrality, x='eigenvector_centrality', nbins=20,\n                   title='Histogram of Eigenvector Centrality',\n                   labels={'eigenvector_centrality': 'Eigenvector Centrality'})\n\nfig.update_layout(\n    yaxis_title=\"Number of Texts\")  # Setze den Titel der Y-Achse\n\n# Show the plot\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\n# Ploting relationship between degree and eigenvector centrality\n\n# Calculate the degree for each node\ndegree_dict = dict(G.degree())\ndegree_series = pd.Series(degree_dict, name='degree')\n\n# Calculate the eigenvector centrality for each node\neigenvector_dict = nx.eigenvector_centrality(G, max_iter=500)\neigenvector_series = pd.Series(eigenvector_dict, name='eigenvector')\n\n# Combine the degree and betweenness centrality into a DataFrame\ndf = pd.DataFrame({'degree': degree_series, 'eigenvector': eigenvector_series})\ndf.index.name = 'text_name' # Set the name of the index\n# Add genre info ('corpus_manual') as a column to the df\ndf = df.merge(metadata[['text_name', 'corpus_manual']], on='text_name', how='left')\n\n# Calculate the average betweenness centrality for each degree\naverage_eigenvector = df.groupby('degree')['eigenvector'].mean().reset_index()\n\n# Create a scatter plot using Plotly Express\nfig = px.scatter(df.reset_index(), x='degree', y='eigenvector',\n                 color='corpus_manual',\n                 hover_data=['text_name', 'degree'],\n                 title='Degree vs Eigenvector Centrality',\n                 labels={'degree': 'Degree', 'eigenvector': 'Eigenvector Centrality'})\n\n# Calculate the average eigenvector centrality for each degree if not already done\naverage_eigenvector = df.groupby('degree')['eigenvector'].mean().reset_index()\n\n# Add the average curve using Plotly Graph Objects\nfig.add_trace(go.Scatter(x=average_eigenvector['degree'], y=average_eigenvector['eigenvector'],\n                         mode='lines', name='Average Eigenvector Centrality', line=dict(color='red')))\n\n# Show the plot\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n# Assuming 'G' is your NetworkX graph\n# Calculate eigenvector centrality\ncentrality = nx.eigenvector_centrality(G, max_iter=1000)\n\n# Get node list and centrality values in the same order\nnodes = list(G.nodes())\ncentrality_values = [centrality[node] for node in nodes]\n\n# Create adjacency matrix: consider only existing connections (1), ignore non-existing ones (0)\nadj_matrix = nx.to_numpy_array(G, nodelist=nodes)\n\n# Weight adjacency matrix by eigenvector centrality of both connected nodes\n# np.outer() calculates the combined influence of node i and node j\nweighted_adj_matrix = np.outer(centrality_values, centrality_values) * adj_matrix\n\n# Use px.imshow to create a heatmap\nfig = px.imshow(weighted_adj_matrix,\n                labels=dict(x=\"Node Index\", y=\"Node Index\", color=\"Weight\"),\n                x=nodes,\n                y=nodes,\n                title=\"Heatmap of Adjacency Matrix Weighted by Eigenvector Centrality\")\nfig.update_xaxes(side=\"bottom\")  # Ensures x-axis labels are on the bottom\n\n# adjust size of the heatmap\nfig.update_layout(\n    autosize=False,\n    width=1500,\n    height=1500)\n\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\nFinding connected groups? issues of community detection\nIdentifying sub-groups of connected nodes within your network is one of the more common tasks in network analysis, especially in social network analysis (SNA). Two main definitions of connected elements are:\n\nClique: A clique is a subset of nodes in a network, where every pair of nodes is connected by an edge. In the social sciences, only cliques of three nodes or more are usually considered. The definition of a clique is independent of whether it applies to the whole network or not. For example, a network can consist of multiple cliques, or an entire network can be one clique. The latter can also be called a complete network. (Collar, Anna et al.Â 2015, Networks in Archaeology: Phenomena, Abstraction, Representation, J Archaeol Method Theory 22, 1â€“32, DOI 10.1007/s10816-014-9235-6, p.19).\nCommunity: A sub-group of nodes that is densely connected internally. (Peeples, Matthew A. and Robert J. Bischoff. 2023 Archaeological networks, community detection, and critical scales of interaction in the U.S. Southwest/Mexican Northwest Journal of Anthropological Archaeology 70:101511).\n\nThis notebook was created by Avital Romach, with additional code and text by Eliese-Sophia Lincke, Shai Gordin and Daniel A. Werning in Spring 2024 for the course Ancient Language Processing. Code can be reused under a CC-BY 4.0"
  },
  {
    "objectID": "notebooks/06_PMI_ASahala.html",
    "href": "notebooks/06_PMI_ASahala.html",
    "title": "Mutual Pointwise Information",
    "section": "",
    "text": "The code for this notebook has been prepared by Aleksi Sahala (University of Helsinki), 2019-2024, and is based on his research. It should be cited accordingly.\ngithub.com/asahala"
  },
  {
    "objectID": "notebooks/03_VectorizingTexts.html",
    "href": "notebooks/03_VectorizingTexts.html",
    "title": "Exploring Texts using the Vector Space Model",
    "section": "",
    "text": "Text and code copied from: Karsdorp, F., Kestemont, M., & Riddell, A. (2021). Humanities Data Analysis: Case Studies with Python, Princeton University Press. Adapted by Eliese-Sophia Lincke & Shai Gordin for the purposes of the course Ancient Language Processing (summer term 2024)\n\n\nWhen using the vector space model, a corpusâ€”a collection of documents, each represented as a bag of wordsâ€”is typically represented as a matrix, in which each row represents a document from the collection, each column represents a word from the collectionâ€™s vocabulary, and each cell represents the frequency with which a particular word occurs in a document.\nA matrix arranged in this way is often called a document-term matrixâ€”or term-document matrix where: * rows are associated with documents * word counts are in the columns.\n```jifuaxd Example of a vector space representation with four documents (rows) and a vocabulary of four words (columns). For each document the table lists how often each vocabulary item occurs.\n\n\n\n\nroi\nange\nsang\nperdu\n\n\n\n\n\\(d_1\\)\n1\n2\n16\n21\n\n\n\\(d_2\\)\n2\n2\n18\n19\n\n\n\\(d_3\\)\n35\n41\n0\n2\n\n\n\\(d_4\\)\n39\n55\n1\n0\n\n\n\n\nIn this table, each document $d_i$ is represented as a vector, which, essentially, is a list of numbers---word frequencies in our present case. A &lt;span class=\"index\"&gt;vector space&lt;/span&gt; is nothing more than a collection of numerical vectors, which may, for instance, be added together and multiplied by a number. Documents represented in this manner may be compared in terms of their *coordinates* (or *components*). For example, by comparing the four documents on the basis of the second coordinate, we observe that the first two documents ($d_1$ and $d_2$) have similar counts, which might be an indication that these two documents are somehow more similar. To obtain a more accurate and complete picture of document similarity, we would like to be able to compare documents more holistically, using *all* their components. In our example, each document represents a point in a four-dimensional vector space. We might hypothesize that similar documents use similar words, and hence reside close to each other in this space. To illustrate this, we demonstrate how to visualize the documents in space using the first and third components.\n\n![](https://www.humanitiesdataanalysis.org/_images/notebook_2_4.png)\n\n\n* TF-IDF: Concept\n![](https://www.romainberg.com/wp-content/uploads/TF_IDF-final.png)\n\n* TF-IDF: Simple calculation\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wQ-FCEYoioz6tOlYFssXeg.png)\n\n### Preprocessing\n\n::: {#sCB5jEe2jaAp .cell}\n``` {.python .cell-code}\n# raw data\n\nakk05 = ['ana', 'eÅ¡Å¡Å«tu', 'kaÅ¡Äru', 'mÄtu', 'AÅ¡Å¡ur', 'ana', 'UNK', 'Älu', 'epÄ“Å¡u', 'Ä“kallu', 'mÅ«Å¡abu', 'Å¡arrÅ«tu', 'ina', 'libbu', 'nadÃ»', 'UNK', 'Å¡umu', 'nabÃ»', 'kakku', 'AÅ¡Å¡ur', 'bÄ“lu', 'ina', 'libbu', 'ramÃ»', 'niÅ¡u', 'mÄtu', 'kiÅ¡ittu', 'qÄtu', 'ina', 'libbu', 'waÅ¡Äbu', 'biltu', 'maddattu', 'kÃ¢nu', 'itti', 'niÅ¡u', 'mÄtu', 'AÅ¡Å¡ur', 'manÃ»', 'á¹£almu', 'Å¡arrÅ«tu', 'u', 'á¹£almu', 'ilu', 'rabÃ»', 'bÄ“lu', 'epÄ“Å¡u', 'lÄ«tu', 'u', 'danÄnu', 'Å¡a', 'ina', 'zikru', 'AÅ¡Å¡ur', 'bÄ“lu', 'eli', 'mÄtu', 'Å¡akÄnu', 'ina', 'muhhu', 'Å¡aá¹­Äru', 'ina', 'UNK', 'izuzzu', 'UNK', 'UNK', 'biltu', 'hurÄá¹£u', 'ina', 'dannu', 'UNK', 'lÄ«m', 'biltu', 'kaspu', 'UNK', 'maddattu', 'mahÄru', 'ina', 'UNK', 'palÃ»', 'AÅ¡Å¡ur', 'bÄ“lu', 'takÄlu', 'ana', 'Namri', 'UNK', 'Bit-Zatti', 'Bit-Abdadani', 'Bit-Sangibuti', 'UNK', 'alÄku', 'UNK', 'akÄmu', 'gerru', 'amÄru', 'Nikkur', 'Älu', 'dannÅ«tu', 'waÅ¡Äru', 'UNK', 'zanÄnu', 'Nikkurayu', 'kakku', 'UNK', 'sisÃ»', 'parÃ»', 'alpu', 'UNK', 'SassiaÅ¡u', 'TutaÅ¡di', 'UNK']\nakk08 = ['UNK', 'niÅ¡u', 'ana', 'mÄtu', 'AÅ¡Å¡ur', 'warÃ»', 'UNK', 'UNK', 'ina', 'UNK', 'palÃ»', 'AÅ¡Å¡ur', 'bÄ“lu', 'takÄlu', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'Sulumal', 'Meliddayu', 'Tarhu-lara', 'Gurgumayu', 'UNK', 'UNK', 'UNK', 'mÄtitÄn', 'ana', 'emÅ«qu', 'ahÄmiÅ¡', 'takÄlu', 'UNK', 'UNK', 'ina', 'lÄ«tu', 'u', 'danÄnu', 'Å¡a', 'AÅ¡Å¡ur', 'bÄ“lu', 'itti', 'mahÄá¹£u', 'dÄ«ktu', 'dÃ¢ku', 'UNK', 'UNK', 'qurÄdu', 'dÃ¢ku', 'hurru', 'natbÄku', 'Å¡adÃ»', 'malÃ»', 'narkabtu', 'UNK', 'UNK', 'ana', 'lÄ', 'mÄnu', 'leqÃ»', 'ina', 'qablu', 'tidÅ«ku', 'Å¡a', 'IÅ¡tar-duri', 'UNK', 'UNK', 'UNK', 'UNK', 'ina', 'qÄtu', 'á¹£abÄtu', 'UNK', 'lÄ«m', 'UNK', 'lÄ«m', 'UNK', 'meÊ¾atu', 'UNK', 'UNK', 'UNK', 'UNK', 'iÅ¡tu', 'UNK', 'UNK', 'IÅ¡tar-duri', 'ana', 'ezÄ“bu', 'napiÅ¡tu', 'mÅ«Å¡iÅ¡', 'halÄqu', 'lÄma', 'Å¡amÅ¡u', 'urruhiÅ¡', 'napruÅ¡u', 'UNK', 'UNK', 'itti', 'Å¡iltÄhu', 'pÄriÊ¾u', 'napiÅ¡tu', 'adi', 'titÅ«ru', 'Purattu', 'miá¹£ru', 'mÄtu', 'á¹­arÄdu', 'erÅ¡u', 'UNK', 'UNK', 'Å¡a', 'Å¡adÄdu', 'Å¡arrÅ«tu', 'kunukku', 'kiÅ¡Ädu', 'adi', 'abnu', 'kiÅ¡Ädu', 'narkabtu', 'Å¡arrÅ«tu', 'UNK', 'UNK', 'mimma', 'Å¡umu', 'mÄdu', 'Å¡a', 'nÄ«bu', 'lÄ', 'iÅ¡Ã»', 'ekÄ“mu', 'sisÃ»', 'UNK', 'UNK', 'ummiÄnu', 'ana', 'lÄ', 'mÄnu', 'leqÃ»', 'bÄ«tu', 'á¹£Ä“ru', 'kuÅ¡tÄru', 'Å¡arrÅ«tu', 'UNK', 'UNK', 'unÅ«tu', 'tÄhÄzu', 'mÄdu', 'ina', 'qerbu', 'uÅ¡mannu', 'ina', 'iÅ¡Ätu', 'Å¡arÄpu', 'UNK', 'UNK', 'UNK', 'erÅ¡u', 'ana', 'IÅ¡tar', 'Å¡arratu', 'Ninua', 'qiÄÅ¡u', 'UNK']\nakk11 = ['maÅ¡ku', 'pÄ«ru', 'Å¡innu', 'pÄ«ru', 'argamannu', 'takiltu', 'lubuÅ¡tu', 'birmu', 'kitÃ»', 'lubuÅ¡tu', 'mÄtu', 'mÄdu', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'tillu', 'pilaqqu', 'UNK', 'UNK', 'UNK', 'UNK', 'ina', 'qerbu', 'Arpadda', 'mahÄru', 'Tutammu', 'Å¡arru', 'Unqi', 'ina', 'adÃ»', 'ilu', 'rabÃ»', 'haá¹­Ã»', 'Å¡iÄá¹­u', 'napiÅ¡tu', 'gerru', 'UNK', 'lÄ', 'malÄku', 'itti', 'ina', 'uzzu', 'libbu', 'UNK', 'Å¡a', 'Tutammu', 'adi', 'rabÃ»', 'UNK', 'Kunalua', 'Älu', 'Å¡arrÅ«tu', 'kaÅ¡Ädu', 'niÅ¡u', 'adi', 'marÅ¡Ä«tu', 'UNK', 'kÅ«danu', 'ina', 'qerbu', 'ummÄnu', 'kÄ«ma', 'á¹£Ä“nu', 'manÃ»', 'UNK', 'ina', 'qabaltu', 'Ä“kallu', 'Å¡a', 'Tutammu', 'kussÃ»', 'nadÃ»', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'meÊ¾atu', 'biltu', 'kaspu', 'ina', 'dannu', 'UNK', 'meÊ¾atu', 'biltu', 'UNK', 'UNK', 'unÅ«tu', 'tÄhÄzu', 'lubuÅ¡tu', 'birmu', 'kitÃ»', 'rÄ«qu', 'kalÄma', 'bÅ«Å¡u', 'Ä“kallu', 'UNK', 'Kunalua', 'ana', 'eÅ¡Å¡Å«tu', 'á¹£abÄtu', 'Unqi', 'ana', 'pÄá¹­u', 'gimru', 'kanÄÅ¡u', 'UNK', 'Å¡Å«t', 'rÄ“Å¡u', 'bÄ“lu', 'pÄ«hÄtu', 'eli', 'Å¡akÄnu']\nakk13 = ['UNK', 'ana', 'Hatti', 'adi', 'mahru', 'wabÄlu', 'Å¡Å«t', 'rÄ“Å¡u', 'Å¡aknu', 'mÄtu', 'NaÊ¾iri', 'Supurgillu', 'UNK', 'adi', 'Älu', 'Å¡a', 'liwÄ«tu', 'kaÅ¡Ädu', 'Å¡allatu', 'Å¡alÄlu', 'Å iqila', 'rabÃ»', 'birtu', 'UNK', 'Å¡alÄlu', 'ana', 'Hatti', 'adi', 'mahru', 'wabÄlu', 'UNK', 'meÊ¾atu', 'Å¡allatu', 'Amlate', 'Å¡a', 'Damunu', 'UNK', 'lÄ«m', 'UNK', 'meÊ¾atu', 'Å¡allatu', 'Deri', 'ina', 'Kunalua', 'UNK', 'Huzarra', 'TaÊ¾e', 'Tarmanazi', 'Kulmadara', 'Hatatirra', 'Irgillu', 'Älu', 'Å¡a', 'Unqi', 'waÅ¡Äbu', 'UNK', 'Å¡allatu', 'Qutu', 'Bit-Sangibuti', 'UNK', 'lÄ«m', 'UNK', 'meÊ¾atu', 'Illilayu', 'UNK', 'lÄ«m', 'UNK', 'meÊ¾atu', 'UNK', 'Nakkabayu', 'Budayu', 'ina', 'UNK', 'á¹¢imirra', 'Arqa', 'Usnu', 'SiÊ¾annu', 'Å¡a', 'Å¡iddu', 'tiÄmtu', 'waÅ¡Äbu', 'UNK', 'meÊ¾atu', 'UNK', 'Budayu', 'Dunu', 'UNK', 'UNK', 'UNK', 'UNK', 'meÊ¾atu', 'UNK', 'Belayu', 'UNK', 'meÊ¾atu', 'UNK', 'Banitayu', 'UNK', 'meÊ¾atu', 'UNK', 'Palil-andil-mati', 'UNK', 'meÊ¾atu', 'UNK', 'Sangillu', 'UNK', 'Illilayu', 'UNK', 'meÊ¾atu', 'UNK', 'Å¡allatu', 'Qutu', 'Bit-Sangibuti', 'ina', 'pÄ«hÄtu', 'TuÊ¾immu', 'waÅ¡Äbu', 'UNK', 'meÊ¾atu', 'UNK', 'Å¡allatu', 'Qutu', 'Bit-Sangibuti', 'ina', 'Til-karme', 'waÅ¡Äbu', 'itti', 'niÅ¡u', 'mÄtu', 'AÅ¡Å¡ur', 'manÃ»', 'ilku', 'tupÅ¡ikku', 'kÄ«', 'Å¡a', 'AÅ¡Å¡uru', 'emÄ“du', 'maddattu', 'Å¡a', 'KuÅ¡taÅ¡pi', 'Kummuhayu', 'Rahianu', 'Å a-imeriÅ¡ayu', 'Menaheme', 'Samerinayu', 'Hi-rumu', 'á¹¢urrayu', 'Sibitti-BiÊ¾il', 'Gublayu', 'Uriaikki', 'Quayu', 'Pisiris', 'GargamiÅ¡ayu', 'Eni-il', 'Hamatayu', 'Panammu', 'SamÊ¾allayu', 'Tarhu-lara', 'Gurgumayu', 'Sulumal', 'Meliddayu', 'Dadilu']\n:::\n\n# returns a list of lists, each lists is one document in the corpus\n\ntokenized_corpus = []\nfor doc in [akk05, akk08, akk11, akk13] :\n  akk = []\n  for token in doc :\n    akk.append(token)#.lower())\n  tokenized_corpus.append(akk)\n\nfor doc in tokenized_corpus :\n  print(doc)\n\nCounter implements a number of methods specialized for convenient and rapid tallies. For instance, the method Counter.most_common returns the n most frequent items:\n\n# Count token frequencies\n\nvocabulary_akk = Counter(akk05)\nprint(vocabulary_akk)\nprint(vocabulary_akk.most_common(n=5))\n\n\n\n\n\n\nArguments:\n\ntokenized_corpus (list): a tokenized corpus represented, list of lists of strings.\nmin_count (int, optional): the minimum occurrence count of a vocabulary item in the corpus.\nmax_count (int, optional): the maximum occurrence count of a vocabulary item in the corpus. Note that the default maximum count is set to infinity (max_count=float(â€˜infâ€™)). This ensures that none of the high-frequency words are filtered without further specification.\n\nReturns:\n\nlist: an alphabetically ordered list of unique words in the corpus\n\n\n\ndef extract_vocabulary(tokenized_corpus, min_count=1, max_count=float('inf')):\n\n    vocabulary = collections.Counter()\n    for document in tokenized_corpus:\n        vocabulary.update(document)\n    vocabulary = {word for word, count in vocabulary.items()\n                  if count &gt;= min_count and count &lt;= max_count}\n    return sorted(vocabulary)\n\n\n# Call the function\n\nvocabulary = extract_vocabulary(tokenized_corpus, min_count = 1)\nprint(vocabulary)\n\n\n# Check token counts for each type in the vocabulary\nbags_of_words = []\nfor document in tokenized_corpus:\n    tokens = [word for word in document if word in vocabulary]\n    bags_of_words.append(collections.Counter(tokens))\n    #bags_of_words.extend(collections.Counter(tokens))\n\nfor count in bags_of_words :\n    print(count)\n#print(bags_of_words)\n\n\n\n\nTransform a tokenized corpus into a document-term matrix.\n\nArguments:\n\ntokenized_corpus (list): a tokenized corpus as a list of lists of strings.\nvocabulary (list): A list of unique words (types).\n\nReturns:\n\nlist: A list of lists representing the frequency of each term in vocabulary for each document in the corpus.\n\n\n\n## Calculate term frequency (TF)\n# raw count\n\ndef corpus2dtm_raw(tokenized_corpus, vocabulary):\n\n    document_term_matrix = []\n    for document in tokenized_corpus:\n        document_counts = collections.Counter(document)\n        row = [document_counts[word] for word in vocabulary]\n        document_term_matrix.append(row)\n    return document_term_matrix\n\n\n# Call the function\ndocument_term_matrix = corpus2dtm_raw(tokenized_corpus, vocabulary)\nprint(document_term_matrix)\n\n# Convert result into a dataframe\ntf_df_abs = pd.DataFrame(document_term_matrix, columns=vocabulary)\n\ntf_df_abs\n\nThere are three (and possibly more) ways to calculate the TF: * raw count (token count) â€“ like in the function above  This is the simplest form, where TF is just the raw count of the term in the document: TF(t,d) = countÂ ofÂ termÂ tÂ inÂ documentÂ d * relative frequency (token count / number of tokens in the document) TF is normalized by dividing the raw count by the total number of terms in the document:  TF(t,d) = countÂ ofÂ termÂ tÂ inÂ documentÂ d / totalÂ numberÂ ofÂ tokensÂ inÂ documentÂ d * logarithmically scaled: typically involves a normalization step to account for the length of the document. This reduces the impact of (very) frequent terms. TF(t,d) = log(count of term t in document d + 1)  (When the term frequency is 0, + 1 avoids log(0) which would result in an error.)\n\n# Demonstration of different calculations of the term frequency\n\ndef raw_count_tf(term, document):\n    return document.count(term)\n\ndef relative_frequency_tf(term, document):\n    term_count = document.count(term)\n    total_terms = len(document)\n    return term_count / total_terms if total_terms &gt; 0 else 0 # avoid division by 0\n\ndef log_scaled_tf(term, document):\n    total_terms = len(document)\n    term_count = document.count(term)\n    return np.log(1 + term_count) # avoid log(0) by adding 1\n\n# Example document\ndocument = [\"this\", \"is\", \"a\", \"sample\", \"document\", \"document\", \"is\", \"a\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\"]\n\nterm = \"sample\" # change to \"sample\" to see the scaling down effect for frequent terms\n\nprint(\"Raw Count TF:\", raw_count_tf(term, document))\nprint(\"Relative Frequency TF:\", relative_frequency_tf(term, document))\nprint(\"Log Scaled TF:\", log_scaled_tf(term, document)) # natural base of logarithm (e = \"Euler's number\")\n\n\n## Calculate term frequency (TF) for our corpus\n# relative frequency\n\ndef corpus_tf(tokenized_corpus, vocabulary):\n    document_term_matrix = []\n    for document in tokenized_corpus:\n        term_per_document_counts = collections.Counter(document)\n        total_terms = sum(term_per_document_counts.values())\n        #total_terms = len(document_counts)\n        #row = [term_per_document_counts[word] for word in vocabulary]\n        row = [np.log(1 + term_per_document_counts[word]) for word in vocabulary]\n        document_term_matrix.append(row)\n    return document_term_matrix\n\n\n# Call the function\n\n# Calculate term frequency (TF) document-term matrix\nterm_frequency_matrix = corpus_tf(tokenized_corpus, vocabulary)\n\n# Convert the matrix to a DataFrame for easier visualization\ntf_df_log = pd.DataFrame(term_frequency_matrix, columns=vocabulary)\n\ntf_df_log#[\"libbu\"]\n\n\n\n\n\nN = number of documents (in the corpus) \ndf_term_counts = number of documents which contain term t\nabsolute  IDF(t) = number of documents / number of documents containing term t\nlogarithmic  IDF(t) = log(number of documents / number of documents containing term t + 1)\n\n\n# Step 1: Calculate the total number of documents (N): How many documents are there in the corpus?\nN = len(tf_df_abs)\nprint(N)\n\n# Step 2: Calculate the document frequency (DF) for each term: In how many documents does the term appear?\ndf_nonzero = tf_df_abs &gt; 0  # Convert counts to binary (True/False)\n#print(df_nonzero)\ndf_term_counts = df_nonzero.sum(axis=0)  # Sum across rows, i.e. across documents; rows/documents set to 'False' are not counted\nprint(df_term_counts)\n\n# Step 3: Calculate the IDF for each term\n# add 1 for normalization\nidf = np.log(N / (df_term_counts + 1 )) + 1 # natural base (\"Euler's number\")\n\n# Display the IDF values\nidf_df = pd.DataFrame(idf, columns=['IDF']).T\nidf_df.loc['df_term_count'] = df_term_counts\nidf_df#.iloc[:, 90:105]\n\n\n\n\n\n\n\nMultiply term frequency and inverse document frequency\n\n# Perform element-wise multiplication\ntf_idf_df = tf_df_log * idf_df.loc['IDF']\n\n#tf_idf_df.iloc[:, 90:105]\ntf_idf_df#[\"libbu\"]\n\n\n\n\n\ndef calculate_tf_weighted(term, document):\n    term_count = document.count(term)\n    if term_count &gt; 0:\n        tf = 1 + np.log(term_count)\n    else:\n        tf = 0\n    return tf\n\ndef calculate_idf_weighted(term, corpus):\n    num_documents_with_term = sum(1 for doc in corpus if term in doc)\n    idf = np.log((len(corpus) + 1) / (num_documents_with_term + 1)) + 1\n    return idf\n\ndef calculate_tf_idf_weighted(term, document, corpus):\n    tf = calculate_tf_weighted(term, document)\n    idf = calculate_idf_weighted(term, corpus)\n    tf_idf = tf * idf\n    return tf_idf\n\n# Define a function to calculate the weighted TF-IDF for every document in the corpus\ndef calculate_tf_idf_matrix(tokenized_corpus, vocabulary):\n    tf_idf_matrix = []\n    for document in tokenized_corpus:\n        row = [calculate_tf_idf_weighted(term, document, tokenized_corpus) for term in vocabulary]\n        tf_idf_matrix.append(row)\n    return tf_idf_matrix\n\n\n#Calculate the TF-IDF weighted matrix for the entire corpus\ntf_idf_matrix = calculate_tf_idf_matrix(tokenized_corpus, vocabulary)\n\n# Step 3: Convert the matrix to a DataFrame\ntf_idf_df = pd.DataFrame(tf_idf_matrix, columns=vocabulary)\n\n# Display the resulting TF-IDF DataFrame\ntf_idf_df#[\"AÅ¡Å¡ur\"]#[\"libbu\"]\n\n\n\n\n\nIn the main code of the course, we will use a predefined function from the machine learning library scikit-learn.\nLearn more about the weighting and normalization in scikit-learns TF-IDF calculations,  cf.Â in particular the â€œNumeric example of a tf-idf matrixâ€\nTfidfVectorizer requires a list of strings as input. Each string is an entire text (document).\n\n## Alternative preprocessing for the TfidfVectorizer from SciKitLearn\n# returns a list of strings, each string is one document\n\ntokenized_corpus_asStr = []\nfor doc in [akk05, akk08, akk11, akk13] :\n  akk = \"\"\n  for token in doc :\n    akk = akk + token + \" \"\n  tokenized_corpus_asStr.append(akk.strip())\n\nfor doc in tokenized_corpus_asStr :\n  print(doc + \" ------\")\n#print(tokenized_corpus_asStr)\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n#from sklearn.feature_extraction.text import TfidfTransformer\n\n# Step 1: Instantiate TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df = 0, max_df = 100, norm = \"l1\") # with and without normalization (norm = None)\n\n# Step 2: Fit and transform the corpus\ntfidf_matrix = vectorizer.fit_transform(tokenized_corpus_asStr)\n\n# Display the TF-IDF matrix\n#tfidf_matrix.toarray()\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\ntfidf_df#[\"aÅ¡Å¡ur\"]#[\"libbu\"]\n\n# Display the feature names\n#vectorizer.get_feature_names_out()"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "resources",
    "section": "",
    "text": "Mandatory\nGavin, Michael (2022) Literary Mathematics: Quantitative Theory for Textual Studies. Stanford, Stanford University Press. https://www.zotero.org/groups/4809611/ancient_language_processing/items/itemKey/4VL57XZQ\nSvÃ¤rd, S., Alstola, T., Jauhiainen, H., Sahala, A., and LindÃ©n, K. (2020) Fear in Akkadian texts: New digital perspectives on lexical semantics, in: S.-W. Hsu and J. Llop-RaduÃ  (eds.). Culture and History of the Ancient Near East. Leiden, Brill. https://doi.org/10.1163/9789004430761_019.\n\n\nOptional\nAlstola, T., Jauhiainen, H., SvÃ¤rd, S., Sahala, A., and LindÃ©n, K. (2022) Digital Approaches to Analyzing and Translating Emotion: What Is Love?, in: K. Sonik and U. Steinert (eds.). The Routledge Handbook of Emotions in the Ancient Near East. London & New York, Routledge. https://doi.org/10.4324/9780367822873-6\nBerti, Monica, ed.Â (2019) Digital Classical Philology: Ancient Greek and Latin in the Digital Revolution. De Gruyter. https://doi.org/10.1515/9783110599572\nCarroll, S.R., Herczog, E., Hudson, M. et al.Â (2021) Operationalizing the CARE and FAIR Principles for Indigenous data futures. Sci Data 8, 108. https://doi.org/10.1038/s41597-021-00892-0\nFor more see the designated course Zotero library still under construction: https://www.zotero.org/groups/4809611/ancient_language_processing\n\n\nTechnical and Code\nIn ALP we explore how texts can be measurably compared to one another, and to do we turn texts into vectors. If you are interested in studying individual words, e.g.Â look at word vectors, look at the following website"
  },
  {
    "objectID": "about.html#unit-1-what-is-ancient-language-processing",
    "href": "about.html#unit-1-what-is-ancient-language-processing",
    "title": "Schedule",
    "section": "",
    "text": "Date\nTopic\nAssignment\nMaterials\n\n\n\n\n04/15/26\nBrushing-up on your Python\ncharacter encoding; regex; pandas; parsing; functions\nChoose one ancient language dataset: familiarize yourself with the data model, consider how this data model fits with the datasets presented in class. Remodel your data accordingly. Do not forget collecting metadata as defined in the class examples.\nTutorials:\n\nHow to define a function\nList comprehensions\nLoad, parse and extract data from ORACC json files\nRegEx101.com\n\n\n\n23/04/25\nDoing Computational Philology: the (ideal) pipeline\nresearch questions; microscope vs macroscope; quantification; large language models\nExplore your dataset: upload the data into Voyant (or similar programs) or into an LLM and describe it based on distant reading techniques (i.e.Â quantification). Following this analysis decide where close reading would be necessary.\nReading:\nRockwell & Sinclair (2016); Sommerschield et al.Â (2023)\nTools:\n\nVoyant Tools\nAntConc\nLLM (ChatGPT, Claude, Olmoe etc.)"
  },
  {
    "objectID": "notebooks/04_Vector_Space_Model_Egyptian.html",
    "href": "notebooks/04_Vector_Space_Model_Egyptian.html",
    "title": "Intro",
    "section": "",
    "text": "In this notebook, we explore a collection of ancient Akkadian and ancient Egyptian texts using the vector space model approach described by Karsdorp et al.Â in the chapter â€œExploring Texts using the Vector Space Modelâ€. By representing the texts as numeric vectors capturing word frequencies, we can quantify the lexical similarities and differences between corpora in each of these two ancient languages. The vector space model allows us to reason about texts spatially and apply geometric concepts like distance metrics to assess how â€œcloseâ€ texts are to each other based on shared vocabulary.\nWe preprocess the texts by tokenizing them into words, constructing a document-term matrix recording word frequencies per text, and analyzing the matrix using tools from the Python scientific computing stack, including NumPy, SciPy and Scikit-learn. Through techniques like tSNE (t-Distributed Stochastic Neighbor Embedding) and aggregation by text metadata like script type, language or genre, we explore patterns in the Akkadian and Egyptian corpora and showcase how the vector space model can yield quantitative insights into ancient textual data. The notebook serves as an example application of the concepts and methods covered in depth by Karsdorp et al.Â in their chapter.\nThis notebook has been prepared by Avital Romach and is based on her research. It should be cited accordingly (see citation information at the bottom)."
  },
  {
    "objectID": "notebooks/04_Vector_Space_Model_Egyptian.html#imports",
    "href": "notebooks/04_Vector_Space_Model_Egyptian.html#imports",
    "title": "Intro",
    "section": "Imports",
    "text": "Imports\n\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nimport requests\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom scipy.spatial.distance import pdist, squareform\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport plotly.express as px"
  },
  {
    "objectID": "notebooks/04_Vector_Space_Model_Egyptian.html#functions",
    "href": "notebooks/04_Vector_Space_Model_Egyptian.html#functions",
    "title": "Intro",
    "section": "Functions",
    "text": "Functions\n\nTo upload corpus and metadata from GitHub\n\nFunctions and import for the Akkadian corpus\nThe Akkadian corpus consists of a part of the Royal Inscriptions of the Neo-Assyrian Period (RINAP), licensed CC-BY-SA, and was taken from Open Richely Annotated Cuneiform Corpus (ORACC).\n\ndef create_corpus_from_github_api(url):\n  # URL on the Github where the csv files are stored\n  github_url = url\n  response = requests.get(github_url)\n\n  corpus = []\n  # Check if the request was successful\n  if response.status_code == 200:\n    files = response.json()\n    for file in files:\n      if file[\"download_url\"][-3:] == \"csv\":\n        corpus.append(pd.read_csv(file[\"download_url\"], encoding=\"utf-8\", index_col=\"Unnamed: 0\").fillna(\"\"))\n        # For Egyptian adapt like this:\n        #corpus.append(pd.read_csv(file[\"download_url\"], encoding=\"utf-8\").fillna(\"\"))\n  else:\n    print('Failed to retrieve files:', response.status_code)\n\n  return corpus\n\ndef get_metadata_from_raw_github(url):\n  metadata = pd.read_csv(url, encoding=\"utf-8\", index_col=\"Unnamed: 0\").fillna(\"\")\n  return metadata\n\n\n# Prepare Akkadian corpus (list of dataframes)\n\ncorpus = create_corpus_from_github_api('https://api.github.com/repos/DigitalPasts/ALP-course/contents/course_notebooks/data/rinap01')\ncorpus.extend(create_corpus_from_github_api('https://api.github.com/repos/DigitalPasts/ALP-course/contents/course_notebooks/data/rinap05'))\n\n\n# Prepare Akkadian metadata\nmetadata = get_metadata_from_raw_github(\"https://raw.githubusercontent.com/DigitalPasts/ALP-course/master/course_notebooks/data/rinap1_5_metadata.csv\")\n\n\n\nFunctions and import for the Egyptian corpus\nThe Egyptian corpus is an extract of the database of the Thesaurus Linguae Aegyptiae (TLA), containing literary (and if you like: medical) texts. This export from the database is not published under a free license. Therefore, we access it from a private GitHub repository using an access token.\n\ndef create_corpus_from_private_github_api(url, token):\n# URL on the Github where the csv files are stored\n    headers = {\n        \"Authorization\": f\"token {token}\"\n    }\n    github_url = url\n    response = requests.get(github_url, headers=headers)\n\n    dtype_dict = {\"lemma_id\": \"str\"}\n\n    corpus = []\n    # Check if the request was successful\n    if response.status_code == 200:\n        files = response.json() # Github API provides information about the data in the repository, e.g. the download_url\n        for file in files:\n            if file[\"download_url\"][-3:] == \"csv\" or \".csv?token=\" in file[\"download_url\"]:\n                corpus.append(pd.read_csv(file[\"download_url\"], encoding=\"utf-8\", sep = ',', dtype=dtype_dict).fillna(\"\"))\n    else:\n        print('Failed to retrieve files:', response.status_code)\n\n    return corpus\n\nfrom io import StringIO\n\ndef get_metadata_from_raw_private_github(url, token):\n    headers = {\n        \"Authorization\": f\"token {token}\"\n    }\n    github_url = url\n    response = requests.get(github_url, headers=headers)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        csv_data = StringIO(response.text)\n        metadata = pd.read_csv(csv_data, encoding=\"utf-8\", sep = ',', index_col=\"text_id\").fillna(\"\")\n        return metadata\n    else:\n        raise Exception(f\"Failed to retrieve metadata: {response.status_code}\")\n\n\n# only if corpus is not yet loaded\n# Prepare Egyptian corpus (lists of dataframes)\n\n#if False:\n\n# NB: This token will expire at the end ot the year (2025)\ntla_access_token = \"github_pat_...\" # to be added manually.\n\n  ## TLA Literature\ncorpus = create_corpus_from_private_github_api('https://api.github.com/repos/thesaurus-linguae-aegyptiae/test-rawdata/contents/alp-course-2024/TLA_literature/erzaehlungen', tla_access_token)\n\ncorpus.extend(create_corpus_from_private_github_api('https://api.github.com/repos/thesaurus-linguae-aegyptiae/test-rawdata/contents/alp-course-2024/TLA_literature/reden', tla_access_token))\n\ncorpus.extend(create_corpus_from_private_github_api('https://api.github.com/repos/thesaurus-linguae-aegyptiae/test-rawdata/contents/alp-course-2024/TLA_literature/lehren', tla_access_token))\n\n  ## TLA Medical\n  #corpus = create_corpus_from_private_github_api('https://api.github.com/repos/thesaurus-linguae-aegyptiae/test-rawdata/contents/alp-course-2024/TLA_medical/TLA_pEbers', tla_access_token)\n\n  #corpus.extend(create_corpus_from_private_github_api('https://api.github.com/repos/thesaurus-linguae-aegyptiae/test-rawdata/contents/alp-course-2024/TLA_medical/TLA_pEdwinSmith', tla_access_token))\n\n\n# Egyptian metadata\nmetadata = get_metadata_from_raw_private_github(\"https://raw.githubusercontent.com/thesaurus-linguae-aegyptiae/test-rawdata/master/alp-course-2024/TLA_literature/TLA_metadata.csv\", tla_access_token)\n\n\n## Check if data is loaded\ncorpus[0].head()\n\n\n    \n\n\n\n\n\n\ntext\nline\nword\nref\nfrag\nnorm\nunicode_word\nlemma_id\ncf\npos\nmask\nsense\ninst\nreading\nbreak_perc\nunicode\nbreak\n\n\n\n\n0\n2FPMTQIP45HI3LODGNIZFDO77I\n1\n2\nIBUBdxQqwvMcu0CovD1Q5OKc7B8\n[â€¦]\n[â€¦]\n[â€¦]\n\n\n\n\n\n\n\n1.0\n['â€¦']\n['missing']\n\n\n1\n2FPMTQIP45HI3LODGNIZFDO77I\n1\n3\nIBUBd1Mxn1WtEUULim1Z51mT3oc\nqd\nqd\nğ“ªğ“‚§ğ“Œğ“²ğ“»ğ“‘•\n162450\nqdd\nVERB\n\nschlafen\n\n\n0.1\n['ğ“ª', 'ğ“‚§', 'ğ“Œ', 'ğ“²', 'ğ“»']\n['complete', 'complete', 'complete', 'complete...\n\n\n2\n2FPMTQIP45HI3LODGNIZFDO77I\n1\n4\nIBUBd9E0G7Z51UgbrcPup3GC3iY\n[â€¦]\n[â€¦]\n[â€¦]\n\n\n\n\n\n\n\n1.0\n['â€¦']\n['missing']\n\n\n3\n2FPMTQIP45HI3LODGNIZFDO77I\n1\n6\nIBUBd9Ae0nMTckagsrRI5pGbPxQ\n[â€¦]\n[â€¦]\n[â€¦]\n\n\n\n\n\n\n\n1.0\n['â€¦']\n['missing']\n\n\n4\n2FPMTQIP45HI3LODGNIZFDO77I\n1\n7\nIBUBd5FjpDQ83E60jslkgkHSx4Y\n[â€¢]\nâ€¢\nâ€¢\n\nâ€¢\nPUNCT\n\n\n\n\n1.0\n['â€¢']\n['missing']\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\n\n# Prepare text_ids (list of unique ids), and metadata\n\ntext_ids = []\nfor text in corpus:\n  text_ids.append(text[\"text\"].iloc[0])\n\n\nfor id in text_ids:\n  if id not in metadata.index:\n    print(f\"Text {id} missing from metadata\")\n\nmetadata = metadata[metadata.index.isin(text_ids)]\n\nmetadata\n\n\n    \n\n\n\n\n\n\ntext_name\ncorpus_manual\npath\ndateEarliest\ndateLatest\nlanguage_manual\ntlaTextLangName\ntlaTextScriptName\ncontributors\n\n\ntext_id\n\n\n\n\n\n\n\n\n\n\n\n\n\nIMCRUSHSQ5HBHBGS3XQ2WYI3HM\npVandier = pLille 139 || Recto: Meryre und Sis...\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nFrÃ¼hdemotisch\nFrÃ¼hdemotisch\nSpÃ¤thieratische Buchschrift\n['Lutz Popko', 'AltÃ¤gyptisches WÃ¶rterbuch', 'J...\n\n\n2AT7OVM3PZDEDMYZVBIZGYAP44\npBrooklyn 47.218.135 || Brooklyner Weisheitstext\nLehren\nsawlit || Literarische Texte || 3. Weisheitsle...\n\n\nFrÃ¼hdemotisch\nFrÃ¼hdemotisch\nSpÃ¤thieratische Buchschrift\n['Peter Dils', 'AltÃ¤gyptisches WÃ¶rterbuch', 'L...\n\n\n62AD3D3IY5EMTHDDJZ5UV42RWM\nhintere Innenwand || Das Buch von der Himmelsk...\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nMittelÃ¤gyptisch\nfrÃ¼hes und klassisches MittelÃ¤gyptisch\nregulÃ¤re Hieroglyphenschrift\n['Lutz Popko', 'AltÃ¤gyptisches WÃ¶rterbuch', 'J...\n\n\nEQTNMAUWUBDP5D7K2UJLHOZO5I\nhintere Innenwand || Rahmentext\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nMittelÃ¤gyptisch\nfrÃ¼hes und klassisches MittelÃ¤gyptisch\nregulÃ¤re Hieroglyphenschrift\n['Lutz Popko', 'AltÃ¤gyptisches WÃ¶rterbuch', 'J...\n\n\n5EGJISAVIVHCLJE2JUKHR23V6E\nlinke Innenwand || Rahmentext\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nMittelÃ¤gyptisch\nfrÃ¼hes und klassisches MittelÃ¤gyptisch\nregulÃ¤re Hieroglyphenschrift\n['Lutz Popko', 'AltÃ¤gyptisches WÃ¶rterbuch', 'B...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nC6KGH3XC7RGU3DSL7HKYY2K3WM\npTurin CGT 54014, falsch CGT 54024 || Die Lehr...\nLehren\nsawlit || Literarische Texte || 3. Weisheitsle...\n\n\nMittelÃ¤gyptisch\ntraditionelles MittelÃ¤gyptisch\nNeuhieratische Buchschrift\n['Peter Dils', 'AltÃ¤gyptisches WÃ¶rterbuch', 'B...\n\n\nWOAIM6KKGJDJ5OUDFHRFJ542NM\n01. tBerlin 8934 (tB) || Die Lehre des Ani (Ve...\nLehren\nsawlit || Literarische Texte || 3. Weisheitsle...\n\n\nNeuÃ¤gyptisch\ntraditionelles MittelÃ¤gyptisch\nSpÃ¤thieratische Buchschrift\n['Peter Dils', 'AltÃ¤gyptisches WÃ¶rterbuch', 'J...\n\n\n2Z4GIGWZJVEMZO37R4ZP7RIARE\npChassinat II = pLouvre E 25352 || Die Geister...\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nTraditionsÃ¤gyptisch\ntraditionelles MittelÃ¤gyptisch\nSpÃ¤thieratische Buchschrift\n['Peter Dils', 'AltÃ¤gyptisches WÃ¶rterbuch', 'F...\n\n\nYWJMWQT65NDTPL6MPG6TLVJ5MI\n03. pChassinat I = pLouvre E 25351 || Die Gesc...\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nTraditionsÃ¤gyptisch\ntraditionelles MittelÃ¤gyptisch\nSpÃ¤thieratische Buchschrift\n['Peter Dils', 'AltÃ¤gyptisches WÃ¶rterbuch', 'F...\n\n\n4VLZLA44UVGJZN22WIWP774LOQ\nStele Louvre C 284 (\"Bentresch-Stele\") || Stel...\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nTraditionsÃ¤gyptisch\ntraditionelles MittelÃ¤gyptisch\nregulÃ¤re Hieroglyphenschrift\n['Lutz Popko', 'AltÃ¤gyptisches WÃ¶rterbuch', 'J...\n\n\n\n\n209 rows Ã— 9 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\n\nTo convert dataframe to string\nThis is necessary because TfidfVectorizer that we will use to do the tf-idf calculations requires a list of strings as input. Each string is an entire text (document).\nFunction to split the text dataframes according to a column. Used to separate text to lines: * param df: dataframe containing one word in each row. * param column: the column by which to split the dfs, perferably text or line. * return: a list of dataframes split according to the value given to the column parameter.\n\ndef split_df_by_column_value(df, column):\n\n    dfs = []\n    column_values = df[column].unique()\n    for value in column_values:\n        split_df = df[df[column]==value]\n        dfs.append(split_df)\n    return dfs\n\n\nsplit_df_by_column_value(corpus[0].head(), \"line\")\n\n[                         text  line  word                          ref frag  \\\n 0  2FPMTQIP45HI3LODGNIZFDO77I     1     2  IBUBdxQqwvMcu0CovD1Q5OKc7B8  [â€¦]   \n 1  2FPMTQIP45HI3LODGNIZFDO77I     1     3  IBUBd1Mxn1WtEUULim1Z51mT3oc   qd   \n 2  2FPMTQIP45HI3LODGNIZFDO77I     1     4  IBUBd9E0G7Z51UgbrcPup3GC3iY  [â€¦]   \n 3  2FPMTQIP45HI3LODGNIZFDO77I     1     6  IBUBd9Ae0nMTckagsrRI5pGbPxQ  [â€¦]   \n 4  2FPMTQIP45HI3LODGNIZFDO77I     1     7  IBUBd5FjpDQ83E60jslkgkHSx4Y  [â€¢]   \n \n   norm unicode_word lemma_id   cf    pos mask     sense inst reading  \\\n 0  [â€¦]          [â€¦]                                                    \n 1   qd       ğ“ªğ“‚§ğ“Œğ“²ğ“»ğ“‘•   162450  qdd   VERB       schlafen                \n 2  [â€¦]          [â€¦]                                                    \n 3  [â€¦]          [â€¦]                                                    \n 4    â€¢            â€¢             â€¢  PUNCT                               \n \n    break_perc                    unicode  \\\n 0         1.0                      ['â€¦']   \n 1         0.1  ['ğ“ª', 'ğ“‚§', 'ğ“Œ', 'ğ“²', 'ğ“»']   \n 2         1.0                      ['â€¦']   \n 3         1.0                      ['â€¦']   \n 4         1.0                      ['â€¢']   \n \n                                                break  \n 0                                        ['missing']  \n 1  ['complete', 'complete', 'complete', 'complete...  \n 2                                        ['missing']  \n 3                                        ['missing']  \n 4                                        ['missing']  ]\n\n\nFunction to convert the values from the text dataframe to a string of text with or without line breaks and word segmentation. * param df: the text dataframe * param column: the chosen column from the dataframe to construct the text from (preferably unicode_word, cf, or lemma) * param break_perc: a parameter which dictates whether to include broken words depending on the percentage of how broken they are. Compares this value to the break_perc column in the dataframe. Parameter is set to 1 (i.e.Â all words, whether broken or not, are included); can be any float between 0 and 1. * param mask: boolean whether to mask named entities or not; set to True. * return: a string which includes all the words in the texts according to the column chosen. Extra spaces that were between broken words or empty lines are removed.\n\ndef df2str(df, column, break_perc=1, mask=True, segmentation=True):\n\n    # check if column exists in dataframe. If not, return empty text.\n    if column not in df.columns:\n        return (\"\", 0, 0)\n    else:\n        # remove rows that include duplicate values for compound words\n        if column not in [\"norm\", \"cf\", \"sense\", \"pos\"]:\n            df = df.drop_duplicates(\"ref\").copy()\n        # if column entry is empty string, replace with UNK (can happen with normalization or lemmatization)\n        mask_empty = df[column]==\"\"\n        df[column] = df[column].where(~mask_empty, other=\"UNK\")\n        # mask proper nouns\n        if mask and \"pos\" in df.columns:\n            mask_bool = df[\"pos\"].isin([\"PN\", \"RN\", \"DN\", \"GN\", \"MN\", \"SN\", \"n\"])\n            df[column] = df[column].where(~mask_bool, other=df[\"pos\"])\n\n        # change number masking from `n` to `NUM`\n        # !comment out for Egyptian\n        #if mask:\n        #    mask_num = df[column]==\"n\"\n        #    df[column] = df[column].where(~mask_num, other=\"NUM\")\n\n        # remove rows without break_perc (happens with non-Akkadian words)\n        if \"\" in df[\"break_perc\"].unique():\n            df = df[df[\"break_perc\"]!=\"\"].copy()\n        # filter according to break_perc\n        mask_break = df[\"break_perc\"] &lt;= break_perc\n        df[column] = df[column].where(mask_break, other=\"X\")\n        # calculate text length with and without UNK and x tokens\n        text_length_full = df.shape[0]\n        mask_partial = df[column].isin([\"UNK\", \"X\", \"x\"])\n        text_length_partial = text_length_full - sum(mask_partial)\n        # create text lines\n        text = \"\"\n        df_lines = split_df_by_column_value(df, \"line\")\n        for line in df_lines:\n            word_list = list(filter(None, line[column].to_list()))\n            if word_list != []:\n                text += \" \".join(map(str, word_list)).replace(\"x\", \"X\").strip() + \" \" #+ \"\\n\"\n\n        if segmentation == False:\n            # remove all white spaces (word segmentation and line breaks)\n            text = re.sub(r\"[\\s\\u00A0]+\", \"\", text)\n\n        return (text, text_length_full, text_length_partial)\n\n\ndf2str(corpus[0], \"lemma_id\")\n\n('UNK 162450 UNK UNK UNK 851513 10030 UNK 91909 167210 UNK UNK 78900 38540 UNK 168850 94550 UNK 851513 10090 107529 850836 78870 10030 UNK UNK 162450 UNK 851512 10030 60920 78873 10120 91970 UNK 851513 UNK UNK 851513 UNK UNK 131460 78873 10030 64362 162650 UNK 851513 10090 107529 185810 400055 10030 UNK 30730 172720 500027 28550 10130 91902 UNK UNK 500024 10090 107529 185810 400055 10030 UNK 58770 UNK 28550 107529 150110 UNK 851513 10050 107529 60030 137250 91901 33120 UNK 851513 10030 UNK UNK 851513 10030 107529 94180 91903 34860 47271 UNK 851513 58770 119620 92500 10050 UNK UNK UNK 10090 UNK ',\n 105,\n 73)\n\n\n\n\nTo convert to specific word levels and create dictionaries\nFunction to convert the dataframes into strings of lemmatized texts. * param corpus: a list of dataframes * param break_perc: a parameter which dictates whether to include broken words depending on the percentage of how broken they are. Compares this value to the break_perc column in the dataframe. Parameter is set to 1 (i.e.Â all words, whether broken or not, are included); can be any float between 0 and 1. * param mask: boolean whether to mask named entities or not; set to True. * return: a dictionary where the keys are the text IDs and the values are the lemmatized texts\n\ndef get_lemmatized_texts(corpus, break_perc=1, mask=True):\n\n    texts_dict = {}\n    for df in corpus:\n        # get the text number from the dataframe \"text\" column\n        key = df[\"text\"].iloc[0]\n        text, text_length_full, text_length_partial = df2str(df, \"lemma_id\", break_perc, mask)\n        texts_dict[key] = (text, text_length_full, text_length_partial)\n    return texts_dict\n\n\nget_lemmatized_texts((split_df_by_column_value(corpus[0], \"text\")))\n\n{'2FPMTQIP45HI3LODGNIZFDO77I': ('UNK 162450 UNK UNK UNK 851513 10030 UNK 91909 167210 UNK UNK 78900 38540 UNK 168850 94550 UNK 851513 10090 107529 850836 78870 10030 UNK UNK 162450 UNK 851512 10030 60920 78873 10120 91970 UNK 851513 UNK UNK 851513 UNK UNK 131460 78873 10030 64362 162650 UNK 851513 10090 107529 185810 400055 10030 UNK 30730 172720 500027 28550 10130 91902 UNK UNK 500024 10090 107529 185810 400055 10030 UNK 58770 UNK 28550 107529 150110 UNK 851513 10050 107529 60030 137250 91901 33120 UNK 851513 10030 UNK UNK 851513 10030 107529 94180 91903 34860 47271 UNK 851513 58770 119620 92500 10050 UNK UNK UNK 10090 UNK ',\n  105,\n  73)}\n\n\nFunction to convert the dataframes into strings of normalized texts. * param corpus: a list of dataframes * param break_perc: a parameter which dictates whether to include broken words depending on the percentage of how broken they are. Compares this value to the break_perc column in the dataframe. Parameter is set to 1 (i.e.Â all words, whether broken or not, are included); can be any float between 0 and 1. * param mask: boolean whether to mask named entities or not; set to True. * return: a dictionary where the keys are the text IDs and the values are the normalized texts\n\ndef get_normalized_texts(corpus, break_perc=1, mask=True):\n\n    texts_dict = {}\n    for df in corpus:\n        # get the text number from the dataframe \"text\" column\n        key = df[\"text\"].iloc[0]\n        text, text_length_full, text_length_partial = df2str(df, \"norm\", break_perc, mask)\n        texts_dict[key] = (text, text_length_full, text_length_partial)\n    return texts_dict\n\n\nget_normalized_texts((split_df_by_column_value(corpus[0], \"text\")))\n\n{'2FPMTQIP45HI3LODGNIZFDO77I': ('[â€¦] qd [â€¦] [â€¦] â€¢ ê½w â¸—ê½ [â€¦] r gmiÌ¯ [â€¦] [â€¦] n êœ¥ná¸« [â€¦] têœ£ rmá¹¯.t â€¢ ê½w â¸—s á¸¥r __ n â¸—ê½ [â€¦] [â€¦] qd â€¢ ê½w â¸—ê½ priÌ¯ n â¸—t r-bw~n~r â€¢ ê½w [â€¦] â€¢ ê½w [â€¦] [â€¦] sbê½.t n â¸—ê½ m qd â€¢ ê½w â¸—s á¸¥r á¸d n â¸—ê½ â€¢ ê½á¸« trê½ ptê½ ê½rr â¸—tn r [â€¦] â€¢ ê½ â¸—s á¸¥r á¸d n â¸—ê½ [â€¦] pêœ£ [â€¦] ê½riÌ¯.t á¸¥r sd.t â€¢ ê½w â¸—f á¸¥r pnw snf r ê½wtn â€¢ ê½w â¸—ê½ [â€¦] â€¢ ê½w â¸—ê½ á¸¥r rmiÌ¯ r êœ¥êœ£.t wr.t â€¢ ê½w pêœ£ á¸«r r-á¸r â¸—f [â€¦] [â€¦] __ â¸—s [â€¦] ',\n  105,\n  105)}\n\n\nFunction to convert the dataframes into strings of segmented unicode texts. * param corpus: a list of dataframes * param break_perc: a parameter which dictates whether to include broken words depending on the percentage of how broken they are. Compares this value to the break_perc column in the dataframe. Parameter is set to 1 (i.e.Â all words, whether broken or not, are included); can be any float between 0 and 1. * param mask: boolean whether to mask named entities or not; set to True. * return: a dictionary where the keys are the text IDs and the values are the segmented unicode texts\n\ndef get_segmented_unicode_texts(corpus, break_perc=1, mask=True):\n\n    texts_dict = {}\n    for df in corpus:\n        # get the text number from the dataframe \"text\" column\n        key = df[\"text\"].iloc[0]\n        text, text_length_full, text_length_partial = df2str(df, \"unicode_word\", break_perc, mask)\n        texts_dict[key] = (text, text_length_full, text_length_partial)\n    return texts_dict\n\n\nget_segmented_unicode_texts((split_df_by_column_value(corpus[0], \"text\")))\n\n{'2FPMTQIP45HI3LODGNIZFDO77I': ('[â€¦] ğ“ªğ“‚§ğ“Œğ“²ğ“»\\U00013455 [â€¦] [â€¦] â€¢ ğ“‡‹ğ“² ğ“€€ [â€¦] ğ“‚‹ ğ“… ğ“…“[â€¦] [â€¦] [â€¦] ğ“ˆ– ğ“‹¹ğ“ˆ–ğ“ [â€¦] ğ“ğ“„¿ ğ“‚‹ğ“€ğ“€€ğ“ğ“¥ â€¢ ğ“‡‹ğ“² ğ“‹´ ğ“·ğ“¤ [â€¦] ğ“ˆ– ğ“€€ [â€¦] [â€¦] [â€¦]ğ“‚§\\U00013455ğ“Œ\\U00013455ğ“²ğ“» â€¢ ğ“‡‹ğ“² ğ“€€ ğ“‰ğ“‚‹ğ“‚» ğ“ˆ– ğ“€€ ğ“‚‹ğ“ƒ€ğ“²ğ“ˆ–ğ“¥ğ“‚‹ğ“ˆğ“‚» â€¢ ğ“‡‹ğ“² [â€¦] â€¢ ğ“‡‹ğ“² [â€¦] [â€¦] [â€¦]ğ“ƒ€ğ“‡‹ğ“ğ“‚» ğ“ˆ– ğ“€€ ğ“…“ ğ“ªğ“‚§ğ“Œğ“²ğ“» â€¢ ğ“‡‹ğ“² ğ“‹´ ğ“·ğ“¤ ğ“†“ğ“‚§ ğ“ˆ– ğ“€€ â€¢ ğ“‡‹ğ“ğ“› ğ“ğ“‚‹ğ“‡‹ğ“†µğ“› ğ“Šªğ“ğ“‡‹ğ“€ ğ“¹ğ“‚‹ ğ“ğ“ˆ–ğ“¥ ğ“‚‹ [â€¦] â€¢ ğ“‡‹ğ“€ ğ“‹´ ğ“·ğ“¤ ğ“†“ğ“‚§ ğ“ˆ– ğ“€€ [â€¦] ğ“…¯[â€¦] [â€¦] ğ“¹ğ“‚‹ğ“ ğ“·ğ“¤ ğ“‹´ğ“‚§ğ“ğ“´ğ“¥ â€¢ ğ“‡‹ğ“² ğ“†‘ UNK ğ“Šªğ“ˆ–ğ“Œğ“²ğ“´ğ“›ğ“« ğ“Šƒğ“ˆ–ğ“†‘ğ“‚ğ“¥ ğ“‚‹ ğ“ƒ›ğ“²ğ“ğ“ˆ–[â€¦]ğ“ˆ‡ğ“¤ â€¢ ğ“‡‹ğ“² ğ“€€ [â€¦] â€¢ ğ“‡‹ğ“² ğ“€€ ğ“·ğ“¤ ğ“‚‹ğ“…“ğ“²ğ“¿ ğ“‚‹ ğ“‰»ğ“ğ“› ğ“…¨ğ“‚‹ğ“ â€¢ ğ“‡‹ğ“² ğ“…¯ğ“„¿ ğ“ğ“‚‹ğ“ğ“‰ğ“¤ ğ“‚‹ğ“‡¥ğ“‚‹\\U00013455 [â€¦] [â€¦] [â€¦] [â€¦]ğ“»\\U00013455[â€¦]ğ“«\\U00013455 ğ“‹´\\U00013455 [â€¦] ',\n  105,\n  104)}\n\n\n\n\nTo create the vector space model\n\nVectorizing texts with TfidfVectorizer\nğŸ”§ What Does TfidfVectorizer Do?\nTfidfVectorizer is a class that:\n\nReads text data\nCleans and tokenizes it\nBuilds a vocabulary\nCalculates TF-IDF values\nReturns a matrix (lokks similar to a Pandas dataframe but isnâ€™t a dataframe) where each row is a document and each column is a term\n\nConverts a list of texts into a term-document matrix based on TF-IDF scores.\nFull documentation of the variables of TfidfVectorizer from sklearn, see: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer * param corpus: a dataframe in which the texts are in a \"text\" column and the dataframeâ€™s index is the text ids. * param analyzer: whether the feature should be made of word or character n-grams. use \"word\" for word features, \"char_wb\" for character n-grams within word boundaries, or \"char\" for character n-grams without word boundaries. * param ngram_range: the lower and upper boundary of the range of n-values for different n-grams to be extracted. * param max_df: threshold to ignore terms that have a document frequency above a certain value. If the threshold is a float, it represent a proportion of the documents. If the threshold is an integer, it represents absolute counts of number of documents in which the terms appears. * param min_df: threshold to ignore terms that have a document frequency below a certain value. If the threshold is a float, it represent a proportion of the documents. If the threshold is an integer, it represents absolute counts of number of documents in which the terms appears. * param max_features: if not None, build a vocabulary that only considers the top max_features ordered by term frequency across the corpus. * param stop_words: if None, no stop words are used. Otherwise, can be a list with words to be removed from resulting tokens. * return: counts the raw counts of the vectorizer, counts_df a dataframe of the counts where the index is the text ids and the columns are the tokens, stop_words an updated list of stop words\n\nFigure 1. Example of a document-term matrix extracted from a corpus, see Fig. 3 in Karsdorp, F., Kestemont, M., & Riddell, A. (2021). Humanities Data Analysis: Case Studies with Python. Princeton University Press.\n\ndef vectorize(corpus, analyzer=\"word\", ngram_range=(1,1), max_df=1.0, min_df=1, max_features=None, stop_words=[\"UNK\", \"X\"]):\n\n    vectorizer = TfidfVectorizer(input=\"content\", lowercase=False, analyzer=analyzer,\n                                 # RegEx for Akkadian\n                                 #token_pattern=r\"(?u)\\b\\w+\\b\", ngram_range=ngram_range,\n                                 # RegEx for Egyptian\n                                 token_pattern=r\"(?u)\\b[\\w\\.]+\\b\", ngram_range=ngram_range,\n                                 max_df=max_df, min_df=min_df, max_features=max_features, stop_words=stop_words)\n\n    counts = vectorizer.fit_transform(corpus[\"text\"].tolist()).toarray()\n    #stop_words = vectorizer.stop_words_ # use when stop_words are not defined in the parameters\n\n    # saving the vocab used for vectorization, and switching the dictionary so that the feature index is the key\n    vocab = vectorizer.vocabulary_\n    switched_vocab = {value: key for key, value in vocab.items()}\n    # adding the vocab words to the counts dataframe for easier viewing.\n    column_names = []\n    x = 0\n    while x &lt; len(switched_vocab):\n        column_names.append(switched_vocab[x])\n        x += 1\n\n    counts_df = pd.DataFrame(counts, index=corpus.index, columns=column_names)\n\n    return (counts, counts_df, stop_words)\n\n\n\nCalculating distances between vectorized documents\nConverts a term-document matrix to a text similarity matrix. * param counts: the raw counts from the vectorize function. * param metric: the metric by which to calculate the distances between the texts in the corpus. For one place to look into the different types of matrics see â€œComputing distances between documentsâ€ in Karsdrop, Kestemont, & Riddell 2021 Valid metrics are: â€˜braycurtisâ€™, â€˜canberraâ€™, â€˜chebyshevâ€™, â€˜cityblockâ€™, â€˜correlationâ€™, â€˜cosineâ€™, â€˜diceâ€™, â€˜euclideanâ€™, â€˜hammingâ€™, â€˜jaccardâ€™, â€˜jensenshannonâ€™, â€˜kulczynski1â€™, â€˜mahalanobisâ€™, â€˜matchingâ€™, â€˜minkowskiâ€™, â€˜rogerstanimotoâ€™, â€˜russellraoâ€™, â€˜seuclideanâ€™, â€˜sokalmichenerâ€™, â€˜sokalsneathâ€™, â€˜sqeuclideanâ€™, â€˜yuleâ€™. * param text_ids: list of unique text_ids. * return: a dataframe matrix of distance between texts.\n\ndef distance_calculator(counts, metric, text_ids):\n\n    return pd.DataFrame(squareform(pdist(counts, metric=metric)), index=text_ids, columns=text_ids)\n\n\n\nreducing dimensions with pca or tsne\nReduces multidimensional data into two dimensions using PCA. * param df: dataframe holding the dimensions to reduce. All columns should include numerical values only. The dataframeâ€™s index should hold the unique text ids. * param metadata: the rest of the metadata in the corpus, to help visualize the resulting clusters in meaningful ways. The metadataâ€™s index should hold the unique text ids. * return: a dataframe with the coordinates of the two remaining dimensions on all other columns from the metadata.\n\ndef reduce_dimensions_pca(df, metadata):\n\n    pca = PCA(n_components=2)\n    reduced_data = pca.fit_transform(df)\n    reduced_df = pd.DataFrame(data=reduced_data, index=df.index, columns=[\"component 1\", \"component 2\"])\n    reduced_df_metadata = metadata.join(reduced_df)\n    return reduced_df_metadata\n\nReduces multidimensional data into two dimensions using TSNE.\nSee full documentation of sklearnâ€™s TSNE on: https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html * param df: dataframe holding the dimensions to reduce. All columns should include numerical values only. The dataframeâ€™s index should hold the unique text ids. * param perplexity: perplexity is a measure the weighs the importance of nearby versus distant points when creating a lower-dimension mapping. t-SNE first converts the distances between points into conditional probabilities that represent similarities, using Gaussian probability distributions. The perplexity parameter influences the variance used to compute these probabilities. A higher perplexity leads to a broader Gaussian that considers a larger number of neighbors when assessing similarity. Lower perplexity puts more focus on the local structure and considers fewer neighbors. A good perplexity depends greatly on dataset size and density. The documentation recommends a value between 5 and 50. We recommend to start with the square root of the length of the corpus. * param n_iter: maximum number of iterations for optimization. * param metric: the metric to be used when calculating distances between vectors. Valid metrics are: â€˜braycurtisâ€™, â€˜canberraâ€™, â€˜chebyshevâ€™, â€˜cityblockâ€™, â€˜correlationâ€™, â€˜cosineâ€™, â€˜diceâ€™, â€˜euclideanâ€™, â€˜hammingâ€™, â€˜jaccardâ€™, â€˜jensenshannonâ€™, â€˜kulczynski1â€™, â€˜mahalanobisâ€™, â€˜matchingâ€™, â€˜minkowskiâ€™, â€˜rogerstanimotoâ€™, â€˜russellraoâ€™, â€˜seuclideanâ€™, â€˜sokalmichenerâ€™, â€˜sokalsneathâ€™, â€˜sqeuclideanâ€™, â€˜yuleâ€™. * param metadata: the rest of the metadata in the corpus, to help visualize the resulting clusters in meaningful ways. The metadataâ€™s index should hold the unique text ids. * return: a dataframe with the coordinates of the two remaining dimensions on all other columns from the metadata.\n\ndef reduce_dimensions_tsne(df, perplexity, n_iter, metric, metadata):\n\n    tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=n_iter, metric=metric, init=\"pca\")\n    reduced_data = tsne.fit_transform(df)\n    reduced_df = pd.DataFrame(data=reduced_data, index=df.index, columns=[\"component 1\", \"component 2\"])\n    reduced_df_metadata = metadata.join(reduced_df)\n    return reduced_df_metadata"
  },
  {
    "objectID": "notebooks/04_Vector_Space_Model_Egyptian.html#process-texts-from-dataframes-and-combine-results-with-metadata-dataframe",
    "href": "notebooks/04_Vector_Space_Model_Egyptian.html#process-texts-from-dataframes-and-combine-results-with-metadata-dataframe",
    "title": "Intro",
    "section": "Process texts from dataframes and combine results with metadata dataframe",
    "text": "Process texts from dataframes and combine results with metadata dataframe\n\n# Function to combine processed texts with metadata\n\ndef get_corpus_metadata(texts_dict, metadata):\n  texts_df = pd.DataFrame(texts_dict, index=[\"text\", \"full_length\", \"partial_length\"]).transpose()\n  df = metadata.join(texts_df)\n  return df\n\n\n## vectorize lemma forms\ncorpus_dict = get_lemmatized_texts(corpus, break_perc=0)\n## vectorize normalized forms\n#corpus_dict = get_normalized_texts(corpus, break_perc=0)\n## vectorize Unicode cuneiform\n#corpus_dict = get_segmented_unicode_texts(corpus, break_perc=0)\n\ncorpus_metadata = get_corpus_metadata(corpus_dict, metadata)\n\n## For Akkadian\n## remove texts which have less than n words excluding UNK and X\n#n = 10\n#print(f\"Number of texts before filtering: {corpus_metadata.shape[0]}\")\n#corpus_metadata = corpus_metadata[corpus_metadata[\"partial_length\"]&gt;=n]\n#print(f\"Number of texts after filtering: {corpus_metadata.shape[0]}\")\n\n\n# For Egyptian use this instead, reset the index\nn = 150\nprint(f\"Number of texts before filtering: {corpus_metadata.shape[0]}\")\ncorpus_metadata = corpus_metadata[corpus_metadata[\"partial_length\"]&gt;=n].set_index(\"text_name\")\nprint(f\"Number of texts after filtering: {corpus_metadata.shape[0]}\")\n\nNumber of texts before filtering: 209\nNumber of texts after filtering: 76\n\n\n\ncorpus_metadata\n\n\n    \n\n\n\n\n\n\ncorpus_manual\npath\ndateEarliest\ndateLatest\nlanguage_manual\ntlaTextLangName\ntlaTextScriptName\ncontributors\ntext\nfull_length\npartial_length\n\n\ntext_name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npVandier = pLille 139 || Recto: Meryre und Sisobek\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nFrÃ¼hdemotisch\nFrÃ¼hdemotisch\nSpÃ¤thieratische Buchschrift\n['Lutz Popko', 'AltÃ¤gyptisches WÃ¶rterbuch', 'J...\n28160 600043 550055 X 64365 58770 X 78900 8806...\n3393\n1811\n\n\npBrooklyn 47.218.135 || Brooklyner Weisheitstext\nLehren\nsawlit || Literarische Texte || 3. Weisheitsle...\n\n\nFrÃ¼hdemotisch\nFrÃ¼hdemotisch\nSpÃ¤thieratische Buchschrift\n['Peter Dils', 'AltÃ¤gyptisches WÃ¶rterbuch', 'L...\nX X 851523 116230 X X X 78030 55210 X X X 1672...\n2647\n1857\n\n\nhintere Innenwand || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nMittelÃ¤gyptisch\nfrÃ¼hes und klassisches MittelÃ¤gyptisch\nregulÃ¤re Hieroglyphenschrift\n['Lutz Popko', 'AltÃ¤gyptisches WÃ¶rterbuch', 'J...\n79090 116800 400007 94530 90260 203 69320 4000...\n234\n225\n\n\nKV 17: Grab Sethos' I., vordere rechte Seitenkammer der Sargkammer || Das Buch von der Himmelskuh (oder: Die Vernichtung des Menschengeschlechts)\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nMittelÃ¤gyptisch\nfrÃ¼hes und klassisches MittelÃ¤gyptisch\nregulÃ¤re Hieroglyphenschrift\n['Lutz Popko', 'AltÃ¤gyptisches WÃ¶rterbuch', 'J...\nX X X X X 500388 400151 400006 10050 64360 880...\n2166\n1942\n\n\npWestcar = pBerlin P 3033 || Die ErzÃ¤hlungen des pWestcar\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nMittelÃ¤gyptisch\nfrÃ¼hes und klassisches MittelÃ¤gyptisch\nMittelhieratische Buchschrift\n['Verena Lepper', 'AltÃ¤gyptisches WÃ¶rterbuch',...\nX X X X 79800 X X X X X X X X X X 180600 85081...\n3373\n2675\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n11. pAnastasi VII = pBM EA 10222 || Kol. 1-7: Die Lehre des Cheti 10.1-30.6\nLehren\nsawlit || Literarische Texte || 3. Weisheitsle...\n\n\nMittelÃ¤gyptisch\ntraditionelles MittelÃ¤gyptisch\nNeuhieratische Buchschrift\n['Peter Dils', 'AltÃ¤gyptisches WÃ¶rterbuch', 'B...\nX X X X X X X 27180 UNK 851222 179020 91900 10...\n1283\n644\n\n\n13. tLouvre N 693 || Die Lehre des Cheti 13.4-22.2 und 30.1-6; Nilhymnus 1.1-2.6\nLehren\nsawlit || Literarische Texte || 3. Weisheitsle...\n\n\nMittelÃ¤gyptisch\ntraditionelles MittelÃ¤gyptisch\nNeuhieratische Buchschrift\n['Peter Dils', 'AltÃ¤gyptisches WÃ¶rterbuch', 'B...\n64440 10110 96700 10030 851182 400090 42490 90...\n673\n208\n\n\n14. oBM EA 29550 + oDeM 1546 || Die Lehre des Cheti 14.1-21.2\nLehren\nsawlit || Literarische Texte || 3. Weisheitsle...\n\n\nMittelÃ¤gyptisch\ntraditionelles MittelÃ¤gyptisch\nNeuhieratische Buchschrift\n['Peter Dils', 'AltÃ¤gyptisches WÃ¶rterbuch', 'B...\nX X X UNK 54605 129490 91904 125370 UNK 67370 ...\n393\n248\n\n\n03. pChassinat I = pLouvre E 25351 || Die Geschichte von KÃ¶nig Neferkare und General Sasenet\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nTraditionsÃ¤gyptisch\ntraditionelles MittelÃ¤gyptisch\nSpÃ¤thieratische Buchschrift\n['Peter Dils', 'AltÃ¤gyptisches WÃ¶rterbuch', 'F...\nX X X X X 400043 X X 10050 107529 77540 X X X ...\n331\n254\n\n\nStele Louvre C 284 (\"Bentresch-Stele\") || Stele Louvre C 284 (\"Bentresch-Stele\")\nErzÃ¤hlungen\nsawlit || Literarische Texte || 1. ErzÃ¤hlungen...\n\n\nTraditionsÃ¤gyptisch\ntraditionelles MittelÃ¤gyptisch\nregulÃ¤re Hieroglyphenschrift\n['Lutz Popko', 'AltÃ¤gyptisches WÃ¶rterbuch', 'J...\n850566 90360 400833 850566 90360 400833 860011...\n933\n904\n\n\n\n\n76 rows Ã— 11 columns"
  },
  {
    "objectID": "notebooks/02_Python_Brush_up.html#akkadian-example-1",
    "href": "notebooks/02_Python_Brush_up.html#akkadian-example-1",
    "title": "Brushing up on your Python Skills",
    "section": "Akkadian Example 1:",
    "text": "Akkadian Example 1:\nhttps://cdli.mpiwg-berlin.mpg.de/artifacts/225104\n&P225104 = TIM 10, 134 #atf: use lexical #Nippur 2N-T496; proverb; Alster proverbs @tablet @obverse @column 1 1. dub-sar hu-ru 2. a-ga-asz-gi4-gi4-me!(|ME+ASZ|)-e-ne 3. dub-sar hu-ru 4. a-ga-asz-gi4-gi4-me!(|ME+ASZ|)-e#-ne @reverse @column 1 1. igi-bi 3(disz) 3(asz) 6(disz)\n\nTask 1:\nHow do we turn this raw text into a list of words?\n\nakk1 = \"\"\"&P225104 = TIM 10, 134\n#atf: use lexical\n#Nippur 2N-T496; proverb; Alster proverbs\n@tablet\n@obverse\n@column 1\n1. dub-sar hu-ru\n2. a-ga-asz-gi4-gi4-me!(|ME+ASZ|)-e-ne\n3. dub-sar hu-ru\n4. a-ga-asz-gi4-gi4-me!(|ME+ASZ|)-e#-ne\n@reverse\n@column 1\n1. igi-bi 3(disz) 3(asz) 6(disz)\n\"\"\"\n\n\nakk1\n\n\n# split string to lines of texts\nlines = akk1.split(\"\\n\")\nlines\n\n\n# remove blanks\n\nlines_full = []\nfor line in lines:\n  if line != \"\":\n    lines_full.append(line)\n\nlines_full\n\n\n# keep only lines that begin with a number\n# use regular expressions\n\nimport re\n\ntext_lines = []\nfor line in lines_full:\n  if re.match(\"^\\d\", line) != None:\n    text_lines.append(line)\n\ntext_lines\n\n\n# separate lines into words\n\nwords_appended = []\nwords_extended = []\nfor line in text_lines:\n  temp_words = line.split()\n  words_appended.append(temp_words[1:]) # creates list of lists\n  words_extended.extend(temp_words[1:]) # creates list\n\nprint(words_appended)\nprint(\"-------------------------------\")\nprint(words_extended)\n\n\n# rewrite the code above as a function\n\n\nWhat information did we lose when preprocessing the texts in this way?\n\n\nTask 2:\nCreate a dictionary from the raw texts, of the following format:\n{\"pnum\": ...\n \"textID\": ...\n \"surface\": [{\n  \"surfaceType\": ...\n  \"columns\": [{\n    \"columnNum\": ...\n    \"text\": [{\n      \"lineNum\": ...\n      \"words\": [..., ..., ...]\n    }]\n  }]\n }]}\n\n# separate text into lines\n\nlines = akk1.split(\"\\n\")\n\nlines_full = []\nfor line in lines:\n  if line != \"\":\n    lines_full.append(line)\n\nlines_full\n\n\n# store the pnum and textID in variables\n\ntext_ids = lines_full[0]\npnum, textID = text_ids.split(\"=\")\n\npnum = pnum.strip()[1:]\ntextID = textID.strip()\nprint(pnum)\nprint(textID)\n\n\n# create a dictionary for each surface (simple no regex method)\n# what do you do when you have different types of inscribed object? (e.g. cylinder, prism, bowl, slab, etc.)\n\nvalid_surface_values = [\"@obverse\", \"@reverse\"]\n\nsurface_idx = []\n\nfor index, line in enumerate(lines_full):\n  if line in valid_surface_values: # what is dangerous in this line? if the line of text is not exactly(!) part of surface, no lines will be found\n    surface_idx.append(index)\nprint(surface_idx)\n\n\n# create a dictionary for each surface (complicated with regex method)\n# what do you do when you have different type of inscribed object? (e.g. cylinder, prism, bowl, slab, etc.)\n\nvalid_surface_values = [\"@obverse\", \"@reverse\"]\n\npattern = r\"^(?:\" + \"|\".join([re.escape(value) for value in valid_surface_values]) + \")\" # This is called a list comprehension\n\nsurface_idx = []\n\nfor index, line in enumerate(lines_full): # returns the index for the line and the content of the line\n  if re.match(pattern, line) != None:\n    surface_idx.append(index)\nprint(surface_idx)\n\n\n# same code like in cell above but without list comprehension\n# create a dictionary for each surface (complicated with regex method)\n# what do you do when you have different type of inscribed object? (e.g. cylinder, prism, bowl, slab, etc.)\n\nvalid_surface_values = [\"@obverse\", \"@reverse\"]\n\n#pattern = r\"^(?:\" + \"|\".join([re.escape(value) for value in valid_surface_values]) + \")\" # This is called a list comprehension\n\nescaped_values = []\nfor value in valid_surface_values:\n    escaped_values.append(re.escape(value))\nprint(escaped_values)\n\npattern = r\"^(?:\" + \"|\".join(escaped_values) + \")\"\n    \nsurface_idx = []\n\nfor index, line in enumerate(lines_full): # returns the index for the line and the content of the line\n  if re.match(pattern, line) != None:\n    surface_idx.append(index)\nprint(surface_idx)\n\n\n# use surface indices to create surface dictionaries\n# surfaceType; columnNum; lineNum; words\n# surfaceType extracted using id values of lines\n# columnNum needs first to check whether a column actually exists, then extracted using regex(?)/tokenize on space for any number after the word column\n# lineNum is regex for any line that begins with a number plus any tags attached: how would be best to define line numbers, as integers or as string variables?\n# words extracted from each text line after lineNum using regex and tokenized on spaces\n\nfor index, id in enumerate(surface_idx):\n    surfaceType = lines_full[id].replace('@', '')\n    print(index, id)\n    if index &lt; len(surface_idx) - 1:\n        end_of_surface = surface_idx[index+1]\n    else:\n        end_of_surface = len(lines_full)\n\n    # Extract the text content for the current surface designation\n    surface_content = lines_full[id+1:end_of_surface]\n\n    # Print the surface type and its content\n    print(f\"Surface Type: {surfaceType}\")\n    # print(\"Content:\")\n    # print('\\n'.join(surface_content))\n    print('---')\n\n    # Extract column number, line numbers, and words for each surface content\n    for line in surface_content:\n        columnNum = None\n        lineNum = None\n        words = []\n\n        # Check if the line contains a column number\n        if '@column' in line:\n            parts = line.split()\n            if len(parts) &gt;= 2:\n                try:\n                    columnNum = int(parts[1])\n                except ValueError:\n                    pass\n            print(f\"Column Number: {columnNum}\")\n            print('---')\n            continue  # Skip processing the line with @column\n\n        # Check if the line contains a line number\n        if '.' in line:\n            parts = line.split('.')\n            if len(parts) &gt;= 2:\n                lineNum = parts[0].strip()\n\n        # Tokenize the words in the line\n        if lineNum:\n            words = parts[1].strip().split()\n        else:\n            words = line.strip().split()\n\n        # Print the extracted information for each line\n        print(f\"Line Number: {lineNum}\")\n        print(f\"Words: {words}\")\n        print('---')\n\n\n# Combine the surfaces and metadata into one dictionary\n\noutput = {\n    \"pnum\": pnum,\n    \"textID\": textID,\n    \"surface\": []\n}\n\nfor index, id in enumerate(surface_idx):\n    surfaceType = lines_full[id].replace('@', '')\n    surface = {\n        \"surfaceType\": surfaceType,\n        \"columns\": []\n    }\n\n    if index &lt; len(surface_idx) - 1:\n        end_of_surface = surface_idx[index+1]\n    else:\n        end_of_surface = len(lines_full)\n\n    # Extract the text content for the current surface designation\n    surface_content = lines_full[id+1:end_of_surface]\n\n    # Extract column number, line numbers, and words for each surface content\n    columnNum = None\n    column = {\n        \"columnNum\": None,\n        \"text\": []\n    }\n    for line in surface_content:\n        lineNum = None\n        words = []\n\n        # Check if the line contains a column number\n        if '@column' in line:\n            parts = line.split()\n            if len(parts) &gt;= 2:\n                try:\n                    columnNum = int(parts[1])\n                    column[\"columnNum\"] = columnNum\n                except ValueError:\n                    pass\n            continue  # Skip processing the line with @column\n\n        # Check if the line contains a line number\n        if '.' in line:\n            parts = line.split('.')\n            if len(parts) &gt;= 2:\n                lineNum = parts[0].strip()\n\n        # Tokenize the words in the line\n        if lineNum:\n            words = parts[1].strip().split()\n        else:\n            words = line.strip().split()\n\n        # Add the line information to the column\n        line_info = {\n            \"lineNum\": lineNum,\n            \"words\": words\n        }\n        column[\"text\"].append(line_info)\n\n    # Add the column to the surface\n    surface[\"columns\"].append(column)\n\n    # Add the surface to the output\n    output[\"surface\"].append(surface)\n\n# Print the output in the specified dictionary format\nprint(output)\n\n\n# Save the output dictionary as a JSON file\n\nimport json\nwith open(f\"{pnum}.json\", \"w\") as json_file:\n    json.dump(output, json_file, indent=4)\n\n\n# rewrite the code above into a function\n\nprint(json_file)"
  },
  {
    "objectID": "notebooks/02_Python_Brush_up.html#egyptian-example-1",
    "href": "notebooks/02_Python_Brush_up.html#egyptian-example-1",
    "title": "Brushing up on your Python Skills",
    "section": "Egyptian Example 1:",
    "text": "Egyptian Example 1:\nA sentence from the sarcophagus of the Napatan king Aspelta (c.Â 600-580 BCE), found in his pyramid in Nuri, Sudan (Nu. 8), https://collections.mfa.org/objects/145117\nGet the context of the sentence from the Thesaurus Linguae Aegyptiae: https://thesaurus-linguae-aegyptiae.de/text/27KHHMEP4VHSDH737F2OFLKNSE/sentences\n\n# This Dictionary was created from the original json file\n\neg1 = {'publication_statement': {'credit_citation': 'Doris Topmann, Sentence ID 2CBOF5UQ7JGETCXG2CQKPCWDZM &lt;https://github.com/thesaurus-linguae-aegyptiae/tla-raw-data/blob/v17/sentences/2CBOF5UQ7JGETCXG2CQKPCWDZM.json&gt;, in: Thesaurus Linguae Aegyptiae: Raw Data &lt;https://github.com/thesaurus-linguae-aegyptiae/tla-raw-data&gt;, Corpus issue 17 (31 October 2022), ed. by Tonio Sebastian Richter & Daniel A. Werning on behalf of the Berlin-Brandenburgische Akademie der Wissenschaften and Hans-Werner Fischer-Elfert & Peter Dils on behalf of the SÃ¤chsische Akademie der Wissenschaften zu Leipzig (first published: 22 September 2023)', 'collection_editors': 'Tonio Sebastian Richter & Daniel A. Werning on behalf of the Berlin-Brandenburgische Akademie der Wissenschaften and Hans-Werner Fischer-Elfert & Peter Dils on behalf of the SÃ¤chsische Akademie der Wissenschaften zu Leipzig', 'data_engineers': {'input_software_BTS': ['Christoph Plutte', 'Jakob HÃ¶per'], 'database_curation': ['Simon D. Schweitzer'], 'data_transformation': ['Jakob HÃ¶per', 'R. Dominik BlÃ¶se', 'Daniel A. Werning']}, 'date_published_in_TLA': '2022-10-31', 'rawdata_first_published': '2023-09-22', 'corresponding_TLA_URL': 'https://thesaurus-linguae-aegyptiae.de/sentence/2CBOF5UQ7JGETCXG2CQKPCWDZM', 'license': 'Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) &lt;https://creativecommons.org/licenses/by-sa/4.0/&gt;'}, 'context': {'line': 'III', 'paragraph': None, 'pos': 7, 'textId': '27KHHMEP4VHSDH737F2OFLKNSE', 'textType': 'Text', 'variants': 1}, 'eclass': 'BTSSentence', 'glyphs': {'mdc_compact': None, 'unicode': None}, 'id': '2CBOF5UQ7JGETCXG2CQKPCWDZM', 'relations': {'contains': [{'eclass': 'BTSAnnotation', 'id': 'DYJEAXFKBJAXJPVLJGWREJZJ5M', 'ranges': [{'end': 'OKLGJLCEQFHU7HDRYUTYR352YA', 'start': '22TFIMS2CBBCFFCDSCAIT3HR3Y'}], 'type': 'Ã¤gyptologische Textsegmentierung'}], 'partOf': [{'eclass': 'BTSText', 'id': '27KHHMEP4VHSDH737F2OFLKNSE', 'name': 'Isis (HT 15, HT 14, HT 17)', 'type': 'Text'}]}, 'tokens': [{'annoTypes': ['Ã¤gyptologische Textsegmentierung'], 'flexion': {'btsGloss': '(unspecified)', 'lingGloss': 'PTCL', 'numeric': 3}, 'glyphs': {'mdc_artificially_aligned': True, 'mdc_compact': 'D35:N35', 'mdc_original': 'D35-N35', 'mdc_original_safe': None, 'mdc_tla': 'D35-N35', 'order': [1, 2], 'unicode': 'ğ“‚œğ“ˆ–'}, 'id': '22TFIMS2CBBCFFCDSCAIT3HR3Y', 'label': 'nn', 'lemma': {'POS': {'type': 'particle'}, 'id': '851961'}, 'transcription': {'mdc': 'nn', 'unicode': 'nn'}, 'translations': {'de': ['[Negationspartikel]']}, 'type': 'word'}, {'annoTypes': ['Ã¤gyptologische Textsegmentierung'], 'flexion': {'btsGloss': 'SC.act.ngem.nom.subj_Neg.nn', 'lingGloss': 'V\\\\tam.act', 'numeric': 210020}, 'glyphs': {'mdc_artificially_aligned': False, 'mdc_compact': 'W11-V28-A7', 'mdc_original': 'W11-V28-A7', 'mdc_original_safe': None, 'mdc_tla': 'W11-V28-A7', 'order': [2, 3, 4], 'unicode': 'ğ“¼ğ“›ğ“€‰'}, 'id': 'IOLUGQXLCRGNLMTAPJ65LI7MHU', 'label': 'gá¸¥', 'lemma': {'POS': {'subtype': 'verb_3-lit', 'type': 'verb'}, 'id': '166480'}, 'transcription': {'mdc': 'gH', 'unicode': 'gá¸¥'}, 'translations': {'de': ['matt sein']}, 'type': 'word'}, {'annoTypes': ['Ã¤gyptologische Textsegmentierung'], 'flexion': {'btsGloss': 'Noun.pl.stpr.3sgm', 'lingGloss': 'N.f:pl:stpr', 'numeric': 70154}, 'glyphs': {'mdc_artificially_aligned': True, 'mdc_compact': 'D36:X1*F51B-Z2', 'mdc_original': 'D36-X1-F51B-Z2', 'mdc_original_safe': None, 'mdc_tla': 'D36-X1-F51B-Z2', 'order': [5, 6, 7, 8], 'unicode': 'ğ“‚ğ“ğ“„¹ï¸€\\U00013440ğ“¥'}, 'id': 'GUVBJUGCSVF5VN55PN6RYS4YLI', 'label': 'êœ¥,t.pl', 'lemma': {'POS': {'subtype': 'substantive_fem', 'type': 'substantive'}, 'id': '34550'}, 'transcription': {'mdc': 'a.t.PL', 'unicode': 'êœ¥.t.PL'}, 'translations': {'de': ['Glied; KÃ¶rperteil']}, 'type': 'word'}, {'annoTypes': ['Ã¤gyptologische Textsegmentierung'], 'flexion': {'btsGloss': '(unspecified)', 'lingGloss': '-3sg.m', 'numeric': 3}, 'glyphs': {'mdc_artificially_aligned': False, 'mdc_compact': 'I9', 'mdc_original': 'I9', 'mdc_original_safe': None, 'mdc_tla': 'I9', 'order': [9], 'unicode': 'ğ“†‘'}, 'id': 'GIHCJ27JXVAM7GDUYWGEPKBRB4', 'label': '=f', 'lemma': {'POS': {'subtype': 'personal_pronoun', 'type': 'pronoun'}, 'id': '10050'}, 'transcription': {'mdc': '=f', 'unicode': '=f'}, 'translations': {'de': ['[Suffix Pron. sg.3.m.]']}, 'type': 'word'}, {'annoTypes': ['Ã¤gyptologische Textsegmentierung'], 'flexion': {'btsGloss': '(unspecified)', 'lingGloss': 'dem.f.pl', 'numeric': 3}, 'glyphs': {'mdc_artificially_aligned': True, 'mdc_compact': 'M17-Q3:N35', 'mdc_original': 'M17-Q3-N35', 'mdc_original_safe': None, 'mdc_tla': 'M17-Q3-N35', 'order': [10, 11, 12], 'unicode': 'ğ“‡‹ğ“Šªğ“ˆ–'}, 'id': 'Z6HTGGPBPRDT3OZTZNXRF2GRDA', 'label': 'jpâŒ©tâŒªn', 'lemma': {'POS': {'subtype': 'demonstrative_pronoun', 'type': 'pronoun'}, 'id': '850009'}, 'transcription': {'mdc': 'jpâŒ©tâŒªn', 'unicode': 'jpâŒ©tâŒªn'}, 'translations': {'de': ['diese [Dem.Pron. pl.f.]']}, 'type': 'word'}, {'annoTypes': ['Ã¤gyptologische Textsegmentierung'], 'flexion': {'btsGloss': '(unspecified)', 'lingGloss': 'TITL', 'numeric': 3}, 'glyphs': {'mdc_artificially_aligned': False, 'mdc_compact': 'D4-Q1-A40', 'mdc_original': 'D4-Q1-A40', 'mdc_original_safe': None, 'mdc_tla': 'D4-Q1-A40', 'order': [13, 14, 15], 'unicode': 'ğ“¹ğ“Š¨ğ“€­'}, 'id': 'UCFJWBLRKJG4NJWTWT22WDR2MU', 'label': 'Wsr,w', 'lemma': {'POS': {'subtype': 'title', 'type': 'epitheton_title'}, 'id': '49461'}, 'transcription': {'mdc': 'wsr.w', 'unicode': 'Wsr.w'}, 'translations': {'de': ['Osiris (Totentitel des Verstorbenen)']}, 'type': 'word'}, {'annoTypes': ['Ã¤gyptologische Textsegmentierung'], 'flexion': {'btsGloss': '(unspecified)', 'lingGloss': 'N', 'numeric': 3}, 'glyphs': {'mdc_artificially_aligned': True, 'mdc_compact': 'M23-X1:N35', 'mdc_original': 'M23-X1-N35', 'mdc_original_safe': None, 'mdc_tla': 'M23-X1-N35', 'order': [16, 17, 18], 'unicode': 'ğ“‡“ğ“ğ“ˆ–'}, 'id': 'LI5FJI4ZUJEMPIKS5RQ5HHNBUE', 'label': 'nzw', 'lemma': {'POS': {'type': 'substantive'}, 'id': '88040'}, 'transcription': {'mdc': 'nzw', 'unicode': 'nzw'}, 'translations': {'de': ['KÃ¶nig']}, 'type': 'word'}, {'annoTypes': ['Ã¤gyptologische Textsegmentierung'], 'flexion': {'btsGloss': '(unspecified)', 'lingGloss': 'ROYLN', 'numeric': 3}, 'glyphs': {'mdc_artificially_aligned': True, 'mdc_compact': 'V30:N17-N17', 'mdc_original': 'V30-N17-N17', 'mdc_original_safe': None, 'mdc_tla': 'V30-N17-N17', 'order': [19, 20, 21], 'unicode': 'ğ“Ÿğ“‡¿ğ“‡¿'}, 'id': 'ICADWHGbHkfdokpooG4eCy3Zfe8', 'label': 'nb-Têœ£,du', 'lemma': {'POS': {'subtype': 'epith_king', 'type': 'epitheton_title'}, 'id': '400038'}, 'transcription': {'mdc': 'nb-tA.DU', 'unicode': 'nb-Têœ£.DU'}, 'translations': {'de': ['Herr der Beiden LÃ¤nder (KÃ¶nige)']}, 'type': 'word'}, {'annoTypes': ['Ã¤gyptologische Textsegmentierung'], 'flexion': {'btsGloss': '(unspecified)', 'lingGloss': 'TITL', 'numeric': 3}, 'glyphs': {'mdc_artificially_aligned': True, 'mdc_compact': 'V30:D4-Aa1*X1:Y1', 'mdc_original': 'V30-D4-Aa1-X1-Y1', 'mdc_original_safe': None, 'mdc_tla': 'V30-D4-Aa1-X1-Y1', 'order': [22, 23, 24, 25, 26], 'unicode': 'ğ“Ÿğ“¹ğ“ğ“ğ“›'}, 'id': 'ICADWHT2O1dc30SXuRZUlquIDpM', 'label': 'nb-jr(,t)-(j)á¸«,t', 'lemma': {'POS': {'subtype': 'title', 'type': 'epitheton_title'}, 'id': '400354'}, 'transcription': {'mdc': 'nb-jr(.t)-(j)x.t', 'unicode': 'nb-jr(.t)-(j)á¸«.t'}, 'translations': {'de': ['Herr des Rituals']}, 'type': 'word'}, {'annoTypes': ['Ã¤gyptologische Textsegmentierung'], 'flexion': {'btsGloss': '(unspecified)', 'lingGloss': 'ROYLN', 'numeric': 3}, 'glyphs': {'mdc_artificially_aligned': True, 'mdc_compact': '&lt;-M17-O34:Q3-E23-N17-&gt;', 'mdc_original': '&lt;-M17-O34-Q3-E23-N17-&gt;', 'mdc_original_safe': None, 'mdc_tla': '&lt;-M17-O34-Q3-E23-N17-&gt;', 'order': [18, 19, 20, 21, 22, 23], 'unicode': 'ğ“¹\\U0001343cğ“‡‹ğ“Šƒğ“Šªğ“ƒ­ğ“‡¿\\U0001343dğ“º'}, 'id': 'J3MLYALWVNAMDDG33VZ3RIEEUA', 'label': 'Jsplt', 'lemma': {'POS': {'subtype': 'kings_name', 'type': 'entity_name'}, 'id': '850103'}, 'transcription': {'mdc': 'jsplt', 'unicode': 'Jsplt'}, 'translations': {'de': ['Aspelta']}, 'type': 'word'}, {'annoTypes': ['Ã¤gyptologische Textsegmentierung'], 'flexion': {'btsGloss': '(unspecified)', 'lingGloss': 'N.m:sg', 'numeric': 3}, 'glyphs': {'mdc_artificially_aligned': True, 'mdc_compact': 'U5:D36-P8h', 'mdc_original': 'U5-D36-P8h', 'mdc_original_safe': None, 'mdc_tla': 'U5-D36-P8h', 'order': [25, 26, 27], 'unicode': 'ğ“Œ·ğ“‚ğ“Š¤ï¸‚'}, 'id': 'OKLGJLCEQFHU7HDRYUTYR352YA', 'label': 'mêœ£êœ¥-á¸«rw', 'lemma': {'POS': {'subtype': 'substantive_masc', 'type': 'substantive'}, 'id': '66750'}, 'transcription': {'mdc': 'mAa-xrw', 'unicode': 'mêœ£êœ¥-á¸«rw'}, 'translations': {'de': ['Gerechtfertigter (der selige Tote)']}, 'type': 'word'}], 'transcription': {'mdc': 'nn gH a.t.PL=f jpâŒ©tâŒªn wsr.w nzw nb-tA.DU nb-jr(.t)-(j)x.t jsplt mAa-xrw', 'unicode': 'nn gá¸¥ êœ¥.t.PL=f jpâŒ©tâŒªn Wsr.w nzw nb-Têœ£.DU nb-jr(.t)-(j)á¸«.t Jsplt mêœ£êœ¥-á¸«rw'}, 'translations': {'de': ['Diese seine Glieder werden nicht matt sein, (die des) Osiris KÃ¶nigs, des Herrn der Beiden LÃ¤nder, des Herrn des Rituals, Aspelta, des Gerechtfertigten.']}, 'type': None, 'wordCount': 11, 'editors': {'author': 'Doris Topmann', 'contributors': None, 'created': '2020-12-23 12:24:26', 'type': None, 'updated': '2022-08-29 10:22:01'}}\nprint(eg1)\n\n\n# parse the dictionary (json)\n\nunicodeHiero = []\ntranscription = []\ntranslLemma = []\nposLemma = []\ntokenID = []\n\nfor text_word in eg1[\"tokens\"] :\n    print(text_word[\"glyphs\"][\"unicode\"], text_word[\"transcription\"][\"unicode\"], text_word[\"translations\"][\"de\"][0], text_word[\"lemma\"][\"POS\"][\"type\"], text_word[\"id\"] )\n    tokenID.append(text_word[\"id\"])\n    unicodeHiero.append(text_word[\"glyphs\"][\"unicode\"])\n    translLemma.append(text_word[\"translations\"][\"de\"][0])\n    posLemma.append(text_word[\"lemma\"][\"POS\"][\"type\"])\n    \n    if text_word[\"transcription\"][\"unicode\"][0] == \"=\" : # replace equal sign as it will cause trouble in spreadsheet software like MS Excel\n        transcription.append(text_word[\"transcription\"][\"unicode\"].replace(\"=\", 'â¸—')) # U+2E17\n    else :\n        transcription.append(text_word[\"transcription\"][\"unicode\"])\n        \n    \n\n\n# get the ID of this sentence\n\nsentenceID = eg1[\"id\"]\n\n\n# create a dataframe and fill it\n\nimport pandas as pd\n\ndf_eg = pd.DataFrame({\n    'unicode_hieroglyphs': unicodeHiero,\n    'unicode_transcription': transcription,\n    'lemma_translation': translLemma,\n    'part-of-speech': posLemma,\n    'tokenID' : tokenID\n})\n\ndf_eg\n\n\n# save as *.csv\n\nfileName = \"aspelta_TLA_Sentence_\" + sentenceID + \".csv\"\ndf_eg.to_csv(fileName)"
  },
  {
    "objectID": "notebooks/02_Python_Brush_up.html#akkadian-example-2",
    "href": "notebooks/02_Python_Brush_up.html#akkadian-example-2",
    "title": "Brushing up on your Python Skills",
    "section": "Akkadian Example 2:",
    "text": "Akkadian Example 2:\nconsider the following Akkadian text:\nhttp://www.achemenet.com//fr/item/?/sources-textuelles/textes-par-publication/Strassmaier_Cyrus/1665118\n6 udu-nita2 ina Å¡uII Iden-gi a-Å¡Ãº Å¡Ã¡ Id[\na-na 8 gÃ­n 4-tÃº kÃ¹-babbar i-na kÃ¹-babbar\nÅ¡Ã¡ i-di Ã© [ o o o ] a-na Ã©-babbar-ra\nit-ta-din 5 udu-nita2 Å¡Ã¡ Ika-á¹£ir\na-Å¡Ãº Å¡Ã¡ Iden-mu a-na 7 gÃ­n 4-tÃº\nkÃ¹-babbar Å¡Ã¡ muh-hi dul-lu Imu-mu\nÃº-Å¡Ã¡-hi-su a-na lÃ¬b-bi sÃ¬-na\n1 udu-nita2 a-na 1 gÃ­n 4-tÃº kÃ¹-babbar\nina Å¡uII Idutu-ba-Å¡Ã¡! [\n1 udu-nita2 Å¡Ã¡ IDU-[\na-na 1? gÃ­n [\npap [13 udu-nita2-meÅ¡\niti du6 u4 [o-kam] mu sag nam-lugal-la\nIku-ra-Ã¡Å¡ lugal tin-tirki u kur-kur\nHow would you preprocess this text?\n\n## raw text\n\nakk2 = \"\"\"6 udu-nita&lt;sub&gt;2&lt;/sub&gt; &lt;i&gt;ina&lt;/i&gt; Å¡u&lt;sup&gt;II&lt;/sup&gt; &lt;sup&gt;Id&lt;/sup&gt;en-gi a-&lt;i&gt;Å¡Ãº Å¡Ã¡&lt;/i&gt; &lt;sup&gt;Id&lt;/sup&gt;[\n&lt;i&gt;a-na&lt;/i&gt; 8 gÃ­n 4-&lt;i&gt;tÃº &lt;/i&gt;kÃ¹-babbar&lt;i&gt; i-na&lt;/i&gt; kÃ¹-babbar \n&lt;i&gt;Å¡Ã¡&lt;/i&gt; &lt;i&gt;i-di&lt;/i&gt; Ã© [ o o o ]&lt;i&gt; a-na&lt;/i&gt; Ã©-babbar-ra \n&lt;i&gt;it-ta-din&lt;/i&gt; 5 udu-nita&lt;sub&gt;2&lt;/sub&gt; &lt;i&gt;Å¡Ã¡&lt;/i&gt; &lt;sup&gt;I&lt;/sup&gt;&lt;i&gt;ka-á¹£ir&lt;/i&gt; \na-&lt;i&gt;Å¡Ãº Å¡Ã¡&lt;/i&gt; &lt;sup&gt;Id&lt;/sup&gt;en-mu&lt;i&gt; a-na&lt;/i&gt; 7 gÃ­n 4-&lt;i&gt;tÃº&lt;/i&gt; \nkÃ¹-babbar &lt;i&gt;Å¡Ã¡&lt;/i&gt; &lt;i&gt;muh-hi&lt;/i&gt; &lt;i&gt;dul-lu&lt;/i&gt; &lt;sup&gt;I&lt;/sup&gt;mu-mu \n&lt;i&gt;Ãº-Å¡Ã¡-hi-su a-na&lt;/i&gt; &lt;i&gt;lÃ¬b-bi&lt;/i&gt; sÃ¬-&lt;i&gt;na&lt;/i&gt; \n1 udu-nita&lt;sub&gt;2&lt;/sub&gt;&lt;i&gt; a-na&lt;/i&gt; 1 gÃ­n 4-&lt;i&gt;tÃº &lt;/i&gt;kÃ¹-babbar \n&lt;i&gt;ina&lt;/i&gt; Å¡u&lt;sup&gt;II&lt;/sup&gt; &lt;sup&gt;Id&lt;/sup&gt;utu-ba-&lt;i&gt;Å¡Ã¡&lt;/i&gt;&lt;sup&gt;!&lt;/sup&gt; [\n1 udu-nita&lt;sub&gt;2&lt;/sub&gt; &lt;i&gt;Å¡Ã¡&lt;/i&gt; &lt;sup&gt;I&lt;/sup&gt;DU-[\n&lt;i&gt;a-na&lt;/i&gt; 1&lt;sup&gt;?&lt;/sup&gt; gÃ­n [\npap [13 udu-nita&lt;sub&gt;2&lt;/sub&gt;-meÅ¡\niti du&lt;sub&gt;6&lt;/sub&gt; u&lt;sub&gt;4&lt;/sub&gt; [o-kam] mu sag nam-lugal-la \n&lt;sup&gt;I&lt;/sup&gt;&lt;i&gt;ku-ra-Ã¡Å¡&lt;/i&gt; lugal tin-tir&lt;sup&gt;ki&lt;/sup&gt; &lt;i&gt;u&lt;/i&gt; kur-kur\"\"\"\n\nakk2\n\n\n## Clean the raw text in akk2\n\nakk2 = akk2.replace(\"&lt;sub&gt;\", \"\")\nakk2 = akk2.replace(\"&lt;/sub&gt;\", \"\")\n\n## Harmonize word and sign boundaries\n\n# Shift blank before/after &lt;i&gt;/&lt;/i&gt;\nakk2 = akk2.replace(\" &lt;/i&gt;\", \"&lt;/i&gt; \")\nakk2 = akk2.replace(\"&lt;i&gt; \", \" &lt;i&gt;\")\n#print(akk2)\n\nimport re\n# Add hyphen before &lt;sup&gt; tags if there is no space before the tag\nakk2 = re.sub(r'([^ ])(&lt;sup&gt;)', r'\\1-\\2', akk2)              \n# Add hyphen after &lt;/sup&gt; and &lt;/i&gt; tags if there is no space after the tag\nakk2 = re.sub(r'(&lt;/sup&gt;|&lt;/i&gt;)([^ ])', r'\\1-\\2', akk2)\n\n# from a-&lt;i&gt;Å¡Ãº Å¡Ã¡&lt;/i&gt; to a-&lt;i&gt;Å¡Ãº&lt;/i&gt; &lt;i&gt;Å¡Ã¡&lt;/i&gt;\npattern = r\"(&lt;i&gt;[^&lt;]*)([ -])\"\nwhile True:\n    new_text = re.sub(pattern, r\"\\1&lt;/i&gt;\\2&lt;i&gt;\", akk2)\n    if new_text == akk2:  # End loop if there are no more differences between the existing one and the one created by re substitution\n        break\n    akk2 = new_text\n\n# Replace double hyphens by simple ones\nakk2 = akk2.replace(\"--\", \"-\")\n\nprint(akk2)\n\n\n## Create a dictionary with annotations\n\nakk2_lineList = akk2.split(\"\\n\")\n\nlines_list = []\nline_count = 1\nfor line in akk2_lineList:\n    temp_words = line.split()\n    line_dict = {}\n    \n    line_dict['line_id'] = line_count\n    line_dict['words'] = temp_words\n\n    lines_list.append(line_dict)\n    line_count += 1\n\n#print(lines_list)\n\nfor line in lines_list :\n    subword_list = []\n    word_count = 1\n    \n    for word in line['words'] :\n        subword_dict = {}\n        \n        sign_list = []\n        \n        if '-' in word: # if more than one sign, separated by hyphen\n            temp_signs = word.split('-')\n            sign_list.extend(temp_signs)\n        else : # if only individual sign \n            sign_list.append(word)\n        \n        signs_per_word = []\n        for sign in sign_list :\n            signs_per_word.append(sign)\n        \n        subword_dict['word_id'] = word_count\n        word_count += 1  \n        subword_dict['signs'] = signs_per_word\n        \n        list_sign_func_dict = []\n        for sign in subword_dict['signs'] :\n            sign_func_dict = {}\n            #print(sign)\n            if sign.startswith('&lt;i&gt;') and sign.endswith('&lt;/i&gt;') :\n                sign_func_dict['sign'] = sign[3:-4]\n                sign_func_dict['sign_function'] = 'phonogram'\n               # sign = \n            elif sign.startswith('&lt;sup&gt;') and sign.endswith('&lt;/sup&gt;') :\n                sign_func_dict['sign'] = sign[5:-6]\n                sign_func_dict['sign_function'] = 'classifier'\n            else:\n                sign_func_dict['sign'] = sign\n                sign_func_dict['sign_function'] = 'logogram'\n                \n            list_sign_func_dict.append(sign_func_dict)\n        #print(list_sign_func_dict)\n        \n            \n        subword_dict['signs'] = list_sign_func_dict\n        subword_list.append(subword_dict)\n\n    line['words'] = subword_list\n    \n\n#print(lines_list)    \nfor line in lines_list:\n    print(line)\n    #for words in line:\n    #    print(line['words'])"
  },
  {
    "objectID": "notebooks/02_Python_Brush_up.html#egyptian-example-2",
    "href": "notebooks/02_Python_Brush_up.html#egyptian-example-2",
    "title": "Brushing up on your Python Skills",
    "section": "Egyptian Example 2:",
    "text": "Egyptian Example 2:\nHow to deal with non-Unicode hieroglyphs ( tag + Gardiner number)\n\neg2_csv = \"\"\",text,line,word,ref,frag,norm,unicode_word,unicode,lemma_id,cf,pos,sense\n92,3Z5EM77HJFCOPKZDDZFEMI6KVY,5,7,3Z5EM77HJFCOPKZDDZFEMI6KVY.5.7,gêœ£uÌ¯.w,gêœ£uÌ¯.w,&lt;g&gt;V96&lt;/g&gt;ğ“…±,\"['&lt;', 'g', '&gt;', 'V', '9', '6', '&lt;', '/', 'g', '&gt;', 'ğ“…±']\",166210,gêœ£uÌ¯,VERB,eng sein; entbehren; (jmdn.) Not leiden lassen\n151,4WVXFJZFLNAYHP3Y5O5SLWD7DA,2,2,4WVXFJZFLNAYHP3Y5O5SLWD7DA.2.2,nêœ¥w,nêœ¥w,ğ“ˆ–ğ“‚ğ“…±&lt;g&gt;I14C&lt;/g&gt;ğ“¤,\"['ğ“ˆ–', 'ğ“‚', 'ğ“…±', '&lt;', 'g', '&gt;', 'I', '1', '4', 'C', '&lt;', '/', 'g', '&gt;', 'ğ“¤']\",80510,Nêœ¥w,PROPN,Sich windender (Personifikation der Schlange)\n153,4WVXFJZFLNAYHP3Y5O5SLWD7DA,2,5,4WVXFJZFLNAYHP3Y5O5SLWD7DA.2.5,nêœ¥w,nêœ¥w,ğ“ˆ–ğ“‚ğ“…±&lt;g&gt;I14C&lt;/g&gt;ğ“¤,\"['ğ“ˆ–', 'ğ“‚', 'ğ“…±', '&lt;', 'g', '&gt;', 'I', '1', '4', 'C', '&lt;', '/', 'g', '&gt;', 'ğ“¤']\",80510,Nêœ¥w,PROPN,Sich windender (Personifikation der Schlange)\n200,67HZI45S3REA3LWVZOKJ6QJOIE,14,9,67HZI45S3REA3LWVZOKJ6QJOIE.14.9,nbiÌ¯.n,nbiÌ¯.n,ğ“ˆ–ğ“Ÿğ“ƒ€&lt;g&gt;D107&lt;/g&gt;ğ“ˆ–,\"['ğ“ˆ–', 'ğ“Ÿ', 'ğ“ƒ€', '&lt;', 'g', '&gt;', 'D', '1', '0', '7', '&lt;', '/', 'g', '&gt;', 'ğ“ˆ–']\",82520,nbiÌ¯,VERB,schmelzen; gieÃŸen\n204,67HZI45S3REA3LWVZOKJ6QJOIE,14,13,67HZI45S3REA3LWVZOKJ6QJOIE.14.13,ná¸r.n,ná¸r.n,ğ“ˆ–ğ“‡¦ğ“‚‹&lt;g&gt;U19A&lt;/g&gt;ğ“†±ğ“ˆ–,\"['ğ“ˆ–', 'ğ“‡¦', 'ğ“‚‹', '&lt;', 'g', '&gt;', 'U', '1', '9', 'A', '&lt;', '/', 'g', '&gt;', 'ğ“†±', 'ğ“ˆ–']\",91630,ná¸r,VERB,(Holz) bearbeiten; zimmern\n206,67HZI45S3REA3LWVZOKJ6QJOIE,14,15,67HZI45S3REA3LWVZOKJ6QJOIE.14.15,b(w)n.wDU,bwn.wDU,ğ“ƒ€ğ“ˆ–ğ“Œğ“…±&lt;g&gt;T86&lt;/g&gt;&lt;g&gt;T86&lt;/g&gt;,\"['ğ“ƒ€', 'ğ“ˆ–', 'ğ“Œ', 'ğ“…±', '&lt;', 'g', '&gt;', 'T', '8', '6', '&lt;', '/', 'g', '&gt;', '&lt;', 'g', '&gt;', 'T', '8', '6', '&lt;', '/', 'g', '&gt;']\",55330,bwn,NOUN,Speerspitzen (des Fischspeeres)\n\"\"\"\n\n\nimport pandas as pd\nfrom io import StringIO\n\n# Convert the string into a StringIO object\n# This is only necessary because we presented the csv as a string not as a file that is loaded into the notebook\ncsv_data = StringIO(eg2_csv)\n\n# Read the data into a pandas DataFrame\ndf = pd.read_csv(csv_data)\n\n# Display the DataFrame\ndf\n\n\ndef split_tags(text):\n    parts = []  # List to collect output of the function\n    while '&lt;g&gt;' in text and '&lt;/g&gt;' in text:\n        pre, rest = text.split('&lt;g&gt;', 1)  # splits at the first &lt;g&gt; found\n        tag_content, post = rest.split('&lt;/g&gt;', 1)  # splits the rest at the first &lt;/g&gt; found\n\n        # adds elements before the first &lt;g&gt;&lt;/g&gt; tag to the List\n        parts.extend(pre)\n\n        #  adds element inside the first &lt;g&gt;&lt;/g&gt; tag to the List\n        parts.append(tag_content)\n\n        # text variable is set to remaining text\n        text = post\n\n    # After last tag found, the remainder of the text is split and added to the List\n    parts.extend(text)\n    return parts\n\ndef process_text(text):\n    if pd.isna(text): # deals with NaN\n        return []\n    else:\n        return split_tags(text)\n\n# apply functions to every row of the column 'unicode_word'\ndf['unicode_splitted'] = df['unicode_word'].apply(process_text)\n# delete obsolete column\ndf.drop('unicode', axis=1, inplace=True)\n\ndf\n#df.to_csv(\"EG-TLA-example.csv\")"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Ancient Language Processing (ALP 2026)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe scatter plot at the top of the page shows relative word frequencies across the seven tablets of Standard Babylonian EnÅ«ma eliÅ¡ (Poem of Creation), using t-SNE dimension reduction to represent relative distances between words in 2D vector space (reproduced using Voyant Tools, see Sinclair, S. & G. Rockwell. (2023). ScatterPlot. Voyant Tools. Retrieved March 21, 2023)â†©ï¸"
  },
  {
    "objectID": "notebooks/01_colab_intro.html#lets-get-started",
    "href": "notebooks/01_colab_intro.html#lets-get-started",
    "title": "Intro to Google Colab (and some other tips and tricks)",
    "section": "Letâ€™s get started!",
    "text": "Letâ€™s get started!\nColab makes it easy for me to share notebooks with you, and for all of us to install any required libraries on a single platform. (This is a big reason why weâ€™re using Colab in this class). One caveat about working with notebooks on Colab is that they are shared with you in a read-only format.\nTo get started, your very first step should always be to choose File &gt; Save a copy in Drive from the menu at the top of the page, and make all future edits/modifications on that copy.\nIn general, I recommend naming the file something with your name in it so as to tell it apart from the original.\nNB: If you are an advanced Python user and youâ€™d prefer to use git/GitHub to keep track of your notebooks, I will also maintain a repsitory of class notebooks here."
  },
  {
    "objectID": "notebooks/01_colab_intro.html#and-off-we-go",
    "href": "notebooks/01_colab_intro.html#and-off-we-go",
    "title": "Intro to Google Colab (and some other tips and tricks)",
    "section": "And off we go!",
    "text": "And off we go!\nA notebook consists of a number of â€œcells,â€ stacked on the page from top to bottom. Cells can have text or code in them.\nYou can add a new text cell or a code cell by using the + Code or + Text button at the top of the page.\nWhen you click one or the other of those buttons, it will add a cell right below the cell you have currently selected. If your cell ends up in the wrong place, you can click the up or down arrow over there on the far right of the cell to move it up or down, until you get it to the spot you intended."
  },
  {
    "objectID": "notebooks/01_colab_intro.html#your-first-cell",
    "href": "notebooks/01_colab_intro.html#your-first-cell",
    "title": "Intro to Google Colab (and some other tips and tricks)",
    "section": "Your first cell",
    "text": "Your first cell\nLetâ€™s make a text cell!\nTo make a new text cell below this one, do this: * Click on this cell to select it. * Click the + Text button at the top of the page. * Type some stuff and press Shift-Return (or Shift-Enter on some keyboards).\nColab will â€œrenderâ€ the text and display it on the page in rendered format. You can hit Return or Enter, or double-click the cell, to edit its contents again.\nIn Colab, you can just use the formatting options at the top of the cell to format the text.\nNB: If you are curious as to why the formatting options insert some special characers into the text, it is because Colab formats text using a language called Markdown. If you want to learn more about Markdown, here is a tutorial, although I prefer to just consult a cheat sheet like this one."
  },
  {
    "objectID": "notebooks/01_colab_intro.html#exercise-make-a-text-cell-with-some-formatting-using-markdown",
    "href": "notebooks/01_colab_intro.html#exercise-make-a-text-cell-with-some-formatting-using-markdown",
    "title": "Intro to Google Colab (and some other tips and tricks)",
    "section": "Exercise: Make a text cell with some formatting using Markdown",
    "text": "Exercise: Make a text cell with some formatting using Markdown\nHere is your first exericse: Insert a new cell just below this one, and us the formattinng palate to make some text bold, some text italics, and some:\n\ntext\nin\na\nlist"
  },
  {
    "objectID": "notebooks/01_colab_intro.html#three-helpful-shortcuts",
    "href": "notebooks/01_colab_intro.html#three-helpful-shortcuts",
    "title": "Intro to Google Colab (and some other tips and tricks)",
    "section": "Three helpful shortcuts",
    "text": "Three helpful shortcuts\nInstead of pressing Shift-Enter, you can press Alt-Enter (or Option-Return on a Mac) to render the current cell and create a new cell just below. New cells will by default be Code cells.\nTo change a code cell to a text cell, you can press Control-M M.\nTo change a text cell to a code cell, you can press Control-M Y."
  },
  {
    "objectID": "notebooks/01_colab_intro.html#code-cells",
    "href": "notebooks/01_colab_intro.html#code-cells",
    "title": "Intro to Google Colab (and some other tips and tricks)",
    "section": "Code cells",
    "text": "Code cells\nCode cellsâ€“ which are actually the default type of cell in Colabâ€“ use the same set of commands as text cells, but instead of rendering the text, they run chunks of code.\nLetâ€™s not beat around the bush! Press Shift-Enter again to run your first chunk of code.\n\nprint(\"This is a code cell.\")\nprint(\"\")\nprint(\"Any Python code you type in this cell will be run when you press the 'Run' button\")\nprint(\"up there, or when you press any of the keyboard shortcuts you just learned.\")\nprint(\"\")\nprint(\"If the code evaluates to something, or if it produces output, that output will be\")\nprint(\"shown beneath the cell after you run it.\")\nprint(\"\")\nprint(\"Let's keep on going. Press Shift-Enter to see what this cell produces and keep going\")\nprint(\"with our exercise.\")\n\nSidenote: You just learned how the print function works in Python 3!\nAnd, therefore, an obligatory meme (via @cszhu on Twitter):"
  },
  {
    "objectID": "notebooks/01_colab_intro.html#a-bit-more-on-printing-things",
    "href": "notebooks/01_colab_intro.html#a-bit-more-on-printing-things",
    "title": "Intro to Google Colab (and some other tips and tricks)",
    "section": "A bit more on printing things",
    "text": "A bit more on printing things\nHere are a few more examples of how to print things.\nHere is more about print() than you likely ever wanted to know."
  },
  {
    "objectID": "notebooks/01_colab_intro.html#errors",
    "href": "notebooks/01_colab_intro.html#errors",
    "title": "Intro to Google Colab (and some other tips and tricks)",
    "section": "Errors",
    "text": "Errors\nIf your code generates an error, it will be displayed in addition to any output already produced.\nIMPORTANT: These error messages are incredibly helpful when debugging. If you get an error, read it and see if it helps point you to where the problem is in your code.\nSo letâ€™s make ourselves an error. Weâ€™ll commit the cardinal sin of dividing by zero.\nRun the cell below.\n\nprint(\"Here is some printing.\")\nprint(\"And now here is an error:\")\n\n1 / 0"
  },
  {
    "objectID": "notebooks/01_colab_intro.html#variables",
    "href": "notebooks/01_colab_intro.html#variables",
    "title": "Intro to Google Colab (and some other tips and tricks)",
    "section": "Variables",
    "text": "Variables\nThe major difference between Python and Râ€“ at least in terms of getting startedâ€“ is that in Python, you can only use = to assign variables.\nLetâ€™s assign some variables:\n\nx = 0\ny = .5\nz = True # note another difference b/t R and Python: Boolean (T/F) values are not all caps"
  },
  {
    "objectID": "notebooks/01_colab_intro.html#exercise-write-some-code-to-print-out-the-variables-weve-just-assigned",
    "href": "notebooks/01_colab_intro.html#exercise-write-some-code-to-print-out-the-variables-weve-just-assigned",
    "title": "Intro to Google Colab (and some other tips and tricks)",
    "section": "Exercise: Write some code to print out the variables weâ€™ve just assigned",
    "text": "Exercise: Write some code to print out the variables weâ€™ve just assigned\n\n# your code here"
  },
  {
    "objectID": "notebooks/01_colab_intro.html#another-neat-thing-about-notebooks",
    "href": "notebooks/01_colab_intro.html#another-neat-thing-about-notebooks",
    "title": "Intro to Google Colab (and some other tips and tricks)",
    "section": "Another neat thing about notebooks:",
    "text": "Another neat thing about notebooks:\nAny variables you define or modules you import in one code cell will be available in subsequent code cells.\nSo for instance, if you run the cell below:\n\nimport random  # a useful module that we'll come back to later\nstuff = [\"cheddar\", \"daguerrotype\", \"elephant\", \"flea market\"]\n\nprint(random.choice(stuff)) # choice is a function that-- you guessed it-- makes a random choice\n\nâ€¦ then in subsequent cells you can do this:\n\nprint(random.choice(stuff)) # choice is a function that-- you guessed it-- makes a random choice"
  },
  {
    "objectID": "notebooks/01_colab_intro.html#exercise-write-some-code-and-then-run-it",
    "href": "notebooks/01_colab_intro.html#exercise-write-some-code-and-then-run-it",
    "title": "Intro to Google Colab (and some other tips and tricks)",
    "section": "Exercise: Write some code and then run it",
    "text": "Exercise: Write some code and then run it\nUsing the previous two code cells as a guideâ€¦ - Reassign the â€œstuffâ€ variable with four different words - Use the â€œchoiceâ€ function, as above, to randomly print one of those four words\n\n# your code here\n\nFYI: â€œstuffâ€ is a list, which is a type of value in Python which represents a sequence of values. Itâ€™s a very common and versatile data structure and is often used to represent tabular data (among other information).\nMore on lists in tonightâ€™s homework. But to conclude for the day:"
  },
  {
    "objectID": "notebooks/01_colab_intro.html#keyboard-shortcuts",
    "href": "notebooks/01_colab_intro.html#keyboard-shortcuts",
    "title": "Intro to Google Colab (and some other tips and tricks)",
    "section": "Keyboard shortcuts",
    "text": "Keyboard shortcuts\nIn Colab, most keyboard shortcuts start with Control-M. After that,\n\nZ undoes the previous action (inside a cell)\nH is find and replace\nA inserts a code cell above\nB inserts a code cell below\nD deletes a cell"
  },
  {
    "objectID": "notebooks/01_colab_intro.html#saving-submitting-your-work",
    "href": "notebooks/01_colab_intro.html#saving-submitting-your-work",
    "title": "Intro to Google Colab (and some other tips and tricks)",
    "section": "Saving / submitting your work",
    "text": "Saving / submitting your work\nIn theory, Colab will save your work as you type. If you want to make double sure that your work is saved, Cmd-S at any time will save your notebook. (You can also select File-&gt;Save from the menu up there.\nWhen it comes time to submit a notebook for evaluation, select File &gt; Download as &gt; Download .ipynb. This is the file that you should submit.\nNB: Please do not just submit links to Colab notebooks as the grader will need to ensure that youâ€™ve stopped working and submitted your notebook by a particular date."
  },
  {
    "objectID": "notebooks/01_colab_intro.html#viewing-editing-notebooks-outside-of-colab",
    "href": "notebooks/01_colab_intro.html#viewing-editing-notebooks-outside-of-colab",
    "title": "Intro to Google Colab (and some other tips and tricks)",
    "section": "Viewing / editing notebooks outside of Colab",
    "text": "Viewing / editing notebooks outside of Colab\nYou may have already noticed that just double-clicking on an .ipynb file that youâ€™ve saved to your computer doesnâ€™t open it. The technical explanation for this is that Colab runs as a cloud-based server, and every time you do something to / in a notebook file, it invovles a call to that server to figure out what to actually do.\nSometimes, however, you just want to easily view or share an .ipynb file without a server. In many (but not all) cases, .ipynb files hosted on GitHub will render very nicely. To wit, click here to see how this notebook holds up. But in cases where GitHub fails, or when the file is hosted elsewhere, nbviewer is good to know about. Note that the notebook needs to already exist online for nbviewer to work.\nFor times that you donâ€™t have internet access, or if you ever want to wean yourself off of Google products, you can also open/edit .ipynb files on your computer. To this, I recommend installing Anaconda Navigator, which is free and open source. Once you install that and run it, you will find that a bunch of other software/packages have been installed, which just require that you click to run then. Click on the option for Jupyter Notebook and navigate to your .ipynb file from there.\nThis notebook was created by Lauren Klein in Fall 2019, incorporating materials from Alison Parish and Jinho Choi. It was updated in Fall 2021, Spring 2021, and Fall 2022."
  },
  {
    "objectID": "notebooks/06_PMI_ASahala.html#how-to-use",
    "href": "notebooks/06_PMI_ASahala.html#how-to-use",
    "title": "Mutual Pointwise Information",
    "section": "How to use?",
    "text": "How to use?\n\nCreate text object (text per line, lemmas separated by space)\n\n    text = Text('oracc-akkadian.txt')\n\nCalculate co-occurrencies for the text object\n\n    cooc = Associations(text,\n                 words1=['*'],            # All words to all words\n                 formulaic_measure=Lazy,  # Use CSW\n                 minfreq_b = 1,           # Min freq of b\n                 minfreq_ab = 1,          # Min co-oc freq of a and b\n                 symmetry=True,           # Window symmetry\n                 windowsize=5,            # Window size\n                 factorpower=2)           # k-value\n\nCalculate PMI from co-occurrences from the associations object\n\n    results = cooc.score(PMI2)            # Select association measure\n\nPrint results from\n\n    x.print_scores(results, limit=1000, gephi=True, filename='oracc.pmi')"
  },
  {
    "objectID": "notebooks/06_PMI_ASahala.html#imports-and-constants",
    "href": "notebooks/06_PMI_ASahala.html#imports-and-constants",
    "title": "Mutual Pointwise Information",
    "section": "Imports and constants",
    "text": "Imports and constants\n\n#from dictionary import dct\n\nimport itertools\nimport json\nimport math\nimport re\nimport statistics\nimport sys\nimport time\nfrom urllib.parse import quote\nimport random\nfrom collections import Counter\n\nimport requests\nimport tempfile\n#import os\n\n\n__version__ = \"2024-05-19\"\n\nprint('pmizer.py version %s\\n' % __version__)\n\n\"\"\" Constants \"\"\"\nWINDOW_SCALING = True     # Apply window size penalty to scores\nLOGBASE = 2               # Logarithm base; set to None for ln\nLACUNAE = ['_']           # List of symbols for lacunae or removed words\nLINEBREAK = '&lt;LB&gt;'        # Line break or text boundary\nBUFFER = '&lt;BF&gt;'           # Buffer/padding symbol\nINDENT = 4                # Indentation level for print\nMETASEPARATOR = '|'       # Character for separating multi-dimensional\n                          # metadata for JSON\nWRAPCHARS = ['[', ']']    # Wrap translations/POS-tags between these\n                          # symbols, e.g. ['\"'] for \"string\". Give two\n                          # if beginning and end symbols are different\nDECIMALSEPARATOR = '.'    # Decimal separator for output files\nHIDE_MIN_SCORE = True     # Hide minimum scores in matrices\nVERBOSE = True            # Print more info\n\nIGNORE = [LINEBREAK, BUFFER]\n\npmizer.py version 2024-05-19"
  },
  {
    "objectID": "notebooks/06_PMI_ASahala.html#functions-and-classes",
    "href": "notebooks/06_PMI_ASahala.html#functions-and-classes",
    "title": "Mutual Pointwise Information",
    "section": "Functions and classes",
    "text": "Functions and classes\n\nLogarithm base definition\n\n# log-base 2 should be used by default\n\ndef _log(n):\n    if LOGBASE is None:\n        return math.log(n)\n    else:\n        return math.log(n, LOGBASE)\n\n\ndef make_korp_oracc_url(w1, w2, wz):\n    \"\"\" Generate URL for Oracc in Korp \"\"\"\n    w1 = re.sub('(.+)_.+?', r'\\1', w1)\n    w2 = re.sub('(.+)_.+?', r'\\1', w2)\n\n    w1 = w1.split('[')[0]\n    w2 = w2.split('[')[0]\n\n    base = 'https://www.kielipankki.fi/korp/?mode=other_languages#'\\\n           '?lang=en&stats_reduce=word'\n    cqp = '&cqp=%5Blemma%20%3D%20%22{w1}%22%5D%20%5B%5D%7B0,'\\\n          '{wz}%7D%20%5Blemma%20%3D%20%22{w2}%22%5D'\\\n          .format(w1=quote(w1), w2=quote(w2), wz=wz)\n    corps = '&corpus=oracc_adsd,oracc_ario,oracc_blms,oracc_cams,oracc_caspo,oracc_ctij'\\\n            ',oracc_dcclt,oracc_dccmt,oracc_ecut,oracc_etcsri,oracc_hbtin,oracc_obmc,'\\\n            'oracc_riao,oracc_ribo,oracc_rimanum,oracc_rinap,oracc_saao,'\\\n            'oracc_others&search_tab=1&search=cqp&within=paragraph'\n\n    ## TEMPORARY FOR ALP COURSE, REMOVE THE LINE BELOW\n    corps = '&corpus=oracc2021_rinap&search_tab=1&search=cqp&within=paragraph'\n\n    return base+cqp+corps\n\n\n\nInput / Output tools\n\nclass IO:\n\n    \"\"\" Basic file IO-operations and verbose \"\"\"\n\n    def read_file(filename):\n        print(': Reading %s' % filename)\n        try:\n            with open(filename, 'r', encoding='utf-8', errors='ignore') as data:\n                return data.read().splitlines()\n        except FileNotFoundError:\n            IO.errormsg(\"File not found: %s\" % filename)\n            sys.exit(0)\n\n    def write_file(filename, content):\n        with open(filename, 'w', encoding='utf-8') as data:\n            data.write(content)\n        print(': Saved %s' % filename)\n\n    def export_json(filename, content):\n        \"\"\" Save lookup table as JSON \"\"\"\n        with open(filename, 'w', encoding=\"utf-8\") as data:\n            json.dump(content, data)\n        print(': Saved %s' % filename)\n\n    def import_json(filename):\n        \"\"\" Load lookup table from JSON \"\"\"\n        try:\n            print(': Reading %s' % filename)\n            with open(filename, encoding='utf-8') as data:\n                return json.load(data)\n        except FileNotFoundError:\n            IO.errormsg(\"File not found: %s\" % filename)\n            sys.exit(0)\n\n    def show_time(time_, process):\n        if VERBOSE:\n            print(\"%s%s took %0.2f seconds\" \\\n                  % (\" \"*INDENT, process, round(time_, 2)))\n\n    def printmsg(message):\n        if VERBOSE:\n            print(message)\n\n    @staticmethod\n    def errormsg(message):\n        print(\": Error! %s\" % message)\n\n\n\nWord association measures\nMeasures take four arguments:\n\nab = co-oc freq\na = freq of a\nb = freq of b\ncz = corpus size\nfactor = CSW value (this is used if postweight is set)\noo = estimated number of all bigrams in the corpus\n\n\n## POINTWISE MUTUAL INFORMATION BASED MEASURES\n\nclass PMI:\n    \"\"\" Pointwise Mutual Information. The score orientation is\n    -log p(a,b) &gt; 0 &gt; -inf. As in Church & Hanks 1990. \"\"\"\n    minimum = -math.inf\n\n    @staticmethod\n    def raw(ab, a, b, cz, oo=None):\n        return (ab/cz) / ((a/cz)*(b/cz))\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        return factor * (_log(ab*cz) - _log(a*b))\n\nclass PMIDELTA:\n    \"\"\" Smooth PMI (Pantel & Lin 2002). This measure reduces\n    the PMI score more, the rarer the words are, thus reducing\n    the low-frequency bias \"\"\"\n    minimum = -math.inf\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        weight = (ab/(ab+1)) * (min(a,b)/(min(a,b,)+1))\n        return factor * weight * (_log(ab*cz) - _log(a*b))\n\nclass PMICDS:\n    \"\"\" Context distribution smoothed PMI (Levy, Goldberg &\n    Dagan 2015). This measure raises the f(b) to the power of\n    alpha = 0.75 \"\"\"\n    minimum = -math.inf\n\n    ## WORKS ONLY IN MATRIX FACTORIZATION\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        alpha = 0.75\n        return max(factor * _log((cz*ab) / (a*(b**alpha))),0)\n\nclass NPMI:\n    \"\"\" Normalized PMI. The score orientation is  +1 &gt; 0 &gt; -1\n    as in Bouma 2009 \"\"\"\n    minimum = -1.0\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        return factor * (PMI.score(ab, a, b, cz, 1) / -_log(ab/cz))\n\n\nclass PMISIG:\n    \"\"\" Washtell & Markert (2009). \"\"\"\n    minimum = -math.inf\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        pa = a / cz\n        pb = b / cz\n        return factor * (math.sqrt(min(pa, pb))\\\n                         * (PMI.raw(ab, a, b, cz)))\n\nclass SCISIG:\n    \"\"\" Washtell & Markert (2009) The original publication does\n    not tell the score orientation, but it can be shown to be\n    +1 &gt; âˆšp(a,b) &gt; 0 \"\"\"\n    minimum = 0\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        pa = a / cz\n        pb = b / cz\n        pab = ab / cz\n        return factor * (math.sqrt(min(pa, pb))\\\n                         * (pab / ((pa) * math.sqrt(pb))))\n\n\nclass cPMI:\n    \"\"\" Corpus Level Significant PMI as in Damani 2013. According to\n    the original research paper, delta value of 0.9 is recommended \"\"\"\n    minimum = -math.inf\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        delta = 0.9\n        t = math.sqrt(_log(delta) / (-2 * a))\n        return _log(ab / (a * b / cz) + a * t) * factor\n\n\nclass PMI2:\n    \"\"\" PMI^2. Fixes the low-frequency bias of PMI and NPMI by squaring\n    the numerator to compensate multiplication done in the denominnator.\n    Scores are oriented as: 0 &gt; log p(a,b) &gt; -inf. As in Daille 1994 \"\"\"\n    minimum = -math.inf\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        return (PMI.score(ab, a, b, cz, 1) - (-_log(ab/cz))) / factor\n\n\nclass PMI3:\n    \"\"\" PMI^3 (no low-freq bias, favors common bigrams). Although not\n    mentioned in any papers at my disposal, the scores are oriented\n    log p(a,b) &gt; 2 log p(a,b) &gt; -inf. As in Daille 1994\"\"\"\n    minimum = -math.inf\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        return (PMI.score(ab, a, b, cz, 1) - (-(2*_log(ab/cz)))) / factor\n\nclass SPMI:\n    \"\"\" Positive shifted PMI. Works as the regular PMI but discards negative\n    scores: -log p(a,b) &gt; 0 = 0; Shift by 3 \"\"\"\n    minimum = 0\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        return max(3 + PMI.score(ab, a, b, cz, factor), 0)\n\nclass PPMI:\n    \"\"\" Positive PMI. Works as the regular PMI but discards negative\n    scores: -log p(a,b) &gt; 0 = 0 \"\"\"\n    minimum = 0\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        return max(PMI.score(ab, a, b, cz, factor), 0)\n\n\nclass PPMI2:\n    \"\"\" Positive derivative of PMI^2 as in Role & Nadif 2011.\n    Shares exaclty the same properties but the score orientation\n    is on the positive plane: 1 &gt; 2^log p(a,b) &gt; 0 \"\"\"\n    minimum = 0\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        return factor * (2 ** PMI2.score(ab, a, b, cz, 1))\n\nclass PPMI3:\n    \"\"\" Positive derivative of PMI^3. Shares exaclty the same properties\n    but the score orientation is: p(a,b) &gt; p(a,b)^2 &gt; 0\n\n    Not mentioned in Role & Nadif \"\"\"\n\n    minimum = 0\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        return factor * (2 ** PMI3.score(ab, a, b, cz, 1))\n\n\nclass NPMI2:\n    \"\"\" NPMI^2. Removes the low-frequency bias as PMI^2 and has\n    a fixed score orientation as NPMI: 1 &gt; 0 = 0. Take cube root\n    of the result to trim excess decimals. Sahala & Linden (2020) \"\"\"\n\n    minimum = 0\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        ind = 2 ** _log(ab / cz)\n        base_score = 2 ** PMI2.score(ab, a, b, cz, 1) - ind\n        return (max(base_score / (1 - ind), 0) * factor) ** (1/3)\n\n\nclass NPMI3:\n    \"\"\" NPMI^3. Removes the low-frequency bias as PMI^3 and has\n    a fixed score orientation as NPMI: 1 &gt; 0 = 0. Take cube root\n    of the result to trim excess decimals. Sahala & Linden (2020) \"\"\"\n\n    minimum = 0\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        pab = (ab/cz)\n        base_score = 2 ** PMI3.score(ab, a, b, cz, 1) - (pab**2)\n        return (max(base_score / (pab - (pab**2)), 0) * factor) ** (1/3)\n\n\n\nOther measures and statistical tests\n\nclass NormalizedExpectation:\n    \"\"\" Normalized Expectation as in Pecina 2006\n\n       2f(xy) / f(x)f(y) \"\"\"\n\n    minimum = 0\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        return (2*ab*factor) / (a+b)\n\n\nclass tTest:\n    \"\"\" Student's t-test \"\"\"\n\n    minimum = 0\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        ind = a*b/cz\n        return ((ab*factor)-ind) / math.sqrt(factor*ab*(1-(factor*ab/cz)))\n\n\nclass zScore:\n    \"\"\" Z-score \"\"\"\n\n    minimum = 0\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        ind = a*b/cz\n        return ((ab*factor)-ind) / math.sqrt(ind*(1-(ind/cz)))\n\n# Association coefficients, See Pecina 2006\n\ndef c_table(ab, a, b, cz, factor, oo=None):\n    \"\"\" Return contingency table. Note that the total number of\n    co-occurrences in the corpus is based on an estimate, because\n    not all bigrams are calculated for efficiency \"\"\"\n    ab = ab*factor\n    A = ab\n    B = a - ab\n    C = b - ab\n    D = oo - ab\n    return A, B, C, D\n\n\nclass Jaccard:\n\n    minimum = 0\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        A, B, C, D = c_table(ab, a, b, cz, factor, oo)\n        return A / (A+B+C)\n\n\nclass Odds:\n\n    minimum = 0\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        A, B, C, D = c_table(ab, a, b, cz, factor, oo)\n        return (A*D)/(B*C)\n\n\n\nclass Simpson:\n    \"\"\" Note: this has an extreme low-freq bias \"\"\"\n    minimum = 0\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        A, B, C, D = c_table(ab, a, b, cz, factor, oo)\n\n        return A / min(A+B, A+C)\n\n\nclass BraunBlanquet:\n\n    minimum = 0\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        A, B, C, D = c_table(ab, a, b, cz, factor, oo)\n\n        return A / max(A+B, A+C)\n\n\nclass Pearson:\n\n    minimum = 0\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        A, B, C, D = c_table(ab, a, b, cz, factor, oo)\n        u = (A * D) - (B * C)\n        d = math.sqrt((A+B)*(A+C)*(D+B)*(D+C))\n        return u / d\n\n\nclass UnigramSubtuples:\n\n    minimum = 0\n\n    @staticmethod\n    def score(ab, a, b, cz, factor, oo=None):\n        A, B, C, D = c_table(ab, a, b, cz, factor, oo)\n\n        subs = [1/A, 1/B, 1/C, 1/D]\n        return _log((A*D)/(B*C)) - (3.29 * math.sqrt(sum(subs)))\n\n\n\n\nContext Similarity Weighting\nThis is an initial version of CSW as in Sahala & Linden (2020); performs similarly but has a space complexity of O(nÃ—m^2) (n = corpus size, m = window len)\nSahala, Aleksi & Krister LindÃ©n. 2020. Improving Word Association Measures in Repetitive Corpora with Context Similarity Weighting, in: A L N Fred & J Filipe (eds.), Proceedings of the 12th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management, IC3K 2020, Volume 1: KDIR, Budapest, Hungary, November 2-4, 2020. SCITEPRESS Science And Technology Publications, SetÃºbal, 48â€“58, https://doi.org/10.5220/0010106800480058\n\nclass FormulaicMeasures:\n    @staticmethod\n    def compensate(words, collocate, uniqs):\n        \"\"\" Compensate window matrix counts:\n        Â´compensationÂ´ is a number of each word that should be\n        ignored in counting (metasymbols, collocates) \"\"\"\n        compensation = uniqs\n        for symbol in LACUNAE + [BUFFER, LINEBREAK, collocate]:\n            compensation += max(words.count(symbol) - 1, 0)\n        return compensation\n\n\nclass Greedy:\n    \"\"\" Removed temporarily \"\"\"\n    pass\n\n\nclass Lazy:\n    @staticmethod\n    def score(windows, collocate):\n        diffs = []\n        for window in zip(*windows[::-1]):\n            \"\"\" Count the number of unique words in transposed window matrix\n            ie. stack windows and count uniques, see compensate() for more\n            info \"\"\"\n            uniqs = len(set(window))\n            compensated = FormulaicMeasures.compensate(window, collocate, uniqs)\n            diffs.append((len(window) - compensated) / len(window))\n            \"\"\" Uncomment to see probabilities \"\"\"\n            #print('\\t'.join(window), (len(window) - compensated) / len(window))\n        return sum(diffs) / max((len(diffs) - 1),1)\n\n\n\nText container and basic text analysis tools\n\nclass Text(object):\n\n    def __init__(self, filename, ignore=LACUNAE):\n        self.filename = filename\n        self.content = []\n        self.content_uniq = []\n        self.metadata = []\n        self.documents = []\n        self.translations = {}\n        self.ignore = ignore\n\n        self._read(filename)\n\n    def __repr__(self):\n        return self.stats\n\n    @staticmethod\n    def _tokenize(string):\n        return string.strip().split(' ')\n\n    @staticmethod\n    def _count(symbols, line):\n        return len([s for s in line if s in symbols])\n\n    def _read(self, filename):\n\n        st = time.time()\n\n        \"\"\" Read input text from lemmatized raw text file \"\"\"\n        metalength = None\n        self.lacunacount = 0\n        self.maxlen = 0\n        self.linecount = 0\n        self.corpus_size = 0\n\n        for line in IO.read_file(self.filename):\n            if line:\n                self.linecount += 1\n                fields = line.split('\\t')\n                text = fields[-1]\n\n                if metalength is None:\n                    metalength = [len(fields)]\n                elif len(fields) not in metalength:\n                    IO.errormsg('(%s at line %i): Inconsistent '\\\n                          'number of metadata fields.' % (self.filename,\n                                                          self.linecount))\n                    sys.exit(0)\n\n                if len(fields) &gt; 1:\n                    \"\"\" Collect metadata \"\"\"\n                    meta = METASEPARATOR.join(fields[0:-1])\n                    self.metadata.append(meta)\n\n                lemmas = self._tokenize(text)\n                \"\"\" Store documents for TF-IDF \"\"\"\n                self.documents.append(lemmas)\n                self.lacunacount += self._count(self.ignore, lemmas)\n\n                \"\"\" Lacunae are count as lemmas but buffers are not \"\"\"\n                lemmacount = len(lemmas)\n                self.corpus_size += lemmacount\n                if lemmacount &gt; self.maxlen:\n                    self.maxlen = lemmacount\n\n                self.content.extend([LINEBREAK] + [BUFFER] + lemmas)\n\n        \"\"\" Add buffer to the end of the file \"\"\"\n        self.content.extend([BUFFER])\n\n        \"\"\" Make frequency list \"\"\"\n        self.word_freqs = Counter(self.content)\n\n        st2 = time.time() - st\n        IO.show_time(st2, \"Reading\")\n\n        IO.printmsg(self.stats)\n\n    @property\n    def stats(self):\n        \"\"\" Return corpus statistics \"\"\"\n        tab = INDENT * ' '\n        non_words = [BUFFER, LINEBREAK]\n        freqs = sorted([f for f in self.word_freqs.values()])\n        log = [('\\n: Text statistics:'),\n               ('%sLines: %i' % (tab, self.linecount)),\n               ('%sLongest line: %i' % (tab, self.maxlen)),\n               ('%sWord count: %i' % (tab, self.corpus_size)),\n               ('%sWord count (non-lacunae): %i' \\\n                % (tab, self.corpus_size - self.lacunacount)),\n               ('%sLacunae or ignored symbols: %i' % (tab, self.lacunacount)),\n               ('%sUnique words: %i' \\\n                % (tab, len(self.word_freqs.keys()) - len(non_words))),\n               ('%sMedian word frequency: %i' \\\n                % (tab, statistics.median(freqs))),\n               ('%sAverage word frequency: %.2f' \\\n                % (tab, sum(freqs) / len(freqs)))]\n        return '\\n'.join(log) + '\\n'\n\n    @property\n    def metadata_stats(self):\n        \"\"\" Return word and line counts for each metadata group \"\"\"\n        if not self.metadata:\n            return None\n\n        meta = {}\n        tab = ' '*INDENT\n        for index, content in enumerate(self.documents):\n            wordcount = len(content)\n            meta.setdefault(self.metadata[index], {'words': 0, 'lines': 0})\n            meta[self.metadata[index]]['words'] += wordcount\n            meta[self.metadata[index]]['lines'] += 1\n\n        return '\\n'+'\\n'.join([('%s%s\\t%i\\t%i' \\\n                % (tab, k.replace(METASEPARATOR, '\\t'), v['words'], v['lines']))\\\n                          for k, v in sorted(meta.items())]) + '\\n'\n\n    def iterate(self, windowsize=1):\n        \"\"\" Iterate content of the text and extend buffers to match\n        window size \"\"\"\n        for word in self.content:\n            if word == BUFFER:\n                for i in range(0, windowsize):\n                    yield word\n            else:\n                yield word\n\n\n    def tf_idf(self, threshold=0):\n        \"\"\" Returns a TF-IDF based stopword list based on the text.\n        This can be passed to Associations() as a keyword argument\n\n        Â´thresholdÂ´ defines the size of the list, if no argument is\n        give, will return a list relative to corpus size \"\"\"\n\n        print(': Making TF-IDF stopword list')\n\n        st = time.time()\n        tf_idfs = {}\n        words = []\n\n        if threshold == 0:\n            threshold = int(0.000005 * self.corpus_size)\n\n        for document in self.documents:\n            N = len(document)\n\n            for word in set(document):\n                t = document.count(word)\n                tf_idfs.setdefault(word, {'tf': [], 'found_in': 0})\n                tf_idfs[word]['tf'].append(t/N)\n                tf_idfs[word]['found_in'] += 1\n\n        for word, vals in tf_idfs.items():\n            scores = []\n            for tf in vals['tf']:\n                scores.append(tf * math.log(len(self.documents)/vals['found_in'], 10))\n            words.append([sum(scores), word])\n\n        st2 = time.time() - st\n        IO.show_time(st2, \"TF-IDF\")\n        return [x[1] for x in sorted(words, reverse=True)[0:threshold]]\n\n    def read_dict(self):\n        filename = self.filename.split('.')[0] + '.dict'\n        with open(filename, 'r', encoding='utf-8', errors='ignore') as data:\n            for line in data.read().splitlines():\n                key, value = line.split('\\t')\n                self.translations[key] = value\n\n    def uniquify(self, wz):\n        # This feature is not finished\n\n        \"\"\" Produce a version of text that do not need window scaling.\n        Iterate text and disallow words occurring more than once\n        withing a given distance from each other. Replace non-unique\n        words with lacunae \"\"\"\n        print(': Uniquifying windows')\n        st = time.time()\n        count = 0\n        count_non_lacunae = 0\n        removed = 0\n        for word in self.content:\n            \"\"\" Initialize buffer \"\"\"\n            if word == LINEBREAK:\n                buffer = []\n            \"\"\" Keep buffer length \"\"\"\n            if len(buffer) == wz + 1:\n                buffer.pop(0)\n            \"\"\" Replace non-uniques with lacunae \"\"\"\n            if word in buffer and word not in LACUNAE + IGNORE:\n                removed += 1\n                word = LACUNAE[0]\n\n            if word not in IGNORE:\n                count += 1\n                if word not in LACUNAE:\n                    count_non_lacunae += 1\n            buffer.append(word)\n            self.content_uniq.append(word)\n\n        self.corpus_size_uniq = Counter(self.content_uniq)\n\n        st2 = time.time() - st\n        IO.show_time(st2, \"Uniquifying\")\n        print(\"%s--&gt; %i words removed\" \\\n              % (' '*INDENT, removed))\n\n        #for k, v in sorted(Counter(removed).items()):\n        #    print(v, k)\n\n    \"\"\" ================================================================\n    Random sampling tools (for measure evaluation purposes)\n    ================================================================ \"\"\"\n\n    \"\"\" Sample a population of words from frequency list from\n    given frequency range. Â´quantityÂ´ is the population size and\n    Â´freq_rangeÂ´ a list that contains the min and max freq,\n    e.g. [30,50] \"\"\"\n\n    def pick_random(self, quantity, freq_range):\n        sampled = []\n        for k, v in self.word_freqs.items():\n            if freq_range[1] &gt; v &gt; freq_range[0]:\n                sampled.append(k)\n        self.random_sample = random.sample(sampled, quantity)\n        return self.random_sample\n\n    \"\"\" Random samples can be saved and loaded with the\n    following funtions \"\"\"\n\n    def save_random(self, filename):\n        with open(filename, 'w', encoding='utf-8') as data:\n            data.write('\\n'.join(self.random_sample))\n\n    def load_random(self, filename):\n        with open(filename, 'r', encoding='utf-8') as data:\n            self.random_sample = data.read().splitlines()\n            return self.random_sample\n\n\n\nAssociation measure tools\n\nclass Associations:\n\n    def __init__(self, text, **kwargs):\n        if not isinstance(text, Text):\n            IO.errormsg('Association must have Text object as argument.')\n            sys.exit(0)\n\n        self.text = text\n        self.word_freqs = self.text.word_freqs\n        self.corpus_size = self.text.corpus_size\n\n        self.windowsize = 2\n        self.minfreq_b = 1\n        self.minfreq_ab = 1\n\n        self.distances = {}\n        self.WINS = {}\n\n        self.translations = {}\n        if self.text.translations:\n            self.translations = self.text.translations\n\n        self.track_distance = False\n        self.symmetry = False\n        self.track_distance = False\n        self.positive_condition = False\n        self.formulaic_measure = None\n        self.postweight = False\n        self.factorpower = 1\n\n        self.words = {1: [], 2: []}\n        self.regex_words = {1: [], 2: []}\n\n        self.metadata = {}\n\n        self.conditions = {'stopwords': LACUNAE + ['', BUFFER, LINEBREAK],\n                           'stopwords_regex': [],\n                           'conditions': [],\n                           'conditions_regex': []}\n\n        self.set_constraints(**kwargs)\n        self.count_bigrams()\n\n        if self.corpus_size &lt; self.windowsize:\n            IO.errormsg('Window size exceeds corpus size.')\n            sys.exit(1)\n\n    def __repr__(self):\n        \"\"\" Define what is not shown in .log files \"\"\"\n        debug = []\n        tab = max([len(k)+2 for k in self.__dict__.keys()])\n        for k in sorted(self.__dict__.keys()):\n            if k not in ['scored', 'text', 'regex_stopwords', 'metadata',\n                         'regex_words', 'distances', 'anywords',\n                         'anywords1', 'output', 'anywords2', 'bigram_freqs',\n                         'anycondition', 'word_freqs', 'positive_condition',\n                         'minimum', 'WINS', 'documents', 'translations']:\n                v = self.__dict__[k]\n                debug.append('%s%s%s' % (k, ' '*(tab-len(k)+1), str(v)))\n\n        return '\\n'.join(debug) + '\\n' + '-'*20 +\\\n               ' \\npmizer version: ' + __version__\n\n    def set_constraints(self, **kwargs):\n        \"\"\" Set constraints. Separate regular expressions from the\n        string variables, as string comparison is significantly faster\n        than re.match() \"\"\"\n\n        for key, value in kwargs.items():\n            if key in ['stopwords', 'conditions']:\n                for word in value:\n                    if isinstance(word, str):\n                        self.conditions[key].append(word)\n                    else:\n                        self.conditions[key+'_regex'].append(word)\n            elif key in ['words1', 'words2']:\n                index = int(key[-1])\n                for word in value:\n                    if isinstance(word, str):\n                        self.words[index].append(word)\n                    else:\n                        self.regex_words[index].append(word)\n            else:\n                setattr(self, key, value)\n\n        \"\"\" Combine tables for faster comparison \"\"\"\n        self.anywords = any([self.words[1], self.words[2],\n                         self.regex_words[1], self.regex_words[2]])\n        self.anywords1 = any([self.words[1], self.regex_words[1]])\n        self.anywords2 = any([self.words[2], self.regex_words[2]])\n        self.anycondition = any([self.conditions['conditions'],\n                                 self.conditions['conditions_regex']])\n\n    \"\"\" ================================================================\n    Helper funtions ====================================================\n    ================================================================ \"\"\"\n\n    def _trim_float(self, number):\n        if not number:\n            return number\n        elif isinstance(number, int):\n            return number\n        else:\n            return round(number, 3)\n\n    def _get_translation(self, word):\n        \"\"\" Get translation from dictionary \"\"\"\n        try:\n            translation = '%s%s%s' % (WRAPCHARS[0], self.translations[word], WRAPCHARS[-1])\n        except:\n            translation = '%s?%s' % (WRAPCHARS[0], WRAPCHARS[-1])\n        return translation\n\n    def _get_distance(self, bigram):\n        \"\"\" Calculate average distance for bigram's words; if not\n        used, the distance will be equal to window size. \"\"\"\n        if self.track_distance:\n            distance = self._trim_float(sum(self.distances[bigram])\n                                    / len(self.distances[bigram]))\n        else:\n            distance = ''\n        return distance\n\n    def _match_regex(self, words, regexes):\n        \"\"\" Matches a list of regexes to list of words \"\"\"\n        return any([re.match(r, w) for r in regexes for w in words])\n\n    def _meets_anycondition(self, condition, words):\n        \"\"\" Compare words with stopword/conditions list and regexes. \"\"\"\n        if not self.conditions[condition +'_regex']:\n            return any(w in self.conditions[condition] for w in words)\n        else:\n            return self._match_regex(words, self.conditions[condition+'_regex'])\\\n                   or any(w in self.conditions[condition] for w in words)\n\n    def _is_wordofinterest(self, word, index):\n        \"\"\" Compare words with the list of words of interest.\n        Return True if in the list; never accept lacunae or buffers \"\"\"\n\n        if self.words[1] == ['*'] and word not in [LINEBREAK, BUFFER]:\n            return True\n\n        if word in [LINEBREAK, BUFFER]:\n            return False\n\n        if not self.regex_words[index]:\n            return word in self.words[index]\n        else:\n            return self._match_regex([word], self.regex_words[index])\\\n                   or word in self.words[index]\n\n    def _is_valid(self, w1, w2, freq):\n        \"\"\" Validate bigram. Discard stopwords and those which\n        do not match with the word of interest lists \"\"\"\n        if freq &gt;= self.minfreq_ab and self.word_freqs[w2] &gt;= self.minfreq_b:\n            if not self.anywords:\n                return not self._meets_anycondition('stopwords', [w1, w2])\n            elif self.anywords and self.anywords2:\n                return self._is_wordofinterest(w1, 1) and\\\n                       self._is_wordofinterest(w2, 2)\n            else:\n                if self.anywords1:\n                    return self._is_wordofinterest(w1, 1) and\\\n                           not self._meets_anycondition('stopwords', [w2])\n                if self.anywords2:\n                    return self._is_wordofinterest(w2, 2) and\\\n                           not self._meets_anycondition('stopwords', [w1])\n                else:\n                    return False\n        else:\n            return False\n\n    def _has_condition(self, window):\n        \"\"\" Check if conditions are defined. Validate window if true \"\"\"\n        if not self.anycondition:\n            return True\n        else:\n            if self.positive_condition:\n                if self._meets_anycondition('conditions', window):\n                    return True\n                else:\n                    return False\n            elif not self.positive_condition:\n                if not self._meets_anycondition('conditions', window):\n                    return True\n                else:\n                    return False\n            else:\n                print('positive_condition must be True or False')\n                sys.exit(1)\n\n\n    \"\"\" ================================================================\n    Bigram counting ====================================================\n    ================================================================ \"\"\"\n\n    def count_bigrams(self):\n\n        print(': Counting bigrams')\n\n        st = time.time()\n\n        \"\"\" Set has_meta if metadata is available. \"\"\"\n        has_meta = len(self.text.metadata) &gt; 0\n\n        text = list(self.text.iterate())\n\n        def _check_formulaic(bigram, window, index):\n            \"\"\" Store windows only if formulaic_measures are used,\n            otherwise skip this to save memory and time. Remove\n            index of the collocate from the window to preserve only\n            context of the bigram \"\"\"\n            if self.formulaic_measure is not None:\n                #window.pop(index)\n                window[index] = LACUNAE[0]\n                self.WINS.setdefault(bigram, []).append(window)\n            return bigram\n\n        def _gather_meta(bigram, meta):\n            \"\"\" Get bigram distribution by metadata \"\"\"\n            self.metadata.setdefault(bigram, {})\n            self.metadata[bigram].setdefault(meta, 0)\n            self.metadata[bigram][meta] += 1\n\n        def count_bigrams_symmetric():\n            \"\"\" Symmetric window \"\"\"\n            wz = self.windowsize - 1\n            #dupes = []\n            for w in zip(*[text[i:] for i in range(1+wz*2)]):\n                #W = [o for o in w if o not in ('_', '&lt;LB&gt;', '&lt;BF&gt;')]\n                #UNIQ = sorted(list(set(W)))\n                #if UNIQ == sorted(W):\n                #    pass\n                #else:\n                #    dupes.append(W)\n\n                if w[0] == LINEBREAK:\n                    if has_meta:\n                        meta = self.text.metadata.pop(0)\n                if self._is_wordofinterest(w[wz], 1) and \\\n                   self._has_condition(w[0:wz]+w[wz+1:]):\n                    for index, bigram in enumerate(itertools.product([w[wz]],\n                                                    w[0:wz]+w[wz+1:])):\n                        if has_meta:\n                            _gather_meta(bigram, meta)\n                        yield _check_formulaic(bigram,\n                                               list(w[0:wz]+w[wz+1:]),\n                                               index)\n            #for x in dupes:\n            #    print(x)\n\n        def count_bigrams_symmetric_dist():\n            \"\"\" Symmetric window and distance tracking. \"\"\"\n\n            def chain(w1, w2):\n                \"\"\" Return convolution chain of two lists.\n                [a, b], [c, d] -&gt; [a, c, b, d] \"\"\"\n                chain = [' '] * len(w1+w2)\n                chain[::2] = w1\n                chain[1::2] = w2\n                return chain\n\n            wz = self.windowsize - 1\n            for w in zip(*[text[i:] for i in range(1+wz*2)]):\n                left = list(w[0:wz])\n                right = list(w[wz+1:])\n                if w[0] == LINEBREAK:\n                    if has_meta:\n                        meta = self.text.metadata.pop(0)\n                if self._is_wordofinterest(w[wz], 1) and \\\n                   self._has_condition(left+right):\n                    for index, bigram in enumerate(itertools.product([w[wz]],\n                                                    left+right)):\n                        bigram = _check_formulaic(bigram, left+right, index)\n                        context = chain(left[::-1], right)\n                        min_dist = math.floor(context.index(bigram[1])/2) + 1\n                        if has_meta:\n                            _gather_meta(bigram, meta)\n                        self.distances.setdefault(bigram, []).append(min_dist)\n                        yield bigram\n\n        def count_bigrams_forward():\n            \"\"\" Calculate bigrams within each forward-looking window \"\"\"\n            for w in zip(*[text[i:] for i in range(self.windowsize)]):\n                if w[0] == LINEBREAK:\n                    \"\"\" Keep track of lines and their metadata \"\"\"\n                    if has_meta:\n                        meta = self.text.metadata.pop(0)\n                if self._is_wordofinterest(w[0], 1) \\\n                        and self._has_condition(w[1:]):\n                    for index, bigram in enumerate(itertools.product([w[0]],\n                                                    w[1:])):\n                        if has_meta:\n                            \"\"\" If metadata is available, store it \"\"\"\n                            _gather_meta(bigram, meta)\n                        yield _check_formulaic(bigram, list(w[1:]), index)\n\n        def count_bigrams_forward_dist():\n            \"\"\" Calculate bigrams within each forward-looking window,\n            calculate also average distance between words. Distance\n            tracking is not included into count_bigrams_forward()\n            for better efficiency \"\"\"\n            for w in zip(*[text[i:] for i in range(self.windowsize)]):\n                if w[0] == LINEBREAK:\n                    if has_meta:\n                        meta = self.text.metadata.pop(0)\n                if self._is_wordofinterest(w[0], 1) and self._has_condition(w[1:]):\n                    for index, bigram in enumerate(itertools.product([w[0]], w[1:])):\n                        bg = _check_formulaic(bigram, list(w[1:]), index)\n                        if has_meta:\n                            _gather_meta(bg, meta)\n                        d = index + 1\n                        self.distances.setdefault(bg, []).append(d)\n                        yield bg\n\n        \"\"\" Selector for window type and distance tracking \"\"\"\n        # TODO: Slice text into smaller parts to prevent memory errors\n        # when analysing large texts\n\n        if self.symmetry:\n            if self.track_distance:\n                self.bigram_freqs = Counter(count_bigrams_symmetric_dist())\n            else:\n                self.bigram_freqs = Counter(count_bigrams_symmetric())\n        else:\n            if self.track_distance:\n                self.bigram_freqs = Counter(count_bigrams_forward_dist())\n            else:\n                self.bigram_freqs = Counter(count_bigrams_forward())\n\n        st2 = time.time() - st\n        IO.show_time(st2, \"Bigram counting\")\n\n    def score(self, measure):\n\n        \"\"\" Score text by using given measure and return dictionary\n        containing the results \"\"\"\n\n        print(': Scoring bigrams')\n\n        st = time.time()\n\n        def scale(bf):\n            \"\"\" Scale bigram frequency with window size to assure\n            Î£ f(a,b) = Î£ f(a) = Î£ f(b) = N regardless of window size \"\"\"\n            if WINDOW_SCALING:\n                if self.symmetry:\n                    return bf / (self.windowsize - 1) / 2\n                else:\n                    return bf / (self.windowsize - 1)\n            else:\n                return bf\n\n        def apply_weight(ab, a, b, cs, F):\n            # Apply only to joint-probability\n            abf = ab * (F**self.factorpower)\n\n            return abf, a, b, cs\n\n        self.measure = measure.__name__\n\n        \"\"\" Declare container for collocation data \"\"\"\n        scored = {'freqs': {},\n                  'translations': {},\n                  'collocations': {},\n                  'words1': [],\n                  'words2': []}\n        w1list, w2list = [], []\n\n        \"\"\" Score and store bigrams, translations etc. \"\"\"\n        F_MEASURE = self.formulaic_measure\n\n        # DEBUG: Test that Î£ f(a,b) = Î£ f(a) = Î£ f(b) = N holds\n        # when calculating I(Ïƒ+;Ïƒ+)\n\n        _abs = 0\n        _as = sum([v for k, v in self.word_freqs.items() if k not in IGNORE])\n\n        \"\"\" Set scoring function according to weighting type \"\"\"\n        #if self.preweight:\n        #    SCORE = measure.score_pre_csim\n        #else:\n        #    SCORE = measure.score\n\n        \"\"\" EXPERIMENTAL:\n        Estimate number of all bigrams for contingency tables \"\"\"\n        all_bigrams = scale((self.windowsize-1) * self.corpus_size)\n\n        for bigram in self.bigram_freqs.keys():\n            w1, w2 = bigram\n\n            # DEBUG, see above\n            _abs += scale(self.bigram_freqs[bigram])\n\n            if self._is_valid(w1, w2, self.bigram_freqs[bigram]):\n\n                freq_w1 = self.word_freqs[w1]\n                freq_w2 = self.word_freqs[w2]\n                distance = self._get_distance(bigram)\n\n                \"\"\" Apply context similarity measure\"\"\"\n                if self.formulaic_measure is not None:\n                    csim_factor = F_MEASURE.score(self.WINS[bigram], w2)\n                else:\n                    csim_factor = 0\n\n                \"\"\" Smooth for zero-division errors \"\"\"\n                csim_factor = max(1-csim_factor, 00000.1)\n\n                \"\"\" Apply CSW to joint distribution if postweight not selected;\n                otherwise apply CSW on the final score \"\"\"\n                if self.formulaic_measure is not None and not self.postweight:\n                    ab, a, b, cs = apply_weight(self.bigram_freqs[bigram],\n                                                freq_w1,\n                                                freq_w2,\n                                                self.corpus_size,\n                                                csim_factor)\n                    factor = 1\n                else:\n                    ab, a, b, cs = (self.bigram_freqs[bigram],\n                                    freq_w1,\n                                    freq_w2,\n                                    self.corpus_size)\n                    factor = csim_factor\n\n                score = measure.score(scale(ab), a, b, cs, factor, all_bigrams)\n\n                data = {'score': score,\n                        'distance': distance,\n                        'frequency': self.bigram_freqs[bigram],\n                        'similarity': 1-csim_factor,\n                        'metadata': self.metadata.get(bigram, None)}\n                scored['translations'][w1] = self._get_translation(w1)\n                scored['translations'][w2] = self._get_translation(w2)\n                scored['freqs'][w1] = freq_w1\n                scored['freqs'][w2] = freq_w2\n                w1list.append(w1)\n                w2list.append(w2)\n                # TODO: use setdefault instead of force\n                # TODO: rearrange results into printable format on the fly\n                #       instead of nested JSON which is slow to parse\n                try:\n                    scored['collocations'][w1][w2] = data\n                except KeyError:\n                    scored['collocations'][w1] = {}\n                    scored['collocations'][w1][w2] = data\n                finally:\n                    pass\n\n        # DEBUG, see above: print f(a,b) and f(b) = f(a) if calculated for *\n        # print('DEBUG:sanity check:',round(_abs), _as)\n        for k,v in self.bigram_freqs.items():\n            pass#print(k,v)\n\n        \"\"\" Store words of interest for JSON \"\"\"\n        scored['words1'] = list(set(w1list))\n        scored['words2'] = list(set(w2list))\n        scored['minimum'] = measure.minimum\n\n        st2 = time.time() - st\n        IO.show_time(st2, \"Scoring bigrams\")\n\n        return scored\n\n    \"\"\" ================================================================\n    Pretty-printing results ============================================\n    ================================================================ \"\"\"\n\n    def _stringify(self, array):\n        \"\"\" Convert all values in table into strings and\n        localize decimal markers. \"\"\"\n        def _format_decimal(item):\n            if isinstance(item, float):\n                return str(item).replace('.', DECIMALSEPARATOR)\n            else:\n                return str(item)\n        return [_format_decimal(x) for x in array]\n\n    def _sort_by_index(self, table, indices):\n        \"\"\" Sort table by given two indices, i.e. [0, 4] sorts\n        the table by 1st and 5th values \"\"\"\n        i = indices[0]\n        j = indices[-1]\n        return sorted(table, key=lambda item: (item[i], item[j]), reverse=True)\n\n    def print_matrix(self, scores, value='score', filename=None):\n        \"\"\" Arguments: Â´scoresÂ´ dictionary (or JSON) produced by\n        Associations.score(); Â´valueÂ´ must be Â´scoreÂ´, Â´frequencyÂ´ or\n        Â´distanceÂ´; set Â´filenameÂ´ to write output into a file \"\"\"\n\n        print(': Building score matrix')\n        st = time.time()\n\n        output = []\n        heading = [value.upper() + ' W1 --&gt;']\n        heading += ['{}'.format(w + ' ' + scores['translations'][w])\\\n                    for w in scores['words2']]\n        rows = [heading]\n        for w1 in sorted(scores['words1']):\n            row = []\n            for w2 in sorted(scores['words2']):\n                if HIDE_MIN_SCORE:\n                    score = ''\n                else:\n                    score = scores['minimum']\n                if w1 in scores['collocations'].keys():\n                    if w2 in scores['collocations'][w1].keys():\n                        bigram = scores['collocations'][w1][w2]\n                        score = bigram.get(value, None)\n                        if score is None:\n                            IO.errormsg('bad argument \"%s\" for print_matrix().' % value)\n                            sys.exit(0)\n                row.append(self._trim_float(score))\n            if any(row) and w1 not in LACUNAE + [BUFFER, LINEBREAK]:\n                rows.append([w1] + row)\n\n        \"\"\" Rotate to clean empty columns \"\"\"\n        for r in [row for row in zip(*rows) if any(row[1:])]:\n            output.append('\\t'.join([str(x) for x in r]))\n\n        st2 = time.time() - st\n        IO.show_time(st2, \"Building matrix\")\n\n        if filename is not None:\n            IO.write_file(filename, '\\n'.join(output))\n        else:\n            print('\\n'.join(output))\n\n    def print_scores(self, scores, limit=10000, sortby=('word1', 'score'),\n                     gephi=False, filename=None):\n\n        st = time.time()\n        print(': Building score table')\n\n        def merge_dict(dict1, dict2):\n           \"\"\" Merge dictionaries and sum the sign frequencies \"\"\"\n           combined = {**dict1, **dict2}\n           for key, value in combined.items():\n               if key in dict1 and key in dict2:\n                   combined[key] = value + dict1[key]\n           return combined\n\n        def collate_meta(meta):\n            \"\"\" Combine multi-value metadata; e.g. SB|Nippur|Literary\n            will be split into three additional subdictionaries and\n            frequency data is collated. \"\"\"\n\n            # TODO: Aliases\n\n            if meta is None:\n                return meta\n\n            vals = len(list(meta.keys())[0].split('|'))\n            original = {i+1:{} for i in range(0, vals)}\n            original[0] = meta\n            for key, val in meta.items():\n                for slot, k in enumerate(key.split('|')):\n                    original[slot+1] = merge_dict({k: val}, original[slot+1])\n            return original\n\n        header = ['word1',  'word2',  'word1 freq', 'word2 freq',\n                  'bigram freq', 'score', 'distance', 'similarity', 'url']\n        sort_indices = [header.index(x) for x in sortby]\n\n        \"\"\" Set headers \"\"\"\n        if gephi:\n            output = [(';'.join(['source', 'target', 'weight']))]\n        else:\n            output = ['\\t'.join([x for x in header])]\n\n        rows = []\n        for w1 in scores['collocations'].keys():\n            for w2 in scores['collocations'][w1].keys():\n                bigram = scores['collocations'][w1][w2]\n                freqs = scores['freqs']\n                meta = collate_meta(bigram['metadata'])\n                items = {'word1': w1,\n                         'word2': w2,\n                         #'attr1': scores['translations'][w1],\n                         #'attr2': scores['translations'][w2],\n                         'word1 freq': freqs[w1],\n                         'word2 freq': freqs[w2],\n                         'bigram freq': bigram['frequency'],\n                         #'metaa': meta,\n                         'score':\n                             float(self._trim_float(bigram['score'])),\n                         'distance': bigram['distance'],\n                         'similarity': self._trim_float(bigram['similarity']),\n                         'url': make_korp_oracc_url(w1, w2, self.windowsize-2)}\n                rows.append([items[key] for key in header])\n\n        \"\"\" Sort and convert into tsv \"\"\"\n        lastword = ''\n        for line in self._sort_by_index(rows, sort_indices):\n            if not gephi:\n                if lastword != line[0]:\n                    i = 0\n                if i &lt; limit:\n                    output.append('\\t'.join(self._stringify(line)))\n                lastword = line[0]\n                i += 1\n            if gephi:\n                if lastword != line[0]:\n                    i = 0\n                if i &lt; limit:\n                    data = [line[0]+' '+line[1]] + [line[2]+' '+line[3]+' ('+str(line[5]) +')'] + [line[7]]\n                    output.append('\\t'.join(self._stringify(data)))\n                lastword = line[0]\n                i += 1\n\n\n        st2 = time.time() - st\n        IO.show_time(st2, \"Building score table\")\n\n        if filename is None:\n            print('\\n'.join(output))\n        else:\n            IO.write_file(filename, '\\n'.join(output))"
  },
  {
    "objectID": "notebooks/06_PMI_ASahala.html#set-options-and-run-the-script",
    "href": "notebooks/06_PMI_ASahala.html#set-options-and-run-the-script",
    "title": "Mutual Pointwise Information",
    "section": "Set options and run the script",
    "text": "Set options and run the script\nThis is the part of the script that can be safely modified.\nDEFINING KEYWORDS:\n\nwords1=['sisÃ»[horse]N', 'parÃ»[mule]N']\nWould find all collocates for sisÃ» and parÃ». You can also use regular expressions but they must be precompiled, for example.\nwords1=[re.compile('banÃ».+')]\nWill find collocates for banÃ» regarldess of their sense or part-of-speech. If you want to only find banÃ»s that are adjectives, you can do\nwords1=[re.compile('banÃ».+AJ')]\n\nDEFINING CLOSED COLLOCATE SETS\nYou can limit your possible collocates by defining words2. For example, if you only want to find collocates that are divine names, use\n\nwords2=[re.compile('.*DN.*')]\n\nDEFINING STOPWORDS\nStopwords can be used to filter out groups of words from your collocate lists. For example, if you want to filter out all proper names and royal names, you can use the regular expression [RD]N, that is (RN|DN).\n\nstopwords=[re.compile('.*([RD]N).*')]\n\nIn general its faster to define stopwords in inverse by using words2, e.g.\n\nwords2=[re.compile('.*\\](N|AJ|V).*')]\n\ndisallows ALL collocates other than nouns, adjectives and verbs.\nDEFINING CONDITIONS (ADVANCED FEATURE)\nTo define conditions for your collocates, you can use arguments conditions and positive_condition. For example, the following arguments\n\nwords1=['kakku[weapon]N'],\nconditions=[re.compile('.*naparÅ¡udu.*')],\npositive_condition=False,\nwill calculate collocates for kakku[weapon]N ONLY if the word naparÅ¡udu does not exist within the window.\n\nOn the contrary, the following would find collocates for kakku[weapon]N only if naparÅ¡udu exists in the window.\n\nwords1=['kakku[weapon]N'],\n    conditions=[re.compile('.*naparÅ¡udu.*')],\n    positive_condition=True,\n\nAUTO-GENERATING STOPWORD LISTS (ADVANCED FEATURE)\n\nstopwords=z.tf_idf(threshold=1000)\nWill generate a list of 1000 most uninteresting words in the text by using TF-IDF. Note that this is a part of the Text class.\n\n\n\ndef fetch_file_from_github(url):\n    # Fetch the content from URL\n    response = requests.get(url)\n    response.raise_for_status()  # Ensure we catch HTTP errors\n\n    # Create a temporary file to write the content\n    temp_file = tempfile.NamedTemporaryFile(delete=False, mode='w+', encoding='utf-8')\n    temp_file.write(response.text)\n    temp_file.close()  # Close the file to ensure it can be reopened by IO.read_file\n\n    return temp_file.name  # Return the path of the temporary file\n\n\n# URL of the raw file on GitHub\nurl = 'https://raw.githubusercontent.com/asahala/Pmizer/master/data/dataset.txt'\n\ntemp_filename = fetch_file_from_github(url)\n\n\n#z = Text('data/PMI_dataset.txt') # Dataset in TPL (text per line) format\nz = Text(temp_filename)\n\nwz = 5 # Window size\nx = Associations(z,\n                 words1=['maddattu[payment]N'],      # keyword of interest\n                 #conditions=[re.compile('.*naparÅ¡udu.*')],\n                 #positive_condition=False,\n                 formulaic_measure=Lazy,   # use CSW\n                 minfreq_b = 2,            # min collocate freq\n                 minfreq_ab = 2,           # min bigram freq\n                 symmetry=True,            # use symmetric window\n                 windowsize=wz,            # window size\n                 factorpower=3)            # CSW k-value\n\nA = x.score(PMIDELTA)              # Select measure (e.g, PMI, PMI2, Jaccard...)\n\n# Save results\nx.print_scores(A, limit=20, gephi=False, filename='PMI_results.tsv')\n\n: Reading /tmp/tmpidhw9is7\n    Reading took 0.06 seconds\n\n: Text statistics:\n    Lines: 432\n    Longest line: 6147\n    Word count: 88252\n    Word count (non-lacunae): 62469\n    Lacunae or ignored symbols: 25783\n    Unique words: 4149\n    Median word frequency: 4\n    Average word frequency: 21.47\n\n: Counting bigrams\n    Bigram counting took 0.13 seconds\n: Scoring bigrams\n    Scoring bigrams took 0.01 seconds\n: Building score table\n    Building score table took 0.01 seconds\n: Saved PMI_results.tsv"
  },
  {
    "objectID": "notebooks/06_PMI_ASahala.html#critical-bug-fixes",
    "href": "notebooks/06_PMI_ASahala.html#critical-bug-fixes",
    "title": "Mutual Pointwise Information",
    "section": "Critical bug fixes",
    "text": "Critical bug fixes\n    2019-06-26: changed buffering from the end of the line\n                to the beginning to prevent rare crash that\n                occurred if the keyword was coincidentally\n                in the middle of the first symmetric window.\n                now linebreak is always the first symbol in text.\n\n    2020-02-10: lazy context similarity algorithm now preserves\n                lacuna positions. other algorithms are not\n                recommended as they are not fixed yet.\n\n    2020-03-03: regular expressions now work properly in forward-\n                looking windows.\n\n                symmetric window scaling is now correct and\n                Î£ f(a,b) = Î£ f(a) = Î£ f(b) = N when I(Ïƒ+;Ïƒ+)\n                where Ïƒ is symbol of the alphabet.\n\n    2020-05-20: Fix incorrect bounds for PMI3. Lazy context\n                similarity measure now discards collocates\n                from counts properly (i.e. subtracts 1 from\n                the denominator).\n\n    2024-05-19: Fix links to Korp."
  },
  {
    "objectID": "notebooks/06_PMI_ASahala.html#other-fixes",
    "href": "notebooks/06_PMI_ASahala.html#other-fixes",
    "title": "Mutual Pointwise Information",
    "section": "Other fixes",
    "text": "Other fixes\n    2020-11-27: Preweight is now default.\n\n    2020-01-01: Additional measures such as Jaccard etc."
  },
  {
    "objectID": "notebooks/03_VectorizingTexts.html#from-texts-to-vectors-tf-idf",
    "href": "notebooks/03_VectorizingTexts.html#from-texts-to-vectors-tf-idf",
    "title": "Exploring Texts using the Vector Space Model",
    "section": "",
    "text": "When using the vector space model, a corpusâ€”a collection of documents, each represented as a bag of wordsâ€”is typically represented as a matrix, in which each row represents a document from the collection, each column represents a word from the collectionâ€™s vocabulary, and each cell represents the frequency with which a particular word occurs in a document.\nA matrix arranged in this way is often called a document-term matrixâ€”or term-document matrix where: * rows are associated with documents * word counts are in the columns.\n```jifuaxd Example of a vector space representation with four documents (rows) and a vocabulary of four words (columns). For each document the table lists how often each vocabulary item occurs.\n\n\n\n\nroi\nange\nsang\nperdu\n\n\n\n\n\\(d_1\\)\n1\n2\n16\n21\n\n\n\\(d_2\\)\n2\n2\n18\n19\n\n\n\\(d_3\\)\n35\n41\n0\n2\n\n\n\\(d_4\\)\n39\n55\n1\n0\n\n\n\n\nIn this table, each document $d_i$ is represented as a vector, which, essentially, is a list of numbers---word frequencies in our present case. A &lt;span class=\"index\"&gt;vector space&lt;/span&gt; is nothing more than a collection of numerical vectors, which may, for instance, be added together and multiplied by a number. Documents represented in this manner may be compared in terms of their *coordinates* (or *components*). For example, by comparing the four documents on the basis of the second coordinate, we observe that the first two documents ($d_1$ and $d_2$) have similar counts, which might be an indication that these two documents are somehow more similar. To obtain a more accurate and complete picture of document similarity, we would like to be able to compare documents more holistically, using *all* their components. In our example, each document represents a point in a four-dimensional vector space. We might hypothesize that similar documents use similar words, and hence reside close to each other in this space. To illustrate this, we demonstrate how to visualize the documents in space using the first and third components.\n\n![](https://www.humanitiesdataanalysis.org/_images/notebook_2_4.png)\n\n\n* TF-IDF: Concept\n![](https://www.romainberg.com/wp-content/uploads/TF_IDF-final.png)\n\n* TF-IDF: Simple calculation\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wQ-FCEYoioz6tOlYFssXeg.png)\n\n### Preprocessing\n\n::: {#sCB5jEe2jaAp .cell}\n``` {.python .cell-code}\n# raw data\n\nakk05 = ['ana', 'eÅ¡Å¡Å«tu', 'kaÅ¡Äru', 'mÄtu', 'AÅ¡Å¡ur', 'ana', 'UNK', 'Älu', 'epÄ“Å¡u', 'Ä“kallu', 'mÅ«Å¡abu', 'Å¡arrÅ«tu', 'ina', 'libbu', 'nadÃ»', 'UNK', 'Å¡umu', 'nabÃ»', 'kakku', 'AÅ¡Å¡ur', 'bÄ“lu', 'ina', 'libbu', 'ramÃ»', 'niÅ¡u', 'mÄtu', 'kiÅ¡ittu', 'qÄtu', 'ina', 'libbu', 'waÅ¡Äbu', 'biltu', 'maddattu', 'kÃ¢nu', 'itti', 'niÅ¡u', 'mÄtu', 'AÅ¡Å¡ur', 'manÃ»', 'á¹£almu', 'Å¡arrÅ«tu', 'u', 'á¹£almu', 'ilu', 'rabÃ»', 'bÄ“lu', 'epÄ“Å¡u', 'lÄ«tu', 'u', 'danÄnu', 'Å¡a', 'ina', 'zikru', 'AÅ¡Å¡ur', 'bÄ“lu', 'eli', 'mÄtu', 'Å¡akÄnu', 'ina', 'muhhu', 'Å¡aá¹­Äru', 'ina', 'UNK', 'izuzzu', 'UNK', 'UNK', 'biltu', 'hurÄá¹£u', 'ina', 'dannu', 'UNK', 'lÄ«m', 'biltu', 'kaspu', 'UNK', 'maddattu', 'mahÄru', 'ina', 'UNK', 'palÃ»', 'AÅ¡Å¡ur', 'bÄ“lu', 'takÄlu', 'ana', 'Namri', 'UNK', 'Bit-Zatti', 'Bit-Abdadani', 'Bit-Sangibuti', 'UNK', 'alÄku', 'UNK', 'akÄmu', 'gerru', 'amÄru', 'Nikkur', 'Älu', 'dannÅ«tu', 'waÅ¡Äru', 'UNK', 'zanÄnu', 'Nikkurayu', 'kakku', 'UNK', 'sisÃ»', 'parÃ»', 'alpu', 'UNK', 'SassiaÅ¡u', 'TutaÅ¡di', 'UNK']\nakk08 = ['UNK', 'niÅ¡u', 'ana', 'mÄtu', 'AÅ¡Å¡ur', 'warÃ»', 'UNK', 'UNK', 'ina', 'UNK', 'palÃ»', 'AÅ¡Å¡ur', 'bÄ“lu', 'takÄlu', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'Sulumal', 'Meliddayu', 'Tarhu-lara', 'Gurgumayu', 'UNK', 'UNK', 'UNK', 'mÄtitÄn', 'ana', 'emÅ«qu', 'ahÄmiÅ¡', 'takÄlu', 'UNK', 'UNK', 'ina', 'lÄ«tu', 'u', 'danÄnu', 'Å¡a', 'AÅ¡Å¡ur', 'bÄ“lu', 'itti', 'mahÄá¹£u', 'dÄ«ktu', 'dÃ¢ku', 'UNK', 'UNK', 'qurÄdu', 'dÃ¢ku', 'hurru', 'natbÄku', 'Å¡adÃ»', 'malÃ»', 'narkabtu', 'UNK', 'UNK', 'ana', 'lÄ', 'mÄnu', 'leqÃ»', 'ina', 'qablu', 'tidÅ«ku', 'Å¡a', 'IÅ¡tar-duri', 'UNK', 'UNK', 'UNK', 'UNK', 'ina', 'qÄtu', 'á¹£abÄtu', 'UNK', 'lÄ«m', 'UNK', 'lÄ«m', 'UNK', 'meÊ¾atu', 'UNK', 'UNK', 'UNK', 'UNK', 'iÅ¡tu', 'UNK', 'UNK', 'IÅ¡tar-duri', 'ana', 'ezÄ“bu', 'napiÅ¡tu', 'mÅ«Å¡iÅ¡', 'halÄqu', 'lÄma', 'Å¡amÅ¡u', 'urruhiÅ¡', 'napruÅ¡u', 'UNK', 'UNK', 'itti', 'Å¡iltÄhu', 'pÄriÊ¾u', 'napiÅ¡tu', 'adi', 'titÅ«ru', 'Purattu', 'miá¹£ru', 'mÄtu', 'á¹­arÄdu', 'erÅ¡u', 'UNK', 'UNK', 'Å¡a', 'Å¡adÄdu', 'Å¡arrÅ«tu', 'kunukku', 'kiÅ¡Ädu', 'adi', 'abnu', 'kiÅ¡Ädu', 'narkabtu', 'Å¡arrÅ«tu', 'UNK', 'UNK', 'mimma', 'Å¡umu', 'mÄdu', 'Å¡a', 'nÄ«bu', 'lÄ', 'iÅ¡Ã»', 'ekÄ“mu', 'sisÃ»', 'UNK', 'UNK', 'ummiÄnu', 'ana', 'lÄ', 'mÄnu', 'leqÃ»', 'bÄ«tu', 'á¹£Ä“ru', 'kuÅ¡tÄru', 'Å¡arrÅ«tu', 'UNK', 'UNK', 'unÅ«tu', 'tÄhÄzu', 'mÄdu', 'ina', 'qerbu', 'uÅ¡mannu', 'ina', 'iÅ¡Ätu', 'Å¡arÄpu', 'UNK', 'UNK', 'UNK', 'erÅ¡u', 'ana', 'IÅ¡tar', 'Å¡arratu', 'Ninua', 'qiÄÅ¡u', 'UNK']\nakk11 = ['maÅ¡ku', 'pÄ«ru', 'Å¡innu', 'pÄ«ru', 'argamannu', 'takiltu', 'lubuÅ¡tu', 'birmu', 'kitÃ»', 'lubuÅ¡tu', 'mÄtu', 'mÄdu', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'tillu', 'pilaqqu', 'UNK', 'UNK', 'UNK', 'UNK', 'ina', 'qerbu', 'Arpadda', 'mahÄru', 'Tutammu', 'Å¡arru', 'Unqi', 'ina', 'adÃ»', 'ilu', 'rabÃ»', 'haá¹­Ã»', 'Å¡iÄá¹­u', 'napiÅ¡tu', 'gerru', 'UNK', 'lÄ', 'malÄku', 'itti', 'ina', 'uzzu', 'libbu', 'UNK', 'Å¡a', 'Tutammu', 'adi', 'rabÃ»', 'UNK', 'Kunalua', 'Älu', 'Å¡arrÅ«tu', 'kaÅ¡Ädu', 'niÅ¡u', 'adi', 'marÅ¡Ä«tu', 'UNK', 'kÅ«danu', 'ina', 'qerbu', 'ummÄnu', 'kÄ«ma', 'á¹£Ä“nu', 'manÃ»', 'UNK', 'ina', 'qabaltu', 'Ä“kallu', 'Å¡a', 'Tutammu', 'kussÃ»', 'nadÃ»', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'meÊ¾atu', 'biltu', 'kaspu', 'ina', 'dannu', 'UNK', 'meÊ¾atu', 'biltu', 'UNK', 'UNK', 'unÅ«tu', 'tÄhÄzu', 'lubuÅ¡tu', 'birmu', 'kitÃ»', 'rÄ«qu', 'kalÄma', 'bÅ«Å¡u', 'Ä“kallu', 'UNK', 'Kunalua', 'ana', 'eÅ¡Å¡Å«tu', 'á¹£abÄtu', 'Unqi', 'ana', 'pÄá¹­u', 'gimru', 'kanÄÅ¡u', 'UNK', 'Å¡Å«t', 'rÄ“Å¡u', 'bÄ“lu', 'pÄ«hÄtu', 'eli', 'Å¡akÄnu']\nakk13 = ['UNK', 'ana', 'Hatti', 'adi', 'mahru', 'wabÄlu', 'Å¡Å«t', 'rÄ“Å¡u', 'Å¡aknu', 'mÄtu', 'NaÊ¾iri', 'Supurgillu', 'UNK', 'adi', 'Älu', 'Å¡a', 'liwÄ«tu', 'kaÅ¡Ädu', 'Å¡allatu', 'Å¡alÄlu', 'Å iqila', 'rabÃ»', 'birtu', 'UNK', 'Å¡alÄlu', 'ana', 'Hatti', 'adi', 'mahru', 'wabÄlu', 'UNK', 'meÊ¾atu', 'Å¡allatu', 'Amlate', 'Å¡a', 'Damunu', 'UNK', 'lÄ«m', 'UNK', 'meÊ¾atu', 'Å¡allatu', 'Deri', 'ina', 'Kunalua', 'UNK', 'Huzarra', 'TaÊ¾e', 'Tarmanazi', 'Kulmadara', 'Hatatirra', 'Irgillu', 'Älu', 'Å¡a', 'Unqi', 'waÅ¡Äbu', 'UNK', 'Å¡allatu', 'Qutu', 'Bit-Sangibuti', 'UNK', 'lÄ«m', 'UNK', 'meÊ¾atu', 'Illilayu', 'UNK', 'lÄ«m', 'UNK', 'meÊ¾atu', 'UNK', 'Nakkabayu', 'Budayu', 'ina', 'UNK', 'á¹¢imirra', 'Arqa', 'Usnu', 'SiÊ¾annu', 'Å¡a', 'Å¡iddu', 'tiÄmtu', 'waÅ¡Äbu', 'UNK', 'meÊ¾atu', 'UNK', 'Budayu', 'Dunu', 'UNK', 'UNK', 'UNK', 'UNK', 'meÊ¾atu', 'UNK', 'Belayu', 'UNK', 'meÊ¾atu', 'UNK', 'Banitayu', 'UNK', 'meÊ¾atu', 'UNK', 'Palil-andil-mati', 'UNK', 'meÊ¾atu', 'UNK', 'Sangillu', 'UNK', 'Illilayu', 'UNK', 'meÊ¾atu', 'UNK', 'Å¡allatu', 'Qutu', 'Bit-Sangibuti', 'ina', 'pÄ«hÄtu', 'TuÊ¾immu', 'waÅ¡Äbu', 'UNK', 'meÊ¾atu', 'UNK', 'Å¡allatu', 'Qutu', 'Bit-Sangibuti', 'ina', 'Til-karme', 'waÅ¡Äbu', 'itti', 'niÅ¡u', 'mÄtu', 'AÅ¡Å¡ur', 'manÃ»', 'ilku', 'tupÅ¡ikku', 'kÄ«', 'Å¡a', 'AÅ¡Å¡uru', 'emÄ“du', 'maddattu', 'Å¡a', 'KuÅ¡taÅ¡pi', 'Kummuhayu', 'Rahianu', 'Å a-imeriÅ¡ayu', 'Menaheme', 'Samerinayu', 'Hi-rumu', 'á¹¢urrayu', 'Sibitti-BiÊ¾il', 'Gublayu', 'Uriaikki', 'Quayu', 'Pisiris', 'GargamiÅ¡ayu', 'Eni-il', 'Hamatayu', 'Panammu', 'SamÊ¾allayu', 'Tarhu-lara', 'Gurgumayu', 'Sulumal', 'Meliddayu', 'Dadilu']\n:::\n\n# returns a list of lists, each lists is one document in the corpus\n\ntokenized_corpus = []\nfor doc in [akk05, akk08, akk11, akk13] :\n  akk = []\n  for token in doc :\n    akk.append(token)#.lower())\n  tokenized_corpus.append(akk)\n\nfor doc in tokenized_corpus :\n  print(doc)\n\nCounter implements a number of methods specialized for convenient and rapid tallies. For instance, the method Counter.most_common returns the n most frequent items:\n\n# Count token frequencies\n\nvocabulary_akk = Counter(akk05)\nprint(vocabulary_akk)\nprint(vocabulary_akk.most_common(n=5))\n\n\n\n\n\n\nArguments:\n\ntokenized_corpus (list): a tokenized corpus represented, list of lists of strings.\nmin_count (int, optional): the minimum occurrence count of a vocabulary item in the corpus.\nmax_count (int, optional): the maximum occurrence count of a vocabulary item in the corpus. Note that the default maximum count is set to infinity (max_count=float(â€˜infâ€™)). This ensures that none of the high-frequency words are filtered without further specification.\n\nReturns:\n\nlist: an alphabetically ordered list of unique words in the corpus\n\n\n\ndef extract_vocabulary(tokenized_corpus, min_count=1, max_count=float('inf')):\n\n    vocabulary = collections.Counter()\n    for document in tokenized_corpus:\n        vocabulary.update(document)\n    vocabulary = {word for word, count in vocabulary.items()\n                  if count &gt;= min_count and count &lt;= max_count}\n    return sorted(vocabulary)\n\n\n# Call the function\n\nvocabulary = extract_vocabulary(tokenized_corpus, min_count = 1)\nprint(vocabulary)\n\n\n# Check token counts for each type in the vocabulary\nbags_of_words = []\nfor document in tokenized_corpus:\n    tokens = [word for word in document if word in vocabulary]\n    bags_of_words.append(collections.Counter(tokens))\n    #bags_of_words.extend(collections.Counter(tokens))\n\nfor count in bags_of_words :\n    print(count)\n#print(bags_of_words)\n\n\n\n\nTransform a tokenized corpus into a document-term matrix.\n\nArguments:\n\ntokenized_corpus (list): a tokenized corpus as a list of lists of strings.\nvocabulary (list): A list of unique words (types).\n\nReturns:\n\nlist: A list of lists representing the frequency of each term in vocabulary for each document in the corpus.\n\n\n\n## Calculate term frequency (TF)\n# raw count\n\ndef corpus2dtm_raw(tokenized_corpus, vocabulary):\n\n    document_term_matrix = []\n    for document in tokenized_corpus:\n        document_counts = collections.Counter(document)\n        row = [document_counts[word] for word in vocabulary]\n        document_term_matrix.append(row)\n    return document_term_matrix\n\n\n# Call the function\ndocument_term_matrix = corpus2dtm_raw(tokenized_corpus, vocabulary)\nprint(document_term_matrix)\n\n# Convert result into a dataframe\ntf_df_abs = pd.DataFrame(document_term_matrix, columns=vocabulary)\n\ntf_df_abs\n\nThere are three (and possibly more) ways to calculate the TF: * raw count (token count) â€“ like in the function above  This is the simplest form, where TF is just the raw count of the term in the document: TF(t,d) = countÂ ofÂ termÂ tÂ inÂ documentÂ d * relative frequency (token count / number of tokens in the document) TF is normalized by dividing the raw count by the total number of terms in the document:  TF(t,d) = countÂ ofÂ termÂ tÂ inÂ documentÂ d / totalÂ numberÂ ofÂ tokensÂ inÂ documentÂ d * logarithmically scaled: typically involves a normalization step to account for the length of the document. This reduces the impact of (very) frequent terms. TF(t,d) = log(count of term t in document d + 1)  (When the term frequency is 0, + 1 avoids log(0) which would result in an error.)\n\n# Demonstration of different calculations of the term frequency\n\ndef raw_count_tf(term, document):\n    return document.count(term)\n\ndef relative_frequency_tf(term, document):\n    term_count = document.count(term)\n    total_terms = len(document)\n    return term_count / total_terms if total_terms &gt; 0 else 0 # avoid division by 0\n\ndef log_scaled_tf(term, document):\n    total_terms = len(document)\n    term_count = document.count(term)\n    return np.log(1 + term_count) # avoid log(0) by adding 1\n\n# Example document\ndocument = [\"this\", \"is\", \"a\", \"sample\", \"document\", \"document\", \"is\", \"a\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\"]\n\nterm = \"sample\" # change to \"sample\" to see the scaling down effect for frequent terms\n\nprint(\"Raw Count TF:\", raw_count_tf(term, document))\nprint(\"Relative Frequency TF:\", relative_frequency_tf(term, document))\nprint(\"Log Scaled TF:\", log_scaled_tf(term, document)) # natural base of logarithm (e = \"Euler's number\")\n\n\n## Calculate term frequency (TF) for our corpus\n# relative frequency\n\ndef corpus_tf(tokenized_corpus, vocabulary):\n    document_term_matrix = []\n    for document in tokenized_corpus:\n        term_per_document_counts = collections.Counter(document)\n        total_terms = sum(term_per_document_counts.values())\n        #total_terms = len(document_counts)\n        #row = [term_per_document_counts[word] for word in vocabulary]\n        row = [np.log(1 + term_per_document_counts[word]) for word in vocabulary]\n        document_term_matrix.append(row)\n    return document_term_matrix\n\n\n# Call the function\n\n# Calculate term frequency (TF) document-term matrix\nterm_frequency_matrix = corpus_tf(tokenized_corpus, vocabulary)\n\n# Convert the matrix to a DataFrame for easier visualization\ntf_df_log = pd.DataFrame(term_frequency_matrix, columns=vocabulary)\n\ntf_df_log#[\"libbu\"]\n\n\n\n\n\nN = number of documents (in the corpus) \ndf_term_counts = number of documents which contain term t\nabsolute  IDF(t) = number of documents / number of documents containing term t\nlogarithmic  IDF(t) = log(number of documents / number of documents containing term t + 1)\n\n\n# Step 1: Calculate the total number of documents (N): How many documents are there in the corpus?\nN = len(tf_df_abs)\nprint(N)\n\n# Step 2: Calculate the document frequency (DF) for each term: In how many documents does the term appear?\ndf_nonzero = tf_df_abs &gt; 0  # Convert counts to binary (True/False)\n#print(df_nonzero)\ndf_term_counts = df_nonzero.sum(axis=0)  # Sum across rows, i.e. across documents; rows/documents set to 'False' are not counted\nprint(df_term_counts)\n\n# Step 3: Calculate the IDF for each term\n# add 1 for normalization\nidf = np.log(N / (df_term_counts + 1 )) + 1 # natural base (\"Euler's number\")\n\n# Display the IDF values\nidf_df = pd.DataFrame(idf, columns=['IDF']).T\nidf_df.loc['df_term_count'] = df_term_counts\nidf_df#.iloc[:, 90:105]\n\n\n\n\n\n\n\nMultiply term frequency and inverse document frequency\n\n# Perform element-wise multiplication\ntf_idf_df = tf_df_log * idf_df.loc['IDF']\n\n#tf_idf_df.iloc[:, 90:105]\ntf_idf_df#[\"libbu\"]\n\n\n\n\n\ndef calculate_tf_weighted(term, document):\n    term_count = document.count(term)\n    if term_count &gt; 0:\n        tf = 1 + np.log(term_count)\n    else:\n        tf = 0\n    return tf\n\ndef calculate_idf_weighted(term, corpus):\n    num_documents_with_term = sum(1 for doc in corpus if term in doc)\n    idf = np.log((len(corpus) + 1) / (num_documents_with_term + 1)) + 1\n    return idf\n\ndef calculate_tf_idf_weighted(term, document, corpus):\n    tf = calculate_tf_weighted(term, document)\n    idf = calculate_idf_weighted(term, corpus)\n    tf_idf = tf * idf\n    return tf_idf\n\n# Define a function to calculate the weighted TF-IDF for every document in the corpus\ndef calculate_tf_idf_matrix(tokenized_corpus, vocabulary):\n    tf_idf_matrix = []\n    for document in tokenized_corpus:\n        row = [calculate_tf_idf_weighted(term, document, tokenized_corpus) for term in vocabulary]\n        tf_idf_matrix.append(row)\n    return tf_idf_matrix\n\n\n#Calculate the TF-IDF weighted matrix for the entire corpus\ntf_idf_matrix = calculate_tf_idf_matrix(tokenized_corpus, vocabulary)\n\n# Step 3: Convert the matrix to a DataFrame\ntf_idf_df = pd.DataFrame(tf_idf_matrix, columns=vocabulary)\n\n# Display the resulting TF-IDF DataFrame\ntf_idf_df#[\"AÅ¡Å¡ur\"]#[\"libbu\"]\n\n\n\n\n\nIn the main code of the course, we will use a predefined function from the machine learning library scikit-learn.\nLearn more about the weighting and normalization in scikit-learns TF-IDF calculations,  cf.Â in particular the â€œNumeric example of a tf-idf matrixâ€\nTfidfVectorizer requires a list of strings as input. Each string is an entire text (document).\n\n## Alternative preprocessing for the TfidfVectorizer from SciKitLearn\n# returns a list of strings, each string is one document\n\ntokenized_corpus_asStr = []\nfor doc in [akk05, akk08, akk11, akk13] :\n  akk = \"\"\n  for token in doc :\n    akk = akk + token + \" \"\n  tokenized_corpus_asStr.append(akk.strip())\n\nfor doc in tokenized_corpus_asStr :\n  print(doc + \" ------\")\n#print(tokenized_corpus_asStr)\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n#from sklearn.feature_extraction.text import TfidfTransformer\n\n# Step 1: Instantiate TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df = 0, max_df = 100, norm = \"l1\") # with and without normalization (norm = None)\n\n# Step 2: Fit and transform the corpus\ntfidf_matrix = vectorizer.fit_transform(tokenized_corpus_asStr)\n\n# Display the TF-IDF matrix\n#tfidf_matrix.toarray()\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\ntfidf_df#[\"aÅ¡Å¡ur\"]#[\"libbu\"]\n\n# Display the feature names\n#vectorizer.get_feature_names_out()"
  }
]